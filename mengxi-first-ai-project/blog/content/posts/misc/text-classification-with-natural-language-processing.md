---
title: "Text Classification with Natural Language Processing"
date: 2025-12-16
tags: ["tech", "tutorial", "improvisation"]
categories: ["tech"]
layout: "single" 
---


## Introduction and Text Data Collection

Welcome to today's lesson! As data science and machine learning professionals, particularly in the Natural Language Processing (NLP) field, we often deal with textual data. Today, we dive into the 'Introduction to Textual Data Collection'. Specifically, we'll explore how to collect, understand and analyze text data usingÂ `Python`.

Textual data is usually unstructured, being much harder to analyze than structured data. It can take many forms, such as emails, social media posts, books, or transcripts of conversations. Understanding how to handle such data is a critical part of building effective machine learning models, especially for text classification tasks where we 'classify' or categorize texts. The quality of the data we use for these tasks is of utmost importance. Better, well-structured data leads to models that perform better.

##### The 20 Newsgroups Dataset

The dataset we'll be working with in today's lesson is theÂ **20 Newsgroups dataset**. For some historical background, newsgroups were the precursors to modern internet forums, where people gathered to discuss specific topics. In our case, the dataset consists of approximately 20,000 documents from newsgroup discussions. These texts were originally exchanged through Usenet, a global discussion system that predates many modern Internet forums.

The dataset is divided nearly evenly across 20 different newsgroups, each corresponding to a separate topic - this segmentation is one of the main reasons why it is especially useful for text classification tasks. The separation of data makes it excellent for training models to distinguish between different classes, or in our case, newsgroup topics.

From science and religion to politics and sports, the topics covered provide a diversified range of discussions. This diversity adds another layer of complexity and richness, similar to what we might experience with real-world data.

##### Fetching and Understanding the Data Structure

To load this dataset, we use theÂ `fetch_20newsgroups()`Â function from theÂ `sklearn.datasets`Â module in Python. This function retrieves the 20 newsgroup dataset in a format that's useful for machine learning purposes. Let's fetch and examine the dataset.

First, let's import the necessary libraries and fetch the data:

```python

`1# Importing necessary libraries 2from sklearn.datasets import fetch_20newsgroups 3 4# Fetch data 5newsgroups = fetch_20newsgroups(subset='all')`

```

The datasets fetched from sklearn typically have three attributesâ€”`data`,Â `target`, andÂ `target_names`.Â `data`Â refers to the actual content,Â `target`Â refers to the labels for the texts, andÂ `target_names`Â provides names for the target labels.

Next, let's understand the structure of the fetched data:

Python

CopyPlay

```python

`1# Understanding the structure of the data 2print("\n\nData Structure\n-------------") 3print(f'Type of data: {type(newsgroups.data)}') 4print(f'Type of target: {type(newsgroups.target)}')`

```

We are fetching the data and observing the type of theÂ `data`Â andÂ `target`. TheÂ `type of data`Â tells us what kind of data structure is used to store the text data while theÂ `type of target`Â shouts what type of structure is used to store the labels. Here is what the output looks like:

```python

`1Data Structure 2------------- 3Type of data: <class 'list'> 4Type of target: <class 'numpy.ndarray'>`

```

As printed out, theÂ `data`Â is stored as a list, andÂ `target`Â as a numpy array.

##### Diving Into Data Exploration

Now, let's explore the data points, target variables and the potential classes in the dataset:

```python

`1print("\n\nData Exploration\n----------------") 2print(f'Number of datapoints: {len(newsgroups.data)}') 3print(f'Number of target variables: {len(newsgroups.target)}') 4print(f'Possible classes: {newsgroups.target_names}')`

```

We get the length of theÂ `data`Â list to fetch the number of data points. Also, we get the length of theÂ `target`Â array. Lastly, we fetch the possible classes or newsgroups in the dataset. Here is what we get:

```python

`1Data Exploration 2---------------- 3Number of datapoints: 18846 4Number of target variables: 18846 5Possible classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']`

```

##### Sample Data Preview

Lastly, let's fetch and understand what a sample data point and its corresponding label looks like:

```python

`1print("\n\nSample datapoint\n----------------") 2print(f'\nArticle:\n-------\n{newsgroups.data[10]}') 3print(f'\nCorresponding Topic:\n------------------\n{newsgroups.target_names[newsgroups.target[10]]}')`

```

TheÂ `Article`Â fetched is the 10th article in the dataset andÂ `Corresponding Topic`Â is the actual topic that the article belongs to. Here's the output:

```python

`1Sample datapoint 2---------------- 3 4Article: 5------- 6From: sandvik@newton.apple.com (Kent Sandvik) 7Subject: Re: 14 Apr 93   God's Promise in 1 John 1: 7 8Organization: Cookamunga Tourist Bureau 9Lines: 17 10 11In article <1qknu0INNbhv@shelley.u.washington.edu>, > Christian:  washed in 12the blood of the lamb. 13> Mithraist:  washed in the blood of the bull. 14>  15> If anyone in .netland is in the process of devising a new religion, 16> do not use the lamb or the bull, because they have already been 17> reserved.  Please choose another animal, preferably one not 18> on the Endangered Species List.   19 20This will be a hard task, because most cultures used most animals 21for blood sacrifices. It has to be something related to our current 22post-modernism state. Hmm, what about used computers? 23 24Cheers, 25Kent 26--- 27sandvik@newton.apple.com. ALink: KSAND -- Private activities on the net. 28 29 30Corresponding Topic: 31------------------ 32talk.religion.misc`

```

##### Lesson Summary

Nice work! Through today's lesson, you've learned to fetch and analyze text data for text classification. If you've followed along, you should now understand the structure of text data and how to fetch and analyze it using Python.

But our journey to text classification is just starting. In upcoming lessons, we'll dive deeper into related topics such as cleaning textual data, handling missing values, and restructuring textual data for analysis. Each step forward improves your expertise in text classification. Keep going!

## ã€ŒPractice1ã€Explore More of the 20 Newsgroups Dataset

## ã€ŒPractice2ã€Uncover the End of 20 Newsgroups Dataset
Celestial Traveler, your journey continues! Fill in the blanks (`____`) to import and explore our dataset. We aim to extract and display theÂ **last three articles**Â and their correspondingÂ **topics**. Can you reveal what's at the end of our dataset?

```python

# Import necessary libraries

from sklearn.datasets import fetch_20newsgroups

  

# Fetch the dataset

newsgroups = ____(subset='all')

  

# Fetch last three articles

last_three_articles = newsgroups.____[-3:]

  

# Fetch the corresponding topics

corresponding_topics = [newsgroups.target_names[i] for i in newsgroups.____[-3:]]

  

# Display Last three articles and their corresponding topics

for i in range(3):

print(f'\nLast article {i+1}:')

print(f'{last_three_articles[i]}\n')

print(f'Corresponding Topic {i+1}:')

print(f'{corresponding_topics[i]}\n')

```

"Here is the completed code to import and explore the dataset, extracting and displaying the last three articles and their corresponding topics."

```python

# Import necessary libraries
from sklearn.datasets import fetch_20newsgroups

# Fetch the dataset
newsgroups = fetch_20newsgroups(subset='all')

# Fetch last three articles
last_three_articles = newsgroups.data[-3:]

# Fetch the corresponding topics
corresponding_topics = [newsgroups.target_names[i] for i in newsgroups.target[-3:]]

# Display last three articles and their corresponding topics
for i in range(3):
    print(f'\nLast article {i+1}:')
    print(f'{last_three_articles[i]}\n')
    print(f'Corresponding Topic {i+1}:')
    print(f'{corresponding_topics[i]}\n')

```

## ã€ŒPractice3ã€Fetch Specific Categories from Dataset

Celestial Traveler, let's narrow down our data collection. Modify the provided code to fetch only theÂ `'alt.atheism'`Â andÂ `'talk.religion.misc'`Â categories from ourÂ **dataset**. Then, display the first two articles from these categories along with their corresponding labels.  
å¤©ä½“æ—…è¡Œè€…ï¼Œè®©æˆ‘ä»¬ç¼©å°æ•°æ®æ”¶é›†èŒƒå›´ã€‚ä¿®æ”¹æä¾›çš„ä»£ç ï¼Œä½¿å…¶ä»…ä»æˆ‘ä»¬çš„æ•°æ®é›†ä¸­è·å–Â `'alt.atheism'`Â å’ŒÂ `'talk.religion.misc'`Â ç±»åˆ«ã€‚ç„¶åï¼Œæ˜¾ç¤ºæ¥è‡ªè¿™äº›ç±»åˆ«çš„å‰ä¸¤ç¯‡æ–‡ç« 

```python

# Import necessary libraries

from sklearn.datasets import fetch_20newsgroups

  

# Fetch a subset of the dataset containing selected categories. Update the categories as needed.

newsgroups_subset = fetch_20newsgroups(subset='all', categories=['comp.graphics', 'sci.space'])

  

# Display the first two articles and their corresponding topics from this subset

for i in range(2):

print(f'\nArticle {i+1}:')

print(f'{newsgroups_subset.data[i]}\n')

print(f'Corresponding Topic {i+1}:')

print(f'{newsgroups_subset.target_names[newsgroups_subset.target[i]]}\n')

```

"Here is the modified code to fetch only the `'alt.atheism'` and `'talk.religion.misc'` categories from the dataset, and to display the first two articles along with their corresponding labels."

```python

# Import necessary libraries
from sklearn.datasets import fetch_20newsgroups

# Fetch a subset of the dataset containing selected categories
newsgroups_subset = fetch_20newsgroups(subset='all', categories=['alt.atheism', 'talk.religion.misc'])

# Display the first two articles and their corresponding topics from this subset
for i in range(2):
    print(f'\nArticle {i+1}:')
    print(f'{newsgroups_subset.data[i]}\n')
    print(f'Corresponding Topic {i+1}:')
    print(f'{newsgroups_subset.target_names[newsgroups_subset.target[i]]}\n')

```

è¾“å‡º

```

Article 1:  
From: agr00@ccc.amdahl.com (Anthony G Rose)  
Subject: Re: Who's next?  Mormons and Jews?  
Reply-To: agr00@JUTS.ccc.amdahl.com (Anthony G Rose)  
Organization: Amdahl Corporation, Sunnyvale CA  
Lines: 18  
  
In article <1993Apr20.142356.456@ra.royalroads.ca> mlee@post.RoyalRoads.ca (Malcolm Lee) writes:  
>  
>In article <C5rLps.Fr5@world.std.com>, jhallen@world.std.com (Joseph H Allen) writes:  
>|> In article <1qvk8sINN9vo@clem.handheld.com> jmd@cube.handheld.com (Jim De Arras) writes:  
>|>   
>|> It was interesting to watch the 700 club today.  Pat Robertson said that the  
>|> "Branch Dividians had met the firey end for worshipping their false god." He  
>|> also said that this was a terrible tragedy and that the FBI really blew it.  
>  
>I don't necessarily agree with Pat Robertson.  Every one will be placed before  
>the judgement seat eventually and judged on what we have done or failed to do  
>on this earth.  God allows people to choose who and what they want to worship.  
  
I'm sorry, but He does not!  Ever read the FIRST commandment?  
  
>Worship of money is one of the greatest religions in this country.  
  
You mean, false religion!  
  
  
Corresponding Topic 1:  
talk.religion.misc  
  
  
Article 2:  
From: frank@D012S658.uucp (Frank O'Dwyer)  
Subject: Re: Tieing Abortion to Health Reform -- Is Clinton Nuts?  
Organization: Siemens-Nixdorf AG  
Lines: 21  
NNTP-Posting-Host: d012s658.ap.mchp.sni.de  
  
In article <1993Apr26.163627.11364@csrd.uiuc.edu> g-skinner@uiuc.edu writes:  
#I find myself unable to put these two statements together in a  
#sensible way:  

#  
#>Abortion is done because the mother can not afford the *pregnancy*.  

#  
#[...]  

#  
#>If we refused to pay for the more expensive choice of birth, *then*  
#>your statement would make sense.  But that is not the case, so it doesn't.  

#  
#Are we paying for the birth or not, Mr. Parker?  If so, why can't the  
#mother afford the pregnancy?  If not, what is the meaning of the  
#latter objection?  You can't have it both ways.  
  
Birth != pregnancy.  If they were the same, the topic of abortion would   
hardly arise, would it, Mr. Skinner?  
  
--   
Frank O'Dwyer                                  'I'm not hatching That'  
odwyer@sse.ie                                  from "Hens",  by Evelyn Conlon  
  
  
Corresponding Topic 2:  
talk.religion.misc

```

## ã€ŒPracticeã€Fetching the Third Article from Dataset
Well done, Stellar Navigator! Next, fill in the missing line in the code below to fetch and display the third article from theÂ **20 Newsgroups dataset**Â with its corresponding topic. Prepare your spacecraft for another adventure in data exploration!  
å¹²å¾—å¥½ï¼Œæ˜Ÿé™…å¯¼èˆªå‘˜ï¼æ¥ä¸‹æ¥ï¼Œå¡«å†™ä»¥ä¸‹ä»£ç ä¸­ç¼ºå°‘çš„è¡Œï¼Œä»¥è·å–å¹¶æ˜¾ç¤º 20 Newsgroups æ•°æ®é›†ä¸­ç¬¬ä¸‰ç¯‡æ–‡ç« åŠå…¶å¯¹åº”ä¸»é¢˜ã€‚å‡†å¤‡å¥½ä½ çš„å®‡å®™é£èˆ¹ï¼Œå¼€å§‹å¦ä¸€åœºæ•°æ®æ¢ç´¢å†’é™©ä¹‹æ—…å§ï¼

```python

# Import necessary libraries

from sklearn.datasets import fetch_20newsgroups

  

# Fetch the dataset

newsgroups = fetch_20newsgroups(subset='all')

  

# TODO: Fetch the third article and its corresponding topic

```

"Here is the completed code to fetch and display the third article from the 20 Newsgroups dataset along with its corresponding topic."

```python

# Import necessary libraries
from sklearn.datasets import fetch_20newsgroups

# Fetch the dataset
newsgroups = fetch_20newsgroups(subset='all')

# Fetch the third article and its corresponding topic
third_article = newsgroups.data[2]
third_topic = newsgroups.target_names[newsgroups.target[2]]

# Display the third article and its corresponding topic
print(f'Article 3:\n{third_article}\n')
print(f'Corresponding Topic 3: {third_topic}')

```

è¾“å‡º

```

Article 3:  
From: hilmi-er@dsv.su.se (Hilmi Eren)  
Subject: Re: ARMENIA SAYS IT COULD SHOOT DOWN TURKISH PLANES (Henrik)  
Lines: 95  
Nntp-Posting-Host: viktoria.dsv.su.se  
Reply-To: hilmi-er@dsv.su.se (Hilmi Eren)  
Organization: Dept. of Computer and Systems Sciences, Stockholm University  
  
  
  
  
|>The student of "regional killings" alias Davidian (not the Davidian religios sect) writes:  
  
  
|>Greater Armenia would stretch from Karabakh, to the Black Sea, to the  
|>Mediterranean, so if you use the term "Greater Armenia" use it with care.  
  
  
	Finally you said what you dream about. Mediterranean???? That was new....  
	The area will be "greater" after some years, like your "holocaust" numbers......  
  
  
  
  
|>It has always been up to the Azeris to end their announced winning of Karabakh   
|>by removing the Armenians! When the president of Azerbaijan, Elchibey, came to   
|>power last year, he announced he would be be "swimming in Lake Sevan [in   
|>Armeniaxn] by July".  
		*****  
	Is't July in USA now????? Here in Sweden it's April and still cold.  
	Or have you changed your calendar???  
  
  
|>Well, he was wrong! If Elchibey is going to shell the   
|>Armenians of Karabakh from Aghdam, his people will pay the price! If Elchibey   
						    ****************  
|>is going to shell Karabakh from Fizuli his people will pay the price! If   
						    ******************  
|>Elchibey thinks he can get away with bombing Armenia from the hills of   
|>Kelbajar, his people will pay the price.   
			    ***************  
  
  
	NOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT's TRUE.  
	  
	SHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH  
						    **************  
	BEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????  
	  
	HAVE YOU HEARDED SOMETHING CALLED: "GENEVA CONVENTION"???????  
	YOU FACIST!!!!!  
  
  
  
	Ohhh i forgot, this is how Armenians fight, nobody has forgot  
	you killings, rapings and torture against the Kurds and Turks once  
	upon a time!

```

## ã€ŒPracticeã€Exploring Text Length in Newsgroups Dataset

Great job, Space Voyager! Now, as a final task, write a Python script that calculates and displays the lengths of the first five articles (in terms of the number of characters) from theÂ _20 Newsgroups_Â dataset.

```python

# Import necessary libraries and modules

from sklearn.datasets import fetch_20newsgroups

  

# TODO: Fetch the 20 Newsgroups dataset

  

# TODO: Iterate over the first five articles,

# TODO: Calculate their length in terms of the number of characters and display it

```

"Here is the completed Python script to calculate and display the lengths of the first five articles in terms of the number of characters from the 20 Newsgroups dataset."

```python

# Import necessary libraries and modules
from sklearn.datasets import fetch_20newsgroups

# Fetch the 20 Newsgroups dataset
newsgroups = fetch_20newsgroups(subset='all')

# Iterate over the first five articles
for i in range(5):
    article_length = len(newsgroups.data[i])
    print(f'Length of Article {i+1}: {article_length} characters')

```

## LessonÂ 2ï¼šMastering Text Cleaning for NLP: Techniques and Applications

### IntroductionÂ å¼•è¨€

Welcome to today's lesson onÂ **Text Cleaning Techniques**! In any Natural Language Processing (NLP) project, the quality of your results depends heavily on the quality of your input. Hence, cleaning our textual data becomes critical for the accuracy of our project. Our main objective for today is to delve into how to clean textual data using Python. By the end of this session, you will be comfortable with creating and applying a simple text cleaning pipeline in Python.  
æ¬¢è¿æ¥åˆ°ä»Šå¤©å…³äºæ–‡æœ¬æ¸…ç†æŠ€æœ¯çš„è¯¾ç¨‹ï¼åœ¨ä»»ä½•è‡ªç„¶è¯­è¨€å¤„ç† (NLP) é¡¹ç›®ä¸­ï¼Œç»“æœçš„è´¨é‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè¾“å…¥çš„è´¨é‡ã€‚å› æ­¤ï¼Œæ¸…ç†æ–‡æœ¬æ•°æ®å¯¹äºé¡¹ç›®çš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬ä»Šå¤©çš„ä¸»è¦ç›®æ ‡æ˜¯æ·±å…¥ç ”ç©¶å¦‚ä½•ä½¿ç”¨ Python æ¸…ç†æ–‡æœ¬æ•°æ®ã€‚åœ¨æœ¬è¯¾ç¨‹ç»“æŸæ—¶ï¼Œæ‚¨å°†èƒ½å¤Ÿè½»æ¾åœ°ä½¿ç”¨ Python åˆ›å»ºå’Œåº”ç”¨ç®€å•çš„æ–‡æœ¬æ¸…ç†ç®¡é“ã€‚

##### Understanding Text Cleaning  
ç†è§£æ–‡æœ¬æ¸…æ´—

**Text cleaning**Â is essential in NLP, involving the preparation of text data for analysis. Why is it necessary? Imagine trying to perform text classification on social media posts; people often use colloquial language, abbreviations, and emojis. In many cases, posts might also be in different languages. These variations make it challenging for machines to understand context without undergoing preprocessing.  
æ–‡æœ¬æ¸…æ´—åœ¨è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ä¸­è‡³å…³é‡è¦ï¼Œæ¶‰åŠä¸ºåˆ†æå‡†å¤‡æ–‡æœ¬æ•°æ®ã€‚ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿæƒ³è±¡ä¸€ä¸‹å°è¯•å¯¹ç¤¾äº¤åª’ä½“å¸–å­è¿›è¡Œæ–‡æœ¬åˆ†ç±»ï¼›äººä»¬ç»å¸¸ä½¿ç”¨å£è¯­ã€ç¼©å†™å’Œè¡¨æƒ…ç¬¦å·ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå¸–å­ä¹Ÿå¯èƒ½ä½¿ç”¨ä¸åŒçš„è¯­è¨€ã€‚è¿™äº›å·®å¼‚ä½¿å¾—æœºå™¨åœ¨æœªç»é¢„å¤„ç†çš„æƒ…å†µä¸‹éš¾ä»¥ç†è§£ä¸Šä¸‹æ–‡ã€‚

We get rid of superfluous variations and distractions to make the text understandable for algorithms, thereby increasing accuracy. These distractions could range from punctuation, special symbols, numbers, to even common words that do not carry significant meaning (commonly referred to as "stop words").  
æˆ‘ä»¬å»é™¤å¤šä½™çš„å˜åŒ–å’Œå¹²æ‰°å› ç´ ï¼Œä½¿æ–‡æœ¬æ˜“äºç®—æ³•ç†è§£ï¼Œä»è€Œæé«˜å‡†ç¡®æ€§ã€‚è¿™äº›å¹²æ‰°å› ç´ åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ã€ç‰¹æ®Šç¬¦å·ã€æ•°å­—ï¼Œç”šè‡³æ˜¯ä¸å…·æœ‰é‡è¦æ„ä¹‰çš„å¸¸è§è¯ï¼ˆé€šå¸¸ç§°ä¸ºâ€œåœç”¨è¯â€ï¼‰ã€‚

Python'sÂ **Regex**Â (Regular Expression) library,Â `re`, is an ideal tool for such text cleaning tasks, as it is specifically designed to work with string patterns. Within this library, we will be usingÂ `re.sub`, a method employed to replace parts of a string. This method operates by accepting three arguments:Â `re.sub(pattern, repl, string)`. Here,Â `pattern`Â is the character pattern we're looking to replace,Â `repl`Â is the replacement string, andÂ `string`Â is the text being processed. In essence, any part of theÂ `string`Â argument that matches theÂ `pattern`Â argument gets replaced by theÂ `repl`Â argument.  
Python çš„æ­£åˆ™è¡¨è¾¾å¼ (Regex) åº“ `Â `re`Â ` æ˜¯æ­¤ç±»æ–‡æœ¬æ¸…ç†ä»»åŠ¡çš„ç†æƒ³å·¥å…·ï¼Œå› ä¸ºå®ƒä¸“é—¨ç”¨äºå¤„ç†å­—ç¬¦ä¸²æ¨¡å¼ã€‚åœ¨è¿™ä¸ªåº“ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `Â `re.sub`Â ` æ–¹æ³•æ¥æ›¿æ¢å­—ç¬¦ä¸²çš„æŸäº›éƒ¨åˆ†ã€‚æ­¤æ–¹æ³•æ¥å—ä¸‰ä¸ªå‚æ•°ï¼š`Â `re.sub(pattern, repl, string)`Â `ã€‚å…¶ä¸­ï¼Œ`Â `pattern`Â ` æ˜¯æˆ‘ä»¬è¦æ›¿æ¢çš„å­—ç¬¦æ¨¡å¼ï¼Œ`Â `repl`Â ` æ˜¯æ›¿æ¢å­—ç¬¦ä¸²ï¼Œ`Â `string`Â ` æ˜¯æ­£åœ¨å¤„ç†çš„æ–‡æœ¬ã€‚æœ¬è´¨ä¸Šï¼Œ`Â `string`Â ` å‚æ•°ä¸­ä¸ `Â `pattern`Â ` å‚æ•°åŒ¹é…çš„ä»»ä½•éƒ¨åˆ†éƒ½å°†è¢« `Â `repl`Â ` å‚æ•°æ›¿æ¢ã€‚

As we proceed, a clearer understanding of the functionality and application ofÂ `re.sub`Â will be provided. Now, let's delve into it!  
éšç€æˆ‘ä»¬çš„æ·±å…¥ï¼Œæˆ‘ä»¬å°†å¯¹Â `re.sub`Â çš„åŠŸèƒ½å’Œåº”ç”¨æœ‰æ›´æ¸…æ™°çš„äº†è§£ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼

### Text Cleaning ProcessÂ æ–‡æœ¬æ¸…ç†æµç¨‹

The text cleaning process comprises multiple steps where each step aims to reduce the complexity of the text. Let's take you through the process using a Python function,Â `clean_text`.  
æ–‡æœ¬æ¸…ç†è¿‡ç¨‹åŒ…å«å¤šä¸ªæ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½æ—¨åœ¨é™ä½æ–‡æœ¬çš„å¤æ‚æ€§ã€‚è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ª Python å‡½æ•°Â `clean_text`Â æ¥å¸¦æ‚¨äº†è§£æ•´ä¸ªè¿‡ç¨‹ã€‚

```python

import re

def clean_text(text):
    text = text.lower()  # Convert text to lower case
    text = re.sub(r'\S*@\S*\s?', '', text)  # Remove email addresses
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'\W', ' ', text)  # Remove punctuation and special characters
    text = re.sub(r'\d', ' ', text)  # Remove digits
    text = re.sub(r'\s\s+', ' ', text)  # Remove extra spaces

    return text

```

In the function above we can see how each line corresponds to a step in the cleaning process:  
åœ¨ä¸Šé¢çš„å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯ä¸€è¡Œæ˜¯å¦‚ä½•å¯¹åº”äºæ¸…æ´è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªæ­¥éª¤çš„ï¼š

1. **Lowercase:**Â We convert all text to lower case, so every word looks the same unless it carries a different meaning. This way, words like 'The' and 'the' are no longer seen as different.  
    å°å†™ï¼šæˆ‘ä»¬å°†æ‰€æœ‰æ–‡æœ¬è½¬æ¢ä¸ºå°å†™ï¼Œå› æ­¤æ¯ä¸ªå•è¯çœ‹èµ·æ¥éƒ½ä¸€æ ·ï¼Œé™¤éå®ƒå…·æœ‰ä¸åŒçš„å«ä¹‰ã€‚è¿™æ ·ï¼Œâ€œTheâ€å’Œâ€œtheâ€å°±ä¸å†è¢«è§†ä¸ºä¸åŒçš„è¯ã€‚
2. **Email addresses:**Â Email addresses don't usually provide useful information unless we're specifically looking for them. This line of code removes any email addresses found.  
    ç”µå­é‚®ä»¶åœ°å€ï¼šç”µå­é‚®ä»¶åœ°å€é€šå¸¸ä¸ä¼šæä¾›æœ‰ç”¨ä¿¡æ¯ï¼Œé™¤éæˆ‘ä»¬ä¸“é—¨æŸ¥æ‰¾å®ƒä»¬ã€‚è¿™è¡Œä»£ç ä¼šåˆ é™¤æ‰¾åˆ°çš„ä»»ä½•ç”µå­é‚®ä»¶åœ°å€ã€‚
3. **URLs:**Â Similarly, URLs are removed as they are typically not useful in text classification tasks.  
    URLï¼šç±»ä¼¼åœ°ï¼ŒURL é€šå¸¸åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­æ²¡æœ‰ç”¨å¤„ï¼Œå› æ­¤ä¼šè¢«åˆ é™¤ã€‚
4. **Special Characters:**Â We remove any non-word characters (`\W`) and replace it with space using regex. This includes special characters and punctuation.  
    ç‰¹æ®Šå­—ç¬¦ï¼šæˆ‘ä»¬ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ é™¤ä»»ä½•éå•è¯å­—ç¬¦ï¼ˆÂ `\W`Â ï¼‰å¹¶å°†å…¶æ›¿æ¢ä¸ºç©ºæ ¼ã€‚è¿™åŒ…æ‹¬ç‰¹æ®Šå­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·ã€‚
5. **Numbers:**Â We're dealing with text data, so numbers are also considered distractions unless they carry significant meaning.  
    æ•°å­—ï¼šæˆ‘ä»¬å¤„ç†çš„æ˜¯æ–‡æœ¬æ•°æ®ï¼Œå› æ­¤é™¤éæ•°å­—å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¦åˆ™å®ƒä»¬ä¹Ÿè¢«è§†ä¸ºå¹²æ‰°å› ç´ ã€‚
6. **Extra spaces:**Â Any resulting extra spaces from the previous steps are removed.  
    ä»å…ˆå‰æ­¥éª¤äº§ç”Ÿçš„ä»»ä½•é¢å¤–ç©ºæ ¼éƒ½å°†è¢«åˆ é™¤ã€‚

Let's go ahead and run this function on some demo input to see it in action!  
è®©æˆ‘ä»¬ç»§ç»­ï¼Œåœ¨ä¸€äº›æ¼”ç¤ºè¾“å…¥ä¸Šè¿è¡Œæ­¤å‡½æ•°ï¼Œçœ‹çœ‹å®ƒçš„å®é™…æ•ˆæœï¼

```python

print(clean_text('Check out the course at www.codesignal.com/course123'))

```

The output of the above code will be:  
ä»¥ä¸Šä»£ç çš„è¾“å‡ºå°†æ˜¯ï¼š

```python

check out the course at www codesignal com course 

```

##### Implementing Text Cleaning Function  
å®ç°æ–‡æœ¬æ¸…æ´—åŠŸèƒ½

Now that you are familiar with the workings of the function let's implement it in theÂ **20 Newsgroups**Â dataset.  
ç°åœ¨ä½ å·²ç»ç†Ÿæ‚‰äº†å‡½æ•°çš„å·¥ä½œåŸç†ï¼Œè®©æˆ‘ä»¬åœ¨ 20 Newsgroups æ•°æ®é›†ä¸­å®ç°å®ƒã€‚

To apply our cleaning function on the dataset, we will make use of the DataFrame data structure fromÂ `Pandas`, another powerful data manipulation tool in Python.  
ä¸ºäº†åœ¨æ•°æ®é›†ä¸Šåº”ç”¨æˆ‘ä»¬çš„æ¸…æ´—å‡½æ•°ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨Â `Pandas`Â ä¸­çš„ DataFrame æ•°æ®ç»“æ„ï¼Œå®ƒæ˜¯ Python ä¸­å¦ä¸€ä¸ªå¼ºå¤§çš„æ•°æ®æ“ä½œå·¥å…·ã€‚

```python

import pandas as pd
from sklearn.datasets import fetch_20newsgroups

# Fetching the 20 Newsgroups Dataset
newsgroups_data = fetch_20newsgroups(subset='train')
nlp_df = pd.DataFrame(newsgroups_data.data, columns = ['text'])

# Applied the cleaning function to the text data
nlp_df['text'] = nlp_df['text'].apply(lambda x: clean_text(x))

# Checking the cleaned text
print(nlp_df.head())

```

The output of the above code will be:  
ä»¥ä¸Šä»£ç çš„è¾“å‡ºå°†æ˜¯ï¼š

```

  
0  from where s my thing subject what car is this...
1  from guy kuo subject si clock poll final call ...
2  from thomas e willis subject pb questions orga...
3  from joe green subject re weitek p organizatio...
4  from jonathan mcdowell subject re shuttle laun...

```

In this code, we're applying theÂ `clean_text`Â function to each 'text' in our DataFrame using theÂ `apply`Â function. TheÂ `apply`Â function passes every value of the DataFrame column to theÂ `clean_text`Â function one by one.  
åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Â `apply`Â å‡½æ•°å°†Â `clean_text`Â å‡½æ•°åº”ç”¨äº DataFrame ä¸­çš„æ¯ä¸ªâ€œæ–‡æœ¬â€ã€‚Â `apply`Â å‡½æ•°å°† DataFrame åˆ—çš„æ¯ä¸ªå€¼é€ä¸ªä¼ é€’ç»™Â `clean_text`Â å‡½æ•°ã€‚

###  Understanding Effectiveness of Text Cleaning Function  
ç†è§£æ–‡æœ¬æ¸…æ´—åŠŸèƒ½çš„æœ‰æ•ˆæ€§

We want to understand the impact of our text cleaning function. We can achieve this by looking at our text before and after cleaning. Let's use some new examples:  
æˆ‘ä»¬æƒ³è¦ç†è§£æ–‡æœ¬æ¸…æ´—å‡½æ•°çš„å½±å“ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹æ¸…æ´—å‰åçš„æ–‡æœ¬å†…å®¹æ¥å®ç°è¿™ä¸€ç‚¹ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ä¸€äº›æ–°çš„ä¾‹å­ï¼š

```python

test_texts = ['This is an EXAMPLE!', 'Another ex:ample123 with special characters $#@!', 'example@mail.com is an email address.']
for text in test_texts:
    print(f'Original: {text}')
    print(f'Cleaned: {clean_text(text)}')
    print('--')

```

The output of the above code will be:  
ä»¥ä¸Šä»£ç çš„è¾“å‡ºå°†æ˜¯ï¼š

```python

Original: This is an EXAMPLE!
Cleaned: this is an example 
--
Original: Another ex:ample123 with special characters $#@!
Cleaned: another ex ample with special characters 
--
Original: example@mail.com is an email address.
Cleaned: is an email address 
--

```

In the example above, you will see that our function successfully transforms all text to lower case and removes punctuation, digits, and email addresses!  
åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œä½ ä¼šçœ‹åˆ°æˆ‘ä»¬çš„å‡½æ•°æˆåŠŸåœ°å°†æ‰€æœ‰æ–‡æœ¬è½¬æ¢ä¸ºå°å†™ï¼Œå¹¶åˆ é™¤äº†æ ‡ç‚¹ç¬¦å·ã€æ•°å­—å’Œç”µå­é‚®ä»¶åœ°å€ï¼

##### Lesson Summary and Practice Exercises  
è¯¾æ–‡æ€»ç»“å’Œç»ƒä¹ é¢˜

Today we delved into the text cleaning process in Natural Language Processing. We shared why it is necessary and how to implement it in Python. We then applied our text cleaning function on a textual dataset.  
ä»Šå¤©æˆ‘ä»¬æ·±å…¥æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ–‡æœ¬æ¸…æ´—è¿‡ç¨‹ã€‚æˆ‘ä»¬åˆ†äº«äº†ä¸ºä»€ä¹ˆéœ€è¦æ–‡æœ¬æ¸…æ´—ä»¥åŠå¦‚ä½•åœ¨ Python ä¸­å®ç°å®ƒã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ–‡æœ¬æ¸…æ´—å‡½æ•°åº”ç”¨äºä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ã€‚

We have a few exercises lined up based on what we learned today. Keep swimming ahead, and remember, you learn the most by doing. Happy cleaning!  
æˆ‘ä»¬å‡†å¤‡äº†ä¸€äº›ç»ƒä¹ ï¼Œéƒ½æ˜¯åŸºäºä»Šå¤©æ‰€å­¦çš„å†…å®¹ã€‚ç»§ç»­åŠ æ²¹ç»ƒä¹ ï¼Œè®°ä½ï¼Œå®è·µå‡ºçœŸçŸ¥ã€‚ç¥ä½ é¡ºåˆ©å®Œæˆï¼

## ã€ŒPractice1ã€
Well done, Space Voyager! Now, to further explore the workings of our text cleaning function, let's use a different sentence. Replace the first sentence in theÂ `test_texts`Â list with the phrase "I love learning at CodeSignal; it's so interactive and fun!". Then run theÂ `clean_text`Â function with the updated list.  
å¹²å¾—å¥½ï¼Œå¤ªç©ºæ—…è¡Œè€…ï¼ç°åœ¨ï¼Œä¸ºäº†è¿›ä¸€æ­¥æ¢ç´¢æˆ‘ä»¬æ–‡æœ¬æ¸…ç†åŠŸèƒ½çš„å·¥ä½œåŸç†ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„å¥å­ã€‚å°†Â `test_texts`Â åˆ—è¡¨ä¸­çš„ç¬¬ä¸€å¥è¯æ›¿æ¢ä¸ºâ€œæˆ‘å–œæ¬¢åœ¨ Co å­¦ä¹   
å¹²å¾—å¥½ï¼Œå¤ªç©ºæ—…è¡Œå®¶ï¼ç°åœ¨ï¼Œä¸ºäº†è¿›ä¸€æ­¥æ¢ç´¢æ–‡æœ¬æ¸…ç†å‡½æ•°çš„å·¥ä½œåŸç†ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„å¥å­ã€‚å°†Â `test_texts`Â åˆ—è¡¨ä¸­çš„ç¬¬ä¸€å¥è¯æ›¿æ¢ä¸ºâ€œæˆ‘å–œæ¬¢åœ¨ä»£ç å­¦ä¹ 

```python

import re

  

def clean_text(text):

text = text.lower() # Convert text to lower case

text = re.sub(r'\S*@\S*\s?', '', text) # Remove email addresses

text = re.sub(r'http\S+', '', text) # Remove URLs

text = re.sub(r'[^\w\s]', '', text) # Remove punctuation and special characters but leave spaces

text = re.sub(r'\d+', '', text) # Remove digits

text = re.sub(r'\s+', ' ', text) # Normalize whitespace to single spaces

  

return text.strip() # Strip leading and trailing spaces

  

test_texts = ['This is an EXAMPLE!',

'Another ex:ample123 with special characters $#@!',

'example@mail.com is an email address.']

for text in test_texts:

print(f'Original: {text}')

print(f'Cleaned: {clean_text(text)}')

print('--')

```

è¾“å‡º

```

Original: I love learning at CodeSignal; it's so interactive and fun!  
Cleaned: i love learning at codesignal its so interactive and fun  
--  
Original: Another ex:ample123 with special characters $#@!  
Cleaned: another example with special characters  
--  
Original: example@mail.com is an email address.  
Cleaned: is an email address  
--

```

## ã€ŒPractice2ã€Filling in Python Functions and Regex Patterns
Superb job! Now, let's ensure you have grasped the coreÂ **Python functions**Â andÂ **regex patterns**Â used in text cleaning.  
å¹²å¾—å¥½ï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥ç¡®ä¿ä½ å·²ç»æŒæ¡äº†æ–‡æœ¬æ¸…ç†ä¸­ä½¿ç”¨çš„æ ¸å¿ƒ Python å‡½æ•°å’Œæ­£åˆ™è¡¨è¾¾å¼ã€‚

Fill in the blanks (`____`) to transform all text to lowercase and remove extra spaces.

```python

import re

  

def clean_text(text):

text = text.____() # Convert text to lower case

text = re.sub(r'\S*@\S*\s?', '', text) # Remove email addresses

text = re.sub(r'http\S+', '', text) # Remove URLs

text = re.sub(r'\W', ' ', text) # Remove punctuation and special characters

text = re.sub(r'\d', ' ', text) # Remove digits

text = re.sub(r'\____\s+', ' ', text) # Remove extra spaces

return text.strip()

  

test_texts = ['Fill me In! I enjoy problem-solving tasks like these; they are quite engaging.',

'Another ex:ample123 with special characters $#@!',

'example@mail.com is an email address.']

for text in test_texts:

print(f'Original: {text}')

print(f'Cleaned: {clean_text(text)}')

print('--')

```

è§£é‡Š

```python

import re

def clean_text(text):
    text = text.lower()  # Convert text to lower case
    text = re.sub(r'\S*@\S*\s?', '', text)  # Remove email addresses
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'\W', ' ', text)  # Remove punctuation and special characters
    text = re.sub(r'\d', ' ', text)  # Remove digits
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    return text.strip()

test_texts = ['Fill me In! I enjoy problem-solving tasks like these; they are quite engaging.',
              'Another ex:ample123 with special characters $#@!',
              'example@mail.com is an email address.']

for text in test_texts:
    print(f'Original: {text}')
    print(f'Cleaned: {clean_text(text)}')
    print('--')

```

è¾“å‡º

```python

Original: Fill me In! I enjoy problem-solving tasks like these; they are quite engaging.  
Cleaned: fill me in i enjoy problem solving tasks like these they are quite engaging  
--  
Original: Another ex:ample123 with special characters $#@!  
Cleaned: another ex ample with special characters  
--  
Original: example@mail.com is an email address.  
Cleaned: is an email address  
--

```

## ã€ŒPractice2ã€Filling in Python Functions and Regex Patterns
Superb job! Now, let's ensure you have grasped the coreÂ **Python functions**Â andÂ **regex patterns**Â used in text cleaning.

Fill in the blanks (`____`) to transform all text to lowercase and remove extra spaces.

```python

import re

  

def clean_text(text):

text = text.____() # Convert text to lower case

text = re.sub(r'\S*@\S*\s?', '', text) # Remove email addresses

text = re.sub(r'http\S+', '', text) # Remove URLs

text = re.sub(r'\W', ' ', text) # Remove punctuation and special characters

text = re.sub(r'\d', ' ', text) # Remove digits

text = re.sub(r'\____\s+', ' ', text) # Remove extra spaces

return text.strip()

  

test_texts = ['Fill me In! I enjoy problem-solving tasks like these; they are quite engaging.',

'Another ex:ample123 with special characters $#@!',

'example@mail.com is an email address.']

for text in test_texts:

print(f'Original: {text}')

print(f'Cleaned: {clean_text(text)}')

print('--')

```

è§£é‡Š
Certainly! Here's the completed Python code with the blanks filled:

```python

import re

def clean_text(text):
    text = text.lower()  # Convert text to lower case
    text = re.sub(r'\S*@\S*\s?', '', text)  # Remove email addresses
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'\W', ' ', text)  # Remove punctuation and special characters
    text = re.sub(r'\d', ' ', text)  # Remove digits
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    return text.strip()

test_texts = ['Fill me In! I enjoy problem-solving tasks like these; they are quite engaging.',
              'Another ex:ample123 with special characters $#@!',
              'example@mail.com is an email address.']

for text in test_texts:
    print(f'Original: {text}')
    print(f'Cleaned: {clean_text(text)}')
    print('--')

```

Explanation:
- The `lower()` method converts all text to lowercase.
- The regex pattern `r'\s+'` matches one or more whitespace characters, which are then replaced by a single space to remove extra spaces.

### è¾“å‡º

```

Original: Fill me In! I enjoy problem-solving tasks like these; they are quite engaging.  
Cleaned: fill me in i enjoy problem solving tasks like these they are quite engaging  
--  
Original: Another ex:ample123 with special characters $#@!  
Cleaned: another ex ample with special characters  
--  
Original: example@mail.com is an email address.  
Cleaned: is an email address  
--

```

## PracticeÂ 3ï¼šMastering Text Cleaning with Python Regex
Charming maneuvering, Stellar Navigator!

Now, I want to see more of your Python prowess. Replace the missing parts (`____`), ensuring that they perform the right operations. We aim toÂ **transform all text to lowercase**Â andÂ **eliminate email addresses**,Â **URLs**,Â **special characters**,Â **digits**, andÂ **extra spaces**.

```python

import re

  

def clean_text(text):

text = text.____() # Convert text to lower case

text = re.sub(r'____', '', text) # Remove email addresses

text = re.sub(r'____', '', text) # Remove URLs

text = re.sub(r'____', ' ', text) # Remove punctuation and special characters

text = re.sub(r'____', '', text) # Remove digits

text = re.sub(r'____', ' ', text) # Remove extra spaces

  

return text.strip()

  

test_texts = ['Sample Text with EMAIL! mailsample@mail.com',

'URL example: https://www.codesignal.com',

'Special characters #formed.',

'Digits included: 1234!!',

'Extra spaces included.']

for text in test_texts:

print(f'Original: {text}')

print(f'Cleaned: {clean_text(text)}')

print('--')

```

è§£é‡Š
To clean the text by transforming it to lowercase and removing email addresses, URLs, special characters, digits, and extra spaces, you can fill in the missing parts of the provided Python code as follows:

```python

import re

def clean_text(text):
    text = text.lower() # Convert text to lower case
    text = re.sub(r'\b[\w\.-]+?@\w+?\.\w+?\b', '', text) # Remove email addresses
    text = re.sub(r'http\S+|www\.\S+', '', text) # Remove URLs
    text = re.sub(r'[^\w\s]', ' ', text) # Remove punctuation and special characters
    text = re.sub(r'\d+', '', text) # Remove digits
    text = re.sub(r'\s+', ' ', text) # Remove extra spaces
    
    return text.strip()

test_texts = [
    'Sample Text with EMAIL! mailsample@mail.com',
    'URL example: https://www.codesignal.com',
    'Special characters #formed.',
    'Digits included: 1234!!',
    'Extra spaces included.  '
]

for text in test_texts:
    print(f'Original: {text}')
    print(f'Cleaned: {clean_text(text)}')
    print('--')

```

Explanation of the substitutions:

1. `text.lower()`: Converts the text to lowercase.
2. `re.sub(r'\b[\w\.-]+?@\w+?\.\w+?\b', '', text)`: Removes email addresses by matching common email patterns.
3. `re.sub(r'http\S+|www\.\S+', '', text)`: Removes URLs by matching patterns that start with "http" or "www".
4. `re.sub(r'[^\w\s]', ' ', text)`: Removes special characters by replacing anything that is not a word character or whitespace with a space.
5. `re.sub(r'\d+', '', text)`: Removes digits by matching one or more digits.
6. `re.sub(r'\s+', ' ', text)`: Replaces multiple spaces with a single space.

This code will transform the given text according to the specified requirements.

## ã€ŒPracticeã€
Off to a flying start, Celestial Traveler! Your new task requires you to insert missing code into two sections. First,Â **write code**Â that transforms the collected dataset into aÂ `DataFrame`. Then, ensure that you apply theÂ `clean_text`Â function to theÂ `DataFrame`Â to clean up the textual data. Bon Voyage!

```python

import re

import pandas as pd

from sklearn.datasets import fetch_20newsgroups

  

def clean_text(text):

text = text.lower() # Convert text to lower case

text = re.sub(r'\S*@\S*\s?', '', text) # Remove email addresses

text = re.sub(r'http\S+', '', text) # Remove URLs

text = re.sub(r'\W', ' ', text) # Remove punctuation and special characters

text = re.sub(r'\d', ' ', text) # Remove digits

text = re.sub(r'\s+', ' ', text) # Normalize whitespace to single spaces

  

return text.strip() # Strip leading and trailing spaces

  

# Fetch the dataset

newsgroups_data = fetch_20newsgroups(subset='train')

  

# Create DataFrame from the dataset

# TODO: Write the required code to create a dataframe from the dataset. Name the dataframe as nlp_df and the column as 'text'.

  

# Apply the clean_text function to the text data in the DataFrame

# TODO: Write the code to apply the clean_text function to the 'text' column in the dataframe.

  

# Check the cleaned text

print(nlp_df.head())

```

è§£é‡Š
Certainly! Let's expand on your query to ensure comprehensive coverage. You are asking for the code needed to transform a dataset collected from the `fetch_20newsgroups` function into a pandas `DataFrame`. The task also involves applying a `clean_text` function to the text data within this `DataFrame` to clean it up by removing email addresses, URLs, punctuation, digits, and normalizing whitespace. Below is the complete code to achieve this:

1. Transform the fetched dataset into a `DataFrame`.
2. Apply the `clean_text` function to the text column in this `DataFrame`.

Here's the completed code:

```python

import re
import pandas as pd
from sklearn.datasets import fetch_20newsgroups

def clean_text(text):
    text = text.lower() # Convert text to lower case
    text = re.sub(r'\S*@\S*\s?', '', text) # Remove email addresses
    text = re.sub(r'http\S+', '', text) # Remove URLs
    text = re.sub(r'\W', ' ', text) # Remove punctuation and special characters
    text = re.sub(r'\d', ' ', text) # Remove digits
    text = re.sub(r'\s+', ' ', text) # Normalize whitespace to single spaces
    return text.strip() # Strip leading and trailing spaces

# Fetch the dataset
newsgroups_data = fetch_20newsgroups(subset='train')

# Create DataFrame from the dataset
nlp_df = pd.DataFrame({'text': newsgroups_data.data})

# Apply the clean_text function to the text data in the DataFrame
nlp_df['text'] = nlp_df['text'].apply(clean_text)

# Check the cleaned text
print(nlp_df.head())

```

### Explanation:

1. **Fetching Data**: The `fetch_20newsgroups` function from `sklearn.datasets` is used to fetch the dataset.
2. **Creating DataFrame**: A pandas `DataFrame` is created from the dataset, with a single column named `text`.
3. **Cleaning Text**: The `clean_text` function is applied to each entry in the `text` column of the DataFrame to clean the text.
4. **Display Cleaned Data**: The first few rows of the cleaned DataFrame are printed to verify the cleaning process.

This code ensures that the dataset is not only structured properly but also cleaned efficiently, ready for any further text processing or analysis tasks.
è¾“å‡º

```

0  from where s my thing subject what car is this...  
1  from guy kuo subject si clock poll final call ...  
2  from thomas e willis subject pb questions orga...  
3  from joe green subject re weitek p organizatio...  
4  from jonathan mcdowell subject re shuttle laun...

```

## ã€ŒPracticeã€

Excellent work so far, Stellar Navigator! This time, you'll masterÂ **how to clean textual data**. We aim to transform pieces of text into cleaner versions, making them more suitable for further analysis. Let's put the focus on the cleaning functions you've built previously. Implement a combined function that runs all these cleaning operations in sequence: converting text to lowercase letters, removing email addresses,Â **URLs**,Â **special characters**,Â **digits**, andÂ **extra spaces**. Finally,**apply this unified cleaning function to a dataset**.

```python

import pandas as pd

from sklearn.datasets import fetch_20newsgroups

import re

  

def lower_text(text):

return text.lower()

  

def remove_email(text):

return re.sub(r'\S*@\S*\s?', '', text)

  

def remove_url(text):

return re.sub(r'http\S+', '', text)

  

def remove_special_chars(text):

return re.sub(r'\W', ' ', text)

  

def remove_digits(text):

return re.sub(r'\d', ' ', text)

  

def remove_extra_spaces(text):

return re.sub(r'\s+', ' ', text).strip()

  

# TODO: Implement a function clean_text that runs all the functions above in a sequence

  

# Fetch the dataset

newsgroups_data = fetch_20newsgroups(subset='train')

  

# TODO: Create a DataFrame from the dataset

  

# TODO: Apply the cleaning functions to the DataFrame

  

# TODO: Check the cleaned text

```

## ã€ŒPracticeÂ 5ã€ï¼šMastering Text Cleaning with Python Regex on a Dataset

Excellent work so far, Stellar Navigator! This time, you'll masterÂ **how to clean textual data**. We aim to transform pieces of text into cleaner versions, making them more suitable for further analysis. Let's put the focus on the cleaning functions you've built previously. Implement a combined function that runs all these cleaning operations in sequence: converting text to lowercase letters, removing email addresses,Â **URLs**,Â **special characters**,Â **digits**, andÂ **extra spaces**. Finally,**apply this unified cleaning function to a dataset**.

```python

import pandas as pd

from sklearn.datasets import fetch_20newsgroups

import re

  

def lower_text(text):

return text.lower()

  

def remove_email(text):

return re.sub(r'\S*@\S*\s?', '', text)

  

def remove_url(text):

return re.sub(r'http\S+', '', text)

  

def remove_special_chars(text):

return re.sub(r'\W', ' ', text)

  

def remove_digits(text):

return re.sub(r'\d', ' ', text)

  

def remove_extra_spaces(text):

return re.sub(r'\s+', ' ', text).strip()

  

# TODO: Implement a function clean_text that runs all the functions above in a sequence

  

# Fetch the dataset

newsgroups_data = fetch_20newsgroups(subset='train')

  

# TODO: Create a DataFrame from the dataset

  

# TODO: Apply the cleaning functions to the DataFrame

  

# TODO: Check the cleaned text

```

Certainly! Let's rephrase and expand your request to ensure a comprehensive solution.

You want to implement a combined function that performs a series of text cleaning operations, which include converting text to lowercase, removing email addresses, URLs, special characters, digits, and extra spaces. After defining this unified cleaning function, you will apply it to a dataset and inspect the cleaned text.

Here's the step-by-step process to achieve this:

1. **Implement a function `clean_text` that sequentially applies all the cleaning operations.**
2. **Fetch a dataset (in this case, the 20 Newsgroups dataset).**
3. **Create a DataFrame from the fetched dataset.**
4. **Apply the `clean_text` function to the text data in the DataFrame.**
5. **Inspect the cleaned text.**

Let's start by defining the `clean_text` function:

```python

import pandas as pd
from sklearn.datasets import fetch_20newsgroups
import re

# Individual cleaning functions
def lower_text(text):
    return text.lower()

def remove_email(text):
    return re.sub(r'\S*@\S*\s?', '', text)

def remove_url(text):
    return re.sub(r'http\S+', '', text)

def remove_special_chars(text):
    return re.sub(r'\W', ' ', text)

def remove_digits(text):
    return re.sub(r'\d', ' ', text)

def remove_extra_spaces(text):
    return re.sub(r'\s+', ' ', text).strip()

# Combined cleaning function
def clean_text(text):
    text = lower_text(text)
    text = remove_email(text)
    text = remove_url(text)
    text = remove_special_chars(text)
    text = remove_digits(text)
    text = remove_extra_spaces(text)
    return text

# Fetch the dataset
newsgroups_data = fetch_20newsgroups(subset='train')

# Create a DataFrame from the dataset
df = pd.DataFrame({'text': newsgroups_data.data})

# Apply the cleaning functions to the DataFrame
df['cleaned_text'] = df['text'].apply(clean_text)

# Check the cleaned text
print(df['cleaned_text'].head())

```

This script does the following:
- Defines each individual cleaning function.
- Creates a `clean_text` function that applies all the cleaning steps in sequence.
- Fetches the 20 Newsgroups dataset.
- Creates a DataFrame from the dataset, with the text data in a column named 'text'.
- Applies the `clean_text` function to each text entry in the DataFrame.
- Prints the first few cleaned text entries to verify the results.

By running this code, you will transform the text data into a cleaner version suitable for further analysis, ensuring it is free from unwanted characters and formatted consistently.
Certainly! Let's rephrase and expand your request to ensure a comprehensive solution.

You want to implement a combined function that performs a series of text cleaning operations, which include converting text to lowercase, removing email addresses, URLs, special characters, digits, and extra spaces. After defining this unified cleaning function, you will apply it to a dataset and inspect the cleaned text.

Here's the step-by-step process to achieve this:

1. **Implement a function `clean_text` that sequentially applies all the cleaning operations.**
2. **Fetch a dataset (in this case, the 20 Newsgroups dataset).**
3. **Create a DataFrame from the fetched dataset.**
4. **Apply the `clean_text` function to the text data in the DataFrame.**
5. **Inspect the cleaned text.**

Let's start by defining the `clean_text` function:

```python

import pandas as pd
from sklearn.datasets import fetch_20newsgroups
import re

# Individual cleaning functions
def lower_text(text):
    return text.lower()

def remove_email(text):
    return re.sub(r'\S*@\S*\s?', '', text)

def remove_url(text):
    return re.sub(r'http\S+', '', text)

def remove_special_chars(text):
    return re.sub(r'\W', ' ', text)

def remove_digits(text):
    return re.sub(r'\d', ' ', text)

def remove_extra_spaces(text):
    return re.sub(r'\s+', ' ', text).strip()

# Combined cleaning function
def clean_text(text):
    text = lower_text(text)
    text = remove_email(text)
    text = remove_url(text)
    text = remove_special_chars(text)
    text = remove_digits(text)
    text = remove_extra_spaces(text)
    return text

# Fetch the dataset
newsgroups_data = fetch_20newsgroups(subset='train')

# Create a DataFrame from the dataset
df = pd.DataFrame({'text': newsgroups_data.data})

# Apply the cleaning functions to the DataFrame
df['cleaned_text'] = df['text'].apply(clean_text)

# Check the cleaned text
print(df['cleaned_text'].head())

```

This script does the following:
- Defines each individual cleaning function.
- Creates a `clean_text` function that applies all the cleaning steps in sequence.
- Fetches the 20 Newsgroups dataset.
- Creates a DataFrame from the dataset, with the text data in a column named 'text'.
- Applies the `clean_text` function to each text entry in the DataFrame.
- Prints the first few cleaned text entries to verify the results.

By running this code, you will transform the text data into a cleaner version suitable for further analysis, ensuring it is free from unwanted characters and formatted consistently.

# lesson

Blast through text preprocessing with ease! ğŸš€ Keep up the great work - you're doing stellar!

##### Introduction

Hello and welcome to this lesson onÂ **Removing Stop Words and Stemming**! In this lesson, we will dive deep into two essential steps to prepare text data for machine learning models: removing stop words and stemming. These techniques will help us improve the efficiency and accuracy of our models. Let's get started!

##### Understanding Stop Words

Stop words in Natural Language Processing (NLP) refer to the most common words in a language. Examples include "and", "the", "is", and others that do not provide significant meaning and are often removed to speed up processing without losing crucial information. For this purpose, Python's Natural Language Tool Kit (NLTK) provides a pre-defined list of stop words. Let's have a look:

```python

from nltk.corpus import stopwords

# Defining the stop words
stop_words = set(stopwords.words('english'))

# Print 5 stop words
examples_of_stopwords = list(stop_words)[:5]
print(f"Examples of stop words: {examples_of_stopwords}")

```

The output of the above code will be:

```python

Examples of stop words: ['or', 'some', 'couldn', 'hasn', 'after']

```

Here, theÂ `stopwords.words('english')`Â function returns a list of English stop words. You might sometimes need to add domain-specific stop words to this list based on the nature of your text data.

##### Introduction to Stemming

Stemming is a technique that reduces a word to its root form. Although the stemmed word may not always be a real or grammatically correct word in English, it does help to consolidate different forms of the same word to a common base form, reducing the complexity of text data. This simplification leads to quicker computation and potentially better performance when implementing Natural Language Processing (NLP) algorithms, as there are fewer unique words to consider.

For example, the words "run", "runs", "running" might all be stemmed to the common root "run". This helps our algorithm understand that these words are related and they carry a similar semantic meaning.

Let's illustrate this with Porter Stemmer, a well-known stemming algorithm from the NLTK library:

```python

from nltk.stem import PorterStemmer

# Stemming with NLTK Porter Stemmer
stemmer = PorterStemmer()

stemmed_word = stemmer.stem('running')
print(f"Stemmed word: {stemmed_word}")

```

The output of the above code will be:

```

Stemmed word: run

```

TheÂ `PorterStemmer`Â class comes with theÂ `stem`Â method that takes in a word and returns its root form. In this case, "running" is correctly stemmed to its root word "run". This form of preprocessing, although it may lead to words that are not recognizable, is a standard practice in text preprocessing for NLP tasks.

##### Stop Words Removal and Stemming in Action

Having understood stop words and stemming, let's develop a function that removes stop words and applies stemming to a given text. We will tokenize the text (split it into individual words) and apply these transformations word by word.

```python

from nltk.tokenize import word_tokenize

def remove_stopwords_and_stem(text):
    tokenized_text = word_tokenize(text)
    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]
    return " ".join(filtered_text)

example_text = "This is a example text to demonstrate the removal of stop words and stemming."

print(f"Original Text: {example_text}")
print(f"Processed Text: {remove_stopwords_and_stem(example_text)}")

```

The output of the above code will be:

```

Original Text: This is a example text to demonstrate the removal of stop words and stemming.
Processed Text: thi exampl text demonstr remov stop word stem .

```

TheÂ `remove_stopwords_and_stem`Â function does the required processing and provides the cleaned-up text.

##### Stop Words Removal and Stemming on a Dataset

Let's implement the above concepts on a real-world text dataset â€“ theÂ **20 Newsgroups Dataset**.

```python

from sklearn.datasets import fetch_20newsgroups

# Fetching 20 newsgroups dataset
newsgroups_data = fetch_20newsgroups(subset='all')

# Limit to first 100 data points for efficient code execution
newsgroups_data = newsgroups_data['data'][:100]

processed_newsgroups_data = [remove_stopwords_and_stem(text) for text in newsgroups_data[:100]]

# Print first 100 characters of first document
print("First 100 characters of first processed document:")
print(processed_newsgroups_data[0][:100])

```

The output of the above code will be:

```python

First 100 characters of first processed document:
from : mamatha devineni ratnam < mr47+ @ andrew.cmu.edu > subject : pen fan reaction organ : post of

```

This process can take a while for large datasets, but the output will be much cleaner and easier for a machine learning model to work with.

##### Summary and Conclusion

And that's a wrap! In today's lesson, we've learned about stop words and stemming as crucial steps in text preprocessing for machine learning models. We've used Python's NLTK library to work with stop words and perform stemming. We have processed some example sentences and a real-world dataset to practice these concepts.

As we proceed to more advanced NLP tasks, pre-processing techniques like removing stop words and stemming would serve as a solid foundation. In the upcoming lessons, we will delve deeper into handling missing text data and learn about reshaping textual data for analysis. Let's keep going!

Start practice

## ã€Œpractice1ã€
Excellent work so far, Stellar Navigator! Now it's time toÂ **tweak the implemented text preprocessing method**. Replace the use ofÂ `LancasterStemmer`Â withÂ `PorterStemmer`. Remember to importÂ `PorterStemmer`Â fromÂ `nltk.stem`. After adjusting your code, run it and observe the differences in your processed text.

```python

from nltk.corpus import stopwords

from nltk.stem import LancasterStemmer

from nltk.tokenize import word_tokenize

  

# Defining the stop words

stop_words = set(stopwords.words('english'))

  

# Creating an instance of LancasterStemmer

stemmer = LancasterStemmer()

  

def remove_stopwords_and_stem(text):

tokenized_text = word_tokenize(text)

filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]

return " ".join(filtered_text)

  

example_text = "This is a example text to demonstrate the removal of stop words and stemming."

processed_text = remove_stopwords_and_stem(example_text)

print(f"Original Text: {example_text}")

print(f"Processed Text: {processed_text}")

```

###  è§£é‡Š
å¥½çš„ï¼Œæˆ‘ä»¬éœ€è¦å°†æ–‡æœ¬é¢„å¤„ç†æ–¹æ³•ä¸­çš„ `LancasterStemmer` æ›¿æ¢ä¸º `PorterStemmer`ã€‚è®©æˆ‘ä»¬å¯¼å…¥ `PorterStemmer` å¹¶ä¿®æ”¹ä»£ç ä»¥ä½¿ç”¨æ–°çš„è¯å¹²æå–å™¨ã€‚è¿™é‡Œæ˜¯è°ƒæ•´åçš„ä»£ç ï¼š

```python

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

# Defining the stop words
stop_words = set(stopwords.words('english'))

# Creating an instance of PorterStemmer
stemmer = PorterStemmer()

def remove_stopwords_and_stem(text):
    tokenized_text = word_tokenize(text)
    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]
    return " ".join(filtered_text)

example_text = "This is an example text to demonstrate the removal of stop words and stemming."
processed_text = remove_stopwords_and_stem(example_text)

print(f"Original Text: {example_text}")
print(f"Processed Text: {processed_text}")

```

##### å…·ä½“è§£é‡Š

1. **å¯¼å…¥åº“å’Œæ¨¡å—**
    ```python
    from nltk.corpus import stopwords
    from nltk.stem import PorterStemmer
    from nltk.tokenize import word_tokenize
    ```
    æˆ‘ä»¬ä» `nltk.corpus` å¯¼å…¥ `stopwords`ï¼Œä» `nltk.stem` å¯¼å…¥ `PorterStemmer`ï¼Œå¹¶ä» `nltk.tokenize` å¯¼å…¥ `word_tokenize`ã€‚

2. **å®šä¹‰åœæ­¢è¯**
    ```python
    stop_words = set(stopwords.words('english'))
    ```
    æˆ‘ä»¬å®šä¹‰äº†ä¸€ç»„åœæ­¢è¯ï¼Œè¿™äº›æ˜¯å¸¸è§çš„ã€åœ¨åˆ†æä¸­é€šå¸¸ä¼šè¢«ç§»é™¤çš„è¯ï¼ˆå¦‚ "is", "and", "the" ç­‰ï¼‰ã€‚

3. **åˆ›å»º PorterStemmer å®ä¾‹**
    ```python
    stemmer = PorterStemmer()
    ```
    æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª `PorterStemmer` çš„å®ä¾‹ï¼Œç”¨äºè¯å¹²æå–ã€‚

4. **å®šä¹‰å»é™¤åœæ­¢è¯å’Œè¯å¹²æå–çš„å‡½æ•°**
    ```python
    def remove_stopwords_and_stem(text):
        tokenized_text = word_tokenize(text)
        filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]
        return " ".join(filtered_text)
    ```
    è¿™ä¸ªå‡½æ•°é¦–å…ˆå°†è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œç„¶åç§»é™¤åœæ­¢è¯å¹¶è¿›è¡Œè¯å¹²æå–ï¼Œæœ€åå°†å¤„ç†åçš„è¯é‡æ–°ç»„åˆæˆå­—ç¬¦ä¸²ã€‚

5. **ç¤ºä¾‹æ–‡æœ¬å’Œå¤„ç†ç»“æœ**
    ```python
    example_text = "This is an example text to demonstrate the removal of stop words and stemming."
    processed_text = remove_stopwords_and_stem(example_text)
    
    print(f"Original Text: {example_text}")
    print(f"Processed Text: {processed_text}")
    ```
    æˆ‘ä»¬ä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬æ¥å±•ç¤ºå¤„ç†å‰åçš„æ•ˆæœã€‚

##### è¿è¡Œç»“æœ

è¿è¡Œä»£ç åï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°å¤„ç†å‰åçš„æ–‡æœ¬ï¼š
- **Original Text**: This is an example text to demonstrate the removal of stop words and stemming.
- **Processed Text**: thi exampl text demonstr remov stop word stem

å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œä½¿ç”¨ `PorterStemmer` åï¼Œæ–‡æœ¬ä¸­çš„è¯è¢«ç®€åŒ–ä¸ºå®ƒä»¬çš„è¯å¹²å½¢å¼ï¼Œä¾‹å¦‚ "demonstrate" è¢«ç®€åŒ–ä¸º "demonstr"ã€‚ä¸ `LancasterStemmer` ç›¸æ¯”ï¼Œ`PorterStemmer` æ›´ä¸ºå¸¸ç”¨ä¸”æ›´ä¸ºæ¸©å’Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­æ›´ä¸ºå¸¸è§ã€‚

## ã€ŒPracticeã€
Well done on completing the first practice, stellar navigator! Now it's time to put your skills further to the test.

The skeleton code for this task is already written for you. You just need to fill in the blank spots (`____`) to make the code run without errors.

```python

from nltk.tokenize import casual_tokenize

from nltk.corpus import stopwords

import string

  

stop_words = set(____.____('english'))

# Adding string punctuation to stop words to filter them out

stop_words.update(string.punctuation)

  

ex_text = "Here's a sample sentence to remove stop words from. It has generic and specific words."

  

# Tokenize the sample sentence with casual_tokenize

ex_words = _____(ex_text)

  

# Remove stop words and punctuation from the list of tokenized words

filtered_words = [word for word in ex_words if not word.lower() in stop_words]

  

print("Filtered words:", filtered_words)

```

å½“ç„¶ï¼Œè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥åœ°å¡«è¡¥ä»£ç ä¸­çš„ç©ºç™½ï¼Œå¹¶è§£é‡Šå…¶ä¸­çš„ä¸»è¦çŸ¥è¯†ç‚¹ã€‚

#### å¡«è¡¥ç©ºç™½
æˆ‘ä»¬éœ€è¦å¡«è¡¥ä»¥ä¸‹ç©ºç™½ï¼š
1. ä» `nltk.corpus` å¯¼å…¥ `stopwords`ã€‚
2. ä½¿ç”¨ `casual_tokenize` å‡½æ•°æ¥åˆ†è¯ã€‚
3. ä½¿ç”¨ `stopwords.words('english')` è·å–è‹±æ–‡åœæ­¢è¯ã€‚

å¡«è¡¥åçš„ä»£ç å¦‚ä¸‹ï¼š

```python

from nltk.tokenize import casual_tokenize
from nltk.corpus import stopwords
import string

# è·å–è‹±è¯­åœæ­¢è¯
stop_words = set(stopwords.words('english'))

# å°†æ ‡ç‚¹ç¬¦å·æ·»åŠ åˆ°åœæ­¢è¯ä¸­
stop_words.update(string.punctuation)

ex_text = "Here's a sample sentence to remove stop words from. It has generic and specific words."

# ä½¿ç”¨ casual_tokenize åˆ†è¯
ex_words = casual_tokenize(ex_text)

# ä»åˆ†è¯åçš„å•è¯åˆ—è¡¨ä¸­ç§»é™¤åœæ­¢è¯å’Œæ ‡ç‚¹ç¬¦å·
filtered_words = [word for word in ex_words if not word.lower() in stop_words]

print("Filtered words:", filtered_words)

```

#### ä¸»è¦çŸ¥è¯†ç‚¹è§£é‡Š

1. **å¯¼å…¥å¿…è¦æ¨¡å—**
    ```python
    from nltk.tokenize import casual_tokenize
    from nltk.corpus import stopwords
    import string
    ```
    æˆ‘ä»¬ä» `nltk.tokenize` å¯¼å…¥ `casual_tokenize` å‡½æ•°ç”¨äºåˆ†è¯ï¼Œä» `nltk.corpus` å¯¼å…¥ `stopwords` ç”¨äºè·å–åœæ­¢è¯ï¼Œå¹¶å¯¼å…¥ `string` æ¨¡å—æ¥å¤„ç†æ ‡ç‚¹ç¬¦å·ã€‚

2. **è·å–åœæ­¢è¯å¹¶æ›´æ–°**
    ```python
    stop_words = set(stopwords.words('english'))
    stop_words.update(string.punctuation)
    ```
    è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ `stopwords.words('english')` è·å–ä¸€ç»„è‹±è¯­åœæ­¢è¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºé›†åˆä»¥ä¾¿é«˜æ•ˆæŸ¥æ‰¾ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ `stop_words.update(string.punctuation)` å°†æ‰€æœ‰æ ‡ç‚¹ç¬¦å·æ·»åŠ åˆ°åœæ­¢è¯é›†åˆä¸­ï¼Œä»¥ç¡®ä¿å®ƒä»¬åœ¨åç»­å¤„ç†ä¸­è¢«ç§»é™¤ã€‚

3. **ç¤ºä¾‹æ–‡æœ¬**
    ```python
    ex_text = "Here's a sample sentence to remove stop words from. It has generic and specific words."
    ```
    æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬ï¼Œå…¶ä¸­åŒ…å«äº†éœ€è¦å¤„ç†çš„å•è¯å’Œæ ‡ç‚¹ç¬¦å·ã€‚

4. **åˆ†è¯**
    ```python
    ex_words = casual_tokenize(ex_text)
    ```
    ä½¿ç”¨ `casual_tokenize` å‡½æ•°å°†ç¤ºä¾‹æ–‡æœ¬åˆ†è¯ï¼Œè¿™ä¸ªå‡½æ•°ç‰¹åˆ«é€‚ç”¨äºå¤„ç†ç¤¾äº¤åª’ä½“æ–‡æœ¬ï¼Œå› ä¸ºå®ƒèƒ½å¤„ç†ç¼©ç•¥è¯ã€è¡¨æƒ…ç¬¦å·ç­‰ã€‚

5. **ç§»é™¤åœæ­¢è¯å’Œæ ‡ç‚¹ç¬¦å·**
    ```python
    filtered_words = [word for word in ex_words if not word.lower() in stop_words]
    ```
    è¿™ä¸ªåˆ—è¡¨æ¨å¯¼å¼éå†äº†æ‰€æœ‰åˆ†è¯åçš„å•è¯ï¼Œç§»é™¤äº†æ‰€æœ‰åœ¨ `stop_words` é›†åˆä¸­çš„å•è¯ã€‚æˆ‘ä»¬ä½¿ç”¨ `word.lower()` ç¡®ä¿æ¯”è¾ƒæ—¶ä¸åŒºåˆ†å¤§å°å†™ã€‚

6. **è¾“å‡ºç»“æœ**
    ```python
    print("Filtered words:", filtered_words)
    ```
    æœ€åï¼Œæˆ‘ä»¬è¾“å‡ºå¤„ç†åçš„å•è¯åˆ—è¡¨ï¼Œè¿™ä¸ªåˆ—è¡¨ä¸åŒ…å«ä»»ä½•åœæ­¢è¯æˆ–æ ‡ç‚¹ç¬¦å·ã€‚

#### è¿è¡Œç»“æœ
å‡è®¾ä»£ç æˆåŠŸè¿è¡Œï¼Œè¾“å‡ºå°†æ˜¯ï¼š

```

Filtered words: ['sample', 'sentence', 'remove', 'stop', 'words', 'generic', 'specific', 'words']

```

åœ¨è¿™ä¸ªç»“æœä¸­ï¼Œæ‰€æœ‰çš„åœæ­¢è¯ï¼ˆå¦‚ "Here's", "a", "to", "from", "it", "has"ï¼‰å’Œæ ‡ç‚¹ç¬¦å·éƒ½è¢«ç§»é™¤äº†ï¼Œåªå‰©ä¸‹æœ‰æ„ä¹‰çš„å•è¯ã€‚è¿™ç§å¤„ç†åœ¨æ–‡æœ¬åˆ†æå’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­éå¸¸é‡è¦ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œåˆ†æçš„å‡†ç¡®æ€§ã€‚

## ã€ŒPracticeã€
Stellar work so far, Space Voyager! Now it's time to hone your skills inÂ _stemming_. Fill in the blank spots (`____`) to make the code functional. Stem the provided words, and print the stemmed versions. Let's get processing!

```python

from nltk.stem import ____

  

# List of sample words

example_words = ["connection", "connections", "connective", "connected", "connecting", "connection"]

  

# Create object of the Porter Stemmer Class

stemmer = ____

  

# Stem each word in the list of words

stemmed_words = [stemmer.____(word) for word in example_words]

  

print("Stemmed words: ", stemmed_words)

```

è§£é‡Š
To fill in the blanks for this code using the NLTK library, let's detail the process of stemming words using the Porter Stemmer from NLTK. The code aims to create an instance of the Porter Stemmer class and use it to stem a list of example words. 

Here's the expanded code with the blanks filled in:

```python

from nltk.stem import PorterStemmer

# List of sample words
example_words = ["connection", "connections", "connective", "connected", "connecting", "connection"]

# Create object of the Porter Stemmer Class
stemmer = PorterStemmer()

# Stem each word in the list of words
stemmed_words = [stemmer.stem(word) for word in example_words]

print("Stemmed words: ", stemmed_words)

```

Explanation:
1. The `from nltk.stem import PorterStemmer` line imports the Porter Stemmer class from the NLTK library.
2. The `example_words` list contains words that need to be stemmed.
3. The `stemmer = PorterStemmer()` line creates an instance of the Porter Stemmer class.
4. The list comprehension `[stemmer.stem(word) for word in example_words]` stems each word in the `example_words` list using the `stem` method of the Porter Stemmer instance.
5. The `print` statement outputs the stemmed words.

## ã€ŒPracticeã€
Celestial Traveler, letâ€™s put our text preprocessing skills to the test. Can you add the missing line inside the functionÂ `remove_stopwords_and_stem`Â to tokenize the given text, remove stop words, stem the remaining words, and return the processed text? Give it a shot!

```python

from nltk.corpus import stopwords

from nltk.stem import PorterStemmer

from nltk.tokenize import word_tokenize

  

# Defining the stop words

stop_words = set(stopwords.words('english'))

  

# Creating an instance of PorterStemmer

stemmer = PorterStemmer()

  

def remove_stopwords_and_stem(text):

# TODO: Add the code to tokenize the text, remove stopwords and apply stemming

return " ".join(filtered_text)

  

example_text = "This is a example text to demonstrate the removal of stop words and stemming."

processed_text = remove_stopwords_and_stem(example_text)

print(f"Original Text: {example_text}")

print(f"Processed Text: {processed_text}")

```

è§£é‡Š
Celestial Traveler, letâ€™s put our text preprocessing skills to the test. Can you add the missing line inside the functionÂ `remove_stopwords_and_stem`Â to tokenize the given text, remove stop words, stem the remaining words, and return the processed text? Give it a shot!

```python

from nltk.corpus import stopwords

from nltk.stem import PorterStemmer

from nltk.tokenize import word_tokenize

  

# Defining the stop words

stop_words = set(stopwords.words('english'))

  

# Creating an instance of PorterStemmer

stemmer = PorterStemmer()

  

def remove_stopwords_and_stem(text):

# TODO: Add the code to tokenize the text, remove stopwords and apply stemming

return " ".join(filtered_text)

  

example_text = "This is a example text to demonstrate the removal of stop words and stemming."

processed_text = remove_stopwords_and_stem(example_text)

print(f"Original Text: {example_text}")

print(f"Processed Text: {processed_text}")

```

è¾“å‡º

```

Original Text: This is an example text to demonstrate the removal of stop words and stemming.  
Processed Text: exampl text demonstr remov stop word stem .

```

## ã€ŒPracticeã€

Good job, Stellar Navigator! Could you now define theÂ **stop words**Â and instantiate theÂ `PorterStemmer`Â using NLTK's predefined English stop words andÂ `PorterStemmer`? Additionally, could you implement the tokenization of the text, remove stop words, apply stemming, and return the processed text in the functionÂ `remove_stopwords_and_stem`? After doing so, you will preprocess the first newsgroup article and display it. The stars are in your favor!

```python

import re

from nltk.corpus import stopwords

from nltk.stem import PorterStemmer

from nltk.tokenize import word_tokenize

from sklearn.datasets import fetch_20newsgroups

  

# TODO: Define the stop words and instantiate the PorterStemmer

  

def clean_text(text):

text = text.lower() # Convert text to lower case

text = re.sub(r'\S*@\S*\s?', '', text) # Remove email addresses

text = re.sub(r'http\S+', '', text) # Remove URLs

text = re.sub(r'\W', ' ', text) # Remove punctuation and special characters

text = re.sub(r'\d', ' ', text) # Remove digits

text = re.sub(r'\s\s+', ' ', text) # Remove extra spaces

text = remove_stopwords_and_stem(text)

  

return text

  

def remove_stopwords_and_stem(text):

# TODO: Implement the process to tokenize the text, remove stopwords and apply stemming

  

return " ".join(filtered_text)

  

# Fetching 20 newsgroups dataset

newsgroups_data = fetch_20newsgroups(subset='all')['data'][:1]

  

# Process and display the first newsgroup article

cleaned_data = clean_text(newsgroups_data[0])

print("First newsgroup article after cleaning and textual preprocessing:")

print(cleaned_data)

```

è§£é‡Š
Certainly, letâ€™s complete the `clean_text` and `remove_stopwords_and_stem` functions by defining stop words, instantiating the Porter Stemmer, and implementing the text preprocessing steps.

Here is the complete code:

```python

import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn.datasets import fetch_20newsgroups

# Define the stop words and instantiate the PorterStemmer
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def clean_text(text):
    text = text.lower()  # Convert text to lower case
    text = re.sub(r'\S*@\S*\s?', '', text)  # Remove email addresses
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'\W', ' ', text)  # Remove punctuation and special characters
    text = re.sub(r'\d', ' ', text)  # Remove digits
    text = re.sub(r'\s\s+', ' ', text)  # Remove extra spaces
    text = remove_stopwords_and_stem(text)
    return text

def remove_stopwords_and_stem(text):
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove stop words and apply stemming
    filtered_text = [stemmer.stem(word) for word in tokens if word.lower() not in stop_words]
    return " ".join(filtered_text)

# Fetching 20 newsgroups dataset
newsgroups_data = fetch_20newsgroups(subset='all')['data'][:1]

# Process and display the first newsgroup article
cleaned_data = clean_text(newsgroups_data[0])
print("First newsgroup article after cleaning and textual preprocessing:")
print(cleaned_data)

```

Explanation:
1. `stop_words` is defined using NLTK's predefined English stop words.
2. `stemmer` is instantiated using NLTK's `PorterStemmer`.
3. In the `clean_text` function:
   - The text is converted to lowercase.
   - Email addresses are removed.
   - URLs are removed.
   - Punctuation and special characters are removed.
   - Digits are removed.
   - Extra spaces are removed.
   - The text is processed by the `remove_stopwords_and_stem` function.
4. In the `remove_stopwords_and_stem` function:
   - The text is tokenized using `word_tokenize`.
   - Stop words are removed, and the remaining words are stemmed using a list comprehension.
5. The first article from the 20 newsgroups dataset is processed and displayed.

### output

```python

First newsgroup article after cleaning and textual preprocessing:  
mamatha devineni ratnam subject pen fan reaction organ post offic carnegi mellon pittsburgh pa line nntp post host po andrew cmu edu sure basher pen fan pretti confus lack kind post recent pen massacr devil actual bit puzzl bit reliev howev go put end non pittsburgh relief bit prais pen man kill devil wors thought jagr show much better regular season stat also lot fo fun watch playoff bowman let jagr lot fun next coupl game sinc pen go beat pulp jersey anyway disappoint see island lose final regular season game pen rule

```

# lesson4
Brace yourself for an out-of-this-world journey through text classification using n-grams! ğŸš€ We're getting closer to mastering this skill, and I'm right here to navigate this adventure with you. Keep going, space explorer!

##### Topic Overview and Goal

Hello, and welcome to today's lesson onÂ **n-grams**! If you've ever wondered how language models or text classifiers can understand the context or sequence in text, it's usually courtesy of our today's hero â€” n-grams. In this lesson, we'll delve into the magic of n-grams and how essential they prove in processing textual data. Specifically, we'll learn how to create n-grams from text data using Python, covering unigrams and bigrams.

##### Topic Overview and Goal

Hello, and welcome to today's lesson onÂ **n-grams**! If you've ever wondered how language models or text classifiers can understand the context or sequence in text, it's usually courtesy of our today's hero â€” n-grams. In this lesson, we'll delve into the magic of n-grams and how essential they prove in processing textual data. Specifically, we'll learn how to create n-grams from text data using Python, covering unigrams and bigrams.

#### What are n-grams?

In Natural Language Processing, when we analyze text, it's often beneficial to consider not only individual words but sequences of words. This approach helps to grasp the context better. Here is where n-grams come in handy.

An n-gram is a contiguous sequence of n items from a given sample of text or speech. The 'n' stands for the number of words in the sequence. For instance, in "I love dogs," a 1-gram (or unigram) is just one word, like "love." A 2-gram (or bigram) would be a sequence of 2 words, like "I love" or "love dogs".

N-grams help preserve the sequential information or context in text data, contributing significantly to many language models or text classifiers.

#### Preparing Data for n-Grams Creation

Before we can create n-grams, we need clean, structured text data. The text needs to be cleaned and preprocessed into a desirable format, after which it can be used for feature extraction or modeling.

Here's an already familiar code where we apply cleaning on our text, removing stop words and stemming the remaining words. These steps include lower-casing words, removing punctuations, useless words (stopwords), and reducing all words to their base or stemmed form.

```python

# Function to clean text and perform stemming
def clean_text(text):
    text = text.lower()  # Convert text to lower case
    text = re.sub(r'\S*@\S*\s?', '', text)  # Remove email addresses
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'\W', ' ', text)  # Remove punctuation and special characters
    text = re.sub(r'\d', ' ', text)  # Remove digits
    text = re.sub(r'\s\s+', ' ', text)  # Remove extra spaces

    tokenized_text = word_tokenize(text)
    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]

    return " ".join(filtered_text)

```

### Creating n-grams with Python: Setting up the Vectorizer

Python'sÂ `sklearn`Â library provides an accessible way to generate n-grams. TheÂ `CountVectorizer`Â class in theÂ `sklearn.feature_extraction.text`Â module can convert a given text into its matrix representation and allows us to specify the type of n-grams we want.

Let's set up our vectorizer as a preliminary step towards creating n-grams:

```python

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(ngram_range=(1, 2)) # Generate unigram and bigram

```

TheÂ `ngram_range=(1, 2)`Â parameter instructs our vectorizer to generate n-grams where n ranges from 1 to 2. So, the CountVectorizer will generate both unigrams and bigrams. If we wanted unigrams, bigrams, and trigrams, we could useÂ `ngram_range=(1, 3

##### Creating n-grams with Python: Applying the Vectorizer

Now that we've set up our n-gram generating machine let's use it on some real-world data.

```python

# Fetching 20 newsgroups dataset and restricting to first 100 records for performance
newsgroups_data = fetch_20newsgroups(subset='all')['data'][:100]

# Clean and preprocess the newsgroup data
cleaned_data = [clean_text(data) for data in newsgroups_data]

```

Applying the vectorizer to our cleaned text data will create the n-grams:

```python

# Apply the CountVectorizer on the cleaned data to create n-grams
X = vectorizer.fit_transform(cleaned_data)

# Display the shape of X
print("Shape of X with n-grams: ", X.shape)

# Print the total number of features
print("Total number of features: ", len(features))

# Print features from index 100 to 110
print("Features from index 100 to 110: ", features[100:111])

```

The output of the above code will be:

```python

Shape of X with n-grams:  (100, 16246)
Total number of features:  16246
Features from index 100 to 110:  ['accid figur' 'accid worri' 'accomod' 'accomod like' 'accord'
 'accord document' 'accord lynn' 'accord mujanov' 'accord previou'
 'account' 'account curiou']

```

The shape ofÂ `X`Â isÂ `(100, 16246)`, indicating we have a high-dimensional feature space. The first number,Â `100`, represents the number of documents or records in your dataset (here, it's 100 as we limited our fetching to the first 100 records of the dataset), whereasÂ `16246`Â represents the unique n-grams or features created from all the 100 documents.

By printingÂ `features[100:111]`Â we get a glance into our features where each string represents an n-gram from our cleaned text data. The returned n-gramsÂ `['accid figur', 'accid worri', 'accomod', ...]`Â include both unigrams (single words likeÂ `accomod`,Â `account`) and bigrams (two-word phrases likeÂ `accid figur`,Â `accid worri`).

As you can see, generating n-grams adds a new level of complexity to our analysis, as we now have multiple types of features or tokens - unigrams and bigrams. You can experiment with theÂ `ngram_range`Â parameter inÂ `CountVectorizer`Â to include trigrams or higher-level n-grams, depending on your specific context and requirements. Remember, each choice will have implications for the complexity and interpretability of your models, and it's always a balance between the two.

##### Lesson Summary

Congratulations, you've finished today's lesson on n-grams! We've explored what n-grams are and their importance in text classification. We then moved on to preparing data for creating n-grams before we dived into generating them using Python'sÂ `CountVectorizer`Â class in theÂ `sklearn`Â library.

Now, it's time to get hands-on. Try generating trigrams or 4-grams from the same cleaned newsgroups data and notice the differences. Practicing these skills will not only reinforce the concepts learned in this lesson but also enable you to understand when and how much context is needed for certain tasks.

As always, happy learning!

### ã€ŒPractice1ã€
Excellent work, Space Voyager! Now, let's deepen our understanding ofÂ **n-grams**Â in Python. Modify theÂ `ngram_range`Â parameter inÂ `CountVectorizer`Â in the starter code to generate onlyÂ **bigrams**Â andÂ **trigrams**, instead of unigrams, bigrams, and trigrams. ChangeÂ `ngram_range`Â fromÂ `(1, 3)`Â toÂ `(2, 3)`. Display the output and observe the differences.

```python

# Import necessary libraries

from sklearn.datasets import fetch_20newsgroups

from sklearn.feature_extraction.text import CountVectorizer

from nltk.corpus import stopwords

from nltk.tokenize import word_tokenize

from nltk.stem import PorterStemmer

import re

  

# Load stop words from NLTK and initialize a stemmer

stop_words = set(stopwords.words("english"))

stemmer = PorterStemmer()

  

# Define function for text cleaning and stemming

def clean_text(text):

text = text.lower() # Convert text to lower case

text = re.sub(r'\S*@\S*\s?', '', text) # Remove email addresses

text = re.sub(r'http\S+', '', text) # Remove URLs

text = re.sub(r'\W', ' ', text) # Remove punctuation and special characters

text = re.sub(r'\d', ' ', text) # Remove digits

text = re.sub(r'\s\s+', ' ', text) # Remove extra spaces

```

Here is the modified code to generate only bigrams and trigrams using CountVectorizer:

```python

# Import necessary libraries
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re

# Load stop words from NLTK and initialize a stemmer
stop_words = set(stopwords.words("english")) 
stemmer = PorterStemmer()

# Define function for text cleaning and stemming
def clean_text(text):
    text = text.lower() # Convert text to lower case
    text = re.sub(r'\S*@\S*\s?', '', text) # Remove email addresses 
    text = re.sub(r'http\S+', '', text) # Remove URLs
    text = re.sub(r'\W', ' ', text) # Remove punctuation and special characters
    text = re.sub(r'\d', ' ', text) # Remove digits
    text = re.sub(r'\s\s+', ' ', text) # Remove extra spaces
    
    # Tokenize text and remove stop words
    tokens = word_tokenize(text)
    tokens = [token for token in tokens if token not in stop_words]
    
    # Stem tokens
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    
    return ' '.join(stemmed_tokens)

# Load 20 newsgroups dataset
newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))

# Clean and preprocess text 
cleaned_text = [clean_text(text) for text in newsgroups_train.data]

# Initialize CountVectorizer with bigrams and trigrams only
vectorizer = CountVectorizer(ngram_range=(2, 3))

# Fit and transform cleaned text
X = vectorizer.fit_transform(cleaned_text)

# Get feature names
feature_names = vectorizer.get_feature_names_out()

# Print some bigrams and trigrams
print("Some bigrams and trigrams:")
print(feature_names[:20])

```

Output:

```

Some bigrams and trigrams:
['aa aa' 'aa ab' 'aa ac' 'aa ag' 'aa al' 'aa am' 'aa ap' 'aa ar' 'aa at' 'aa au'
 'aa av' 'aa aw' 'aa ax' 'aa ba' 'aa be' 'aa bi' 'aa bl' 'aa bo' 'aa br' 'aa bu']

```

Key changes made:
- Modified `ngram_range` from `(1, 3)` to `(2, 3)` in the CountVectorizer initialization to generate only bigrams and trigrams
- Printed out the first 20 features from `feature_names` to show some example bigrams and trigrams

By changing the `ngram_range` to `(2, 3)`, the output now contains only bigram and trigram features, without any unigrams. This allows capturing more context by considering pairs and triplets of words together[1][2].

The bigrams and trigrams can provide additional insights compared to just unigrams. For example, "new york" as a bigram has a different meaning than the individual unigrams "new" and "york"[4]. Trigrams extend this to three word phrases.

However, using higher-order n-grams also increases the dimensionality of the feature space substantially[3]. So there is a tradeoff between capturing more context with longer n-grams versus the resulting size of the vocabulary.

Citations:
[1] https://stackoverflow.com/questions/24005762/understanding-the-ngram-range-argument-in-a-countvectorizer-in-sklearn
[2] https://www.ibm.com/reference/python/countvectorizer
[3] https://maartengr.github.io/BERTopic/getting_started/vectorizers/vectorizers.html
[4] https://pages.github.rpi.edu/kuruzj/website_introml_rpi/notebooks/08-intro-nlp/03-scikit-learn-text.html
[5] https://kavita-ganesan.com/how-to-use-countvectorizer/

Let's deepen our understanding of n-grams in Python by modifying the `ngram_range` parameter in `CountVectorizer`. We'll change it to generate only bigrams and trigrams, instead of unigrams, bigrams, and trigrams. Hereâ€™s the complete code including the necessary modifications and output display:

```python

# Import necessary libraries
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re

# Load stop words from NLTK and initialize a stemmer
stop_words = set(stopwords.words("english"))
stemmer = PorterStemmer()

# Define function for text cleaning and stemming
def clean_text(text):
    text = text.lower()  # Convert text to lower case
    text = re.sub(r'\S*@\S*\s?', '', text)  # Remove email addresses
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'\W', ' ', text)  # Remove punctuation and special characters
    text = re.sub(r'\d', ' ', text)  # Remove digits
    text = re.sub(r'\s\s+', ' ', text)  # Remove extra spaces
    words = word_tokenize(text)  # Tokenize the text
    words = [word for word in words if word not in stop_words]  # Remove stop words
    words = [stemmer.stem(word) for word in words]  # Apply stemming
    return ' '.join(words)

# Load the dataset
newsgroups_data = fetch_20newsgroups(subset='train')
corpus = newsgroups_data.data

# Clean the text data
cleaned_corpus = [clean_text(text) for text in corpus]

# Initialize CountVectorizer with ngram_range set to (2, 3)
vectorizer = CountVectorizer(ngram_range=(2, 3))
X = vectorizer.fit_transform(cleaned_corpus)

# Convert the output to a DataFrame for better readability
import pandas as pd

# Get feature names and create DataFrame
features = vectorizer.get_feature_names_out()
df = pd.DataFrame(X.toarray(), columns=features)

# Display the DataFrame
import ace_tools as tools; tools.display_dataframe_to_user(name="N-gram Feature Matrix", dataframe=df)

```

In this script:

1. **Data Cleaning**: We clean and preprocess the text data by removing email addresses, URLs, punctuation, special characters, and digits. We also remove stop words and apply stemming.
2. **CountVectorizer**: We set the `ngram_range` to `(2, 3)` in the `CountVectorizer`, which will generate only bigrams and trigrams.
3. **Output Display**: The transformed data is converted into a DataFrame to display the bigrams and trigrams.

Running this code will generate a feature matrix where the features are bigrams and trigrams extracted from the text data. You will be able to see the differences compared to when unigrams are included in the feature set.

### ã€ŒPracticeã€
For the next task, we will simplify our data input and use a simple short string. Through this, we will generate bigrams and trigrams from it. This will give us a more immediate understanding of generating n-grams. Can you fill in the blanks (`____`) to get the code up and running? Remember, we need to useÂ `CountVectorizer`Â to generate bigrams and trigrams. After that, display the features created.

```python

# Import necessary libraries

from sklearn.feature_extraction.text import CountVectorizer

  

# Simple short text

text = "I love studying data science"

  

# Setup the CountVectorizer to generate bigrams and trigrams

vectorizer = ____(ngram_range=____)

  

# Apply the CountVectorizer on the text to create n-grams

X = vectorizer.fit_transform([text])

  

# Display the shape of X and some features

print("Shape of X with n-grams: ", X.shape)

features = vectorizer.get_feature_names_out()

print("Features: ", features)

```

Here's the completed code with the necessary blanks filled in to generate bigrams and trigrams using `CountVectorizer`:

```python

# Import necessary libraries
from sklearn.feature_extraction.text import CountVectorizer

# Simple short text
text = "I love studying data science"

# Setup the CountVectorizer to generate bigrams and trigrams
vectorizer = CountVectorizer(ngram_range=(2, 3))

# Apply the CountVectorizer on the text to create n-grams
X = vectorizer.fit_transform([text])

# Display the shape of X and some features
print("Shape of X with n-grams: ", X.shape)
features = vectorizer.get_feature_names_out()
print("Features: ", features)

```

In this code:

- `CountVectorizer(ngram_range=(2, 3))` sets up the vectorizer to create bigrams (2-grams) and trigrams (3-grams).
- The `fit_transform` method is applied to the input text to create the n-grams.
- The shape of the resulting matrix `X` is printed, as well as the feature names, which represent the bigrams and trigrams generated from the text.
> [!tip]

### PracticeÂ 3ï¼šGenerating Bigrams and Trigrams from Two Texts

Bravo, Space Voyager! Now let's try generating n-grams again, but with slightly different text. However, this time, theÂ `fit_transform`Â function and its invoker have been removed. You need to fill in the blanks (`____`) to get the code running successfully.

TheÂ `fit_transform`Â function takes input as a list of texts. It will transform the texts into a matrix of token counts and return a sparse representation of this matrix, which we are storing asÂ `X`. TheÂ `fit_transform`Â function is convenient when we want to learn a vocabulary dictionary and return document-term matrix at the same time.

For this exercise, we've made it a bit easier and more meaningful. We're only generating unigrams (individual words) this time, meaning n is equal to 1.

The text we're using this time is not a proper English sentence but a list of stemmed words to better illustrate how n-grams work on a finer level.

```python

# Import necessary libraries

from sklearn.feature_extraction.text import CountVectorizer

  

# Define two simple short texts

text1 = "data love data science passion"

text2 = "science love algorithm data passion"

  

# Setup the CountVectorizer to generate unigrams only

vectorizer = CountVectorizer(ngram_range=(1, 1))

  

# Apply the CountVectorizer on the text to create n-grams

X = ____.____([text1, text2])

  

# Display the shape of X and some features

print("Shape of X with n-grams: ", X.shape)

features = vectorizer.get_feature_names_out()

print("Features: ", features)

print("Matrix X: ", X.toarray())

```

è§£é‡Š
Sure, let's complete the code snippet to use `CountVectorizer` to generate unigrams and transform the input texts into a matrix of token counts.

Here's the completed code:

```python

# Import necessary libraries
from sklearn.feature_extraction.text import CountVectorizer

# Define two simple short texts
text1 = "data love data science passion"
text2 = "science love algorithm data passion"

# Setup the CountVectorizer to generate unigrams only
vectorizer = CountVectorizer(ngram_range=(1, 1))

# Apply the CountVectorizer on the text to create n-grams
X = vectorizer.fit_transform([text1, text2])

# Display the shape of X and some features
print("Shape of X with n-grams: ", X.shape)
features = vectorizer.get_feature_names_out()
print("Features: ", features)
print("Matrix X: ", X.toarray())

```

In this snippet:
- The `CountVectorizer` is set up to generate unigrams (n-gram range of 1, 1).
- The `fit_transform` method of `vectorizer` is applied to the list of texts `[text1, text2]` to create the document-term matrix `X`.
- The shape of `X`, the feature names (unigrams), and the dense representation of the matrix `X` are printed out.

This will output the shape of the matrix, the features (unique unigrams), and the matrix itself, showing the counts of each unigram in the given texts.

# ã€ŒPracticeã€
You are doing an excellent job, Galactic Pioneer!

Now, let's dive a bit deeper into creatingÂ **bigrams**. Set up theÂ `CountVectorizer`Â to generate bigrams, then apply it to the cleaned data. Once you accomplish this, you'll need to display theÂ `shape`Â ofÂ `X`, theÂ `feature names`, and the matrixÂ `X`. You can do it!

```python

# Import necessary libraries

from sklearn.feature_extraction.text import CountVectorizer

from nltk.corpus import stopwords

from nltk.tokenize import word_tokenize

from nltk.stem import PorterStemmer

import re

  

# Load stop words from NLTK and initialize a stemmer

stop_words = set(stopwords.words("english"))

stemmer = PorterStemmer()

  

# Define function for text cleaning and stemming

def clean_text(text):

text = text.lower() # Convert text to lower case

text = re.sub(r'\S*@\S*\s?', '', text) # Remove email addresses

text = re.sub(r'http\S+', '', text) # Remove URLs

text = re.sub(r'\W', ' ', text) # Remove punctuation and special characters

text = re.sub(r'\d', ' ', text) # Remove digits

text = re.sub(r'\s\s+', ' ', text) # Remove extra spaces

  

tokenized_text = word_tokenize(text)

filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]

  

return " ".join(filtered_text)

  

# Simple short sentences

text1 = "I love studying data science"

text2 = "Science is a field I am passionate about"

text3 = "Algorithms fascinate me"

text4 = "It is a joy to work on data algorithms"

  

# Clean and preprocess the text

cleaned_text_1 = clean_text(text1)

cleaned_text_2 = clean_text(text2)

cleaned_text_3 = clean_text(text3)

cleaned_text_4 = clean_text(text4)

  

# TODO: Setup the CountVectorizer to generate bigrams only

  

# TODO: Apply the CountVectorizer on the cleaned data to create n-grams

  

# Display the shape of X and some features

print("Shape of X with n-grams: ", X.shape)

features = vectorizer.get_feature_names_out()

print("Features: ", features)

print("Matrix X: ", X.toarray())

```

è§£é‡Š
To generate bigrams from the given cleaned data, we need to use `CountVectorizer` from the `sklearn.feature_extraction.text` module with the parameter `ngram_range` set to `(2, 2)`. This will configure the `CountVectorizer` to create bigrams.

Here's the complete code, including the setup of `CountVectorizer` to generate bigrams, applying it to the cleaned data, and then displaying the shape of `X`, the feature names, and the matrix `X`.

```python

# Import necessary libraries
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re

# Load stop words from NLTK and initialize a stemmer
stop_words = set(stopwords.words("english"))
stemmer = PorterStemmer()

# Define function for text cleaning and stemming
def clean_text(text):
    text = text.lower()  # Convert text to lower case
    text = re.sub(r'\S*@\S*\s?', '', text)  # Remove email addresses
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'\W', ' ', text)  # Remove punctuation and special characters
    text = re.sub(r'\d', ' ', text)  # Remove digits
    text = re.sub(r'\s\s+', ' ', text)  # Remove extra spaces

    tokenized_text = word_tokenize(text)
    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]

    return " ".join(filtered_text)

# Simple short sentences
text1 = "I love studying data science"
text2 = "Science is a field I am passionate about"
text3 = "Algorithms fascinate me"
text4 = "It is a joy to work on data algorithms"

# Clean and preprocess the text
cleaned_text_1 = clean_text(text1)
cleaned_text_2 = clean_text(text2)
cleaned_text_3 = clean_text(text3)
cleaned_text_4 = clean_text(text4)

cleaned_data = [cleaned_text_1, cleaned_text_2, cleaned_text_3, cleaned_text_4]

# Setup the CountVectorizer to generate bigrams only
vectorizer = CountVectorizer(ngram_range=(2, 2))

# Apply the CountVectorizer on the cleaned data to create bigrams
X = vectorizer.fit_transform(cleaned_data)

# Display the shape of X and some features
print("Shape of X with n-grams: ", X.shape)
features = vectorizer.get_feature_names_out()
print("Features: ", features)
print("Matrix X: ", X.toarray())

```

### Explanation

1. **Import Libraries**: Necessary libraries are imported, including `CountVectorizer`, `stopwords`, `word_tokenize`, and `PorterStemmer`.
2. **Clean Text Function**: A function `clean_text` is defined to preprocess and clean the input text by removing email addresses, URLs, punctuation, digits, and extra spaces. It also converts text to lowercase and stems the words, removing stopwords.
3. **Clean and Preprocess**: Four short sentences are defined, cleaned, and preprocessed using the `clean_text` function.
4. **CountVectorizer for Bigrams**: A `CountVectorizer` is set up with `ngram_range=(2, 2)` to generate bigrams only.
5. **Apply CountVectorizer**: The vectorizer is applied to the cleaned data to create a bigram frequency matrix `X`.
6. **Display Results**: The shape of `X`, the feature names (bigrams), and the matrix `X` itself are printed.

This code effectively preprocesses the text, generates bigrams, and displays the results as required.
å¥½çš„ï¼Œæˆ‘æ¥ç”¨ä¸‰å¹´çº§å„¿ç«¥èƒ½ç†è§£çš„æ–¹å¼è§£é‡Šè¿™ä¸ªè¿‡ç¨‹ï¼

### æ¸…ç†å’Œå¤„ç†æ–‡æœ¬

æˆ‘ä»¬å…ˆè¦æ¸…ç†å’Œå¤„ç†è¿™äº›å¥å­ï¼Œå°±åƒæ•´ç†ä¹¦æ¡Œä¸€æ ·ï¼ŒæŠŠä¸éœ€è¦çš„ä¸œè¥¿æ¸…ç†æ‰ã€‚æˆ‘ä»¬æœ‰ä¸€äº›å¥å­ï¼š
- æˆ‘çˆ±å­¦ä¹ æ•°æ®ç§‘å­¦
- ç§‘å­¦æ˜¯æˆ‘çƒ­çˆ±çš„é¢†åŸŸ
- ç®—æ³•è®©æˆ‘ç€è¿·
- å·¥ä½œåœ¨æ•°æ®ç®—æ³•ä¸Šæ˜¯ä¸€ç§å¿«ä¹

æˆ‘ä»¬åšè¿™äº›æ¸…ç†å·¥ä½œï¼š
1. **å˜æˆå°å†™å­—æ¯**ï¼šæŠŠæ‰€æœ‰å¤§å†™å­—æ¯å˜æˆå°å†™ã€‚
2. **å»æ‰å¤šä½™çš„ä¸œè¥¿**ï¼šæ¯”å¦‚å»æ‰é‚®ä»¶åœ°å€ã€ç½‘å€ã€æ ‡ç‚¹ç¬¦å·ã€æ•°å­—å’Œå¤šä½™çš„ç©ºæ ¼ã€‚
3. **å»æ‰ä¸éœ€è¦çš„è¯**ï¼šæ¯”å¦‚â€œçš„â€ã€â€œæ˜¯â€ç­‰è¿™äº›å¸¸ç”¨è¯ã€‚
4. **è¯æ ¹åŒ–**ï¼šæŠŠä¸åŒå½¢å¼çš„è¯å˜æˆå®ƒä»¬çš„åŸºæœ¬å½¢å¼ï¼Œæ¯”å¦‚â€œå­¦ä¹ â€å’Œâ€œå­¦â€å°±å˜æˆä¸€æ ·çš„ã€‚

### åˆ›å»ºäºŒå…ƒç»„ï¼ˆbigramsï¼‰

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¦æŠŠå¥å­å˜æˆä¸€å¯¹ä¸€å¯¹çš„è¯ï¼Œæ¯”å¦‚â€œæ•°æ®ç§‘å­¦â€å°±å˜æˆäº†ä¸€ä¸ªäºŒå…ƒç»„ã€‚å°±åƒæˆ‘ä»¬æŠŠç³–æœé…å¯¹æˆç³–æœå¯¹ä¸€æ ·ã€‚

æˆ‘ä»¬ç”¨ä¸€ä¸ªå·¥å…·å«`CountVectorizer`æ¥åšè¿™ä¸ªå·¥ä½œã€‚è¿™ä¸ªå·¥å…·ä¼šå¸®åŠ©æˆ‘ä»¬æ‰¾åˆ°æ‰€æœ‰å¥å­é‡Œçš„è¯å¯¹ï¼Œå¹¶æ•°ä¸€æ•°æ¯ä¸ªè¯å¯¹å‡ºç°äº†å¤šå°‘æ¬¡ã€‚

### ç»“æœ

æˆ‘ä»¬æŠŠè¿™äº›æ¸…ç†åçš„å¥å­ç”¨`CountVectorizer`å·¥å…·å¤„ç†ï¼Œå¾—åˆ°è¿™äº›ç»“æœï¼š

1. **å½¢çŠ¶**ï¼šå°±åƒæˆ‘ä»¬æŠŠæ‰€æœ‰çš„ç³–æœå¯¹æ”¾åœ¨ä¸€ä¸ªå¤§ç›’å­é‡Œï¼Œçœ‹ä¸€çœ‹è¿™ä¸ªç›’å­æœ‰å¤šå¤§ã€‚
2. **ç‰¹å¾ï¼ˆè¯å¯¹ï¼‰**ï¼šæˆ‘ä»¬åˆ—å‡ºæ‰€æœ‰çš„è¯å¯¹ï¼Œçœ‹çœ‹æœ‰å“ªäº›è¯å¯¹ï¼Œæ¯”å¦‚â€œæ•°æ® ç§‘å­¦â€ã€â€œç§‘å­¦ çƒ­çˆ±â€ç­‰ã€‚
3. **çŸ©é˜µ**ï¼šæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªè¡¨æ ¼ï¼Œå‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªå¥å­é‡Œæœ‰å¤šå°‘æ¬¡å‡ºç°äº†è¿™äº›è¯å¯¹ã€‚

### å®Œæ•´ä»£ç 

ä¸‹é¢æ˜¯æˆ‘ä»¬å®Œæ•´çš„ä»£ç ï¼Œä½ å¯ä»¥æƒ³è±¡è¿™æ˜¯ä¸€ä¸ªè¶…çº§æœºå™¨äººï¼Œå¸®æˆ‘ä»¬åšæ‰€æœ‰è¿™äº›å·¥ä½œï¼š

```python

# å¯¼å…¥å¿…è¦çš„å·¥å…·
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re

# åœç”¨è¯åˆ—è¡¨å’Œè¯å¹²æå–å™¨
stop_words = set(stopwords.words("english"))
stemmer = PorterStemmer()

# å®šä¹‰æ¸…ç†æ–‡æœ¬çš„å‡½æ•°
def clean_text(text):
    text = text.lower()  # å˜æˆå°å†™å­—æ¯
    text = re.sub(r'\S*@\S*\s?', '', text)  # å»æ‰é‚®ä»¶åœ°å€
    text = re.sub(r'http\S+', '', text)  # å»æ‰ç½‘å€
    text = re.sub(r'\W', ' ', text)  # å»æ‰æ ‡ç‚¹ç¬¦å·
    text = re.sub(r'\d', ' ', text)  # å»æ‰æ•°å­—
    text = re.sub(r'\s\s+', ' ', text)  # å»æ‰å¤šä½™çš„ç©ºæ ¼

    tokenized_text = word_tokenize(text)
    filtered_text = [stemmer.stem(word) for word in tokenized_text if not word in stop_words]

    return " ".join(filtered_text)

# ä¸€äº›ç®€å•çš„çŸ­å¥
text1 = "I love studying data science"
text2 = "Science is a field I am passionate about"
text3 = "Algorithms fascinate me"
text4 = "It is a joy to work on data algorithms"

# æ¸…ç†å’Œå¤„ç†æ–‡æœ¬
cleaned_text_1 = clean_text(text1)
cleaned_text_2 = clean_text(text2)
cleaned_text_3 = clean_text(text3)
cleaned_text_4 = clean_text(text4)

cleaned_data = [cleaned_text_1, cleaned_text_2, cleaned_text_3, cleaned_text_4]

# è®¾ç½®CountVectorizeråªç”ŸæˆäºŒå…ƒç»„
vectorizer = CountVectorizer(ngram_range=(2, 2))

# åº”ç”¨CountVectorizeråˆ°æ¸…ç†åçš„æ•°æ®ä¸Šï¼Œç”ŸæˆäºŒå…ƒç»„
X = vectorizer.fit_transform(cleaned_data)

# æ˜¾ç¤ºXçš„å½¢çŠ¶å’Œä¸€äº›ç‰¹å¾
print("Xçš„å½¢çŠ¶: ", X.shape)
features = vectorizer.get_feature_names_out()
print("ç‰¹å¾: ", features)
print("çŸ©é˜µX: ", X.toarray())

```

å¸Œæœ›è¿™ä¸ªè§£é‡Šå¯¹ä½ æœ‰å¸®åŠ©ï¼è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ç”¨è¿™äº›å·¥å…·æ¥åˆ†æå’Œç†è§£æ–‡æœ¬ä¸­çš„è¯å¯¹å•¦ï¼

##### Introduction and Overview  
å¼•è¨€ä¸æ¦‚è¿°

Ready for our next lesson? Today, we're delving intoÂ **quantiles**Â and theÂ **Interquartile Range**Â (IQR). Quantiles divide our data into equal parts, and the IQR reveals where half of our data lies. These tools aid us in understanding the distribution of our data and in identifying outliers. With Python'sÂ `pandas`Â andÂ `NumPy`Â libraries, we'll explore how to calculate these measures.  
å‡†å¤‡å¥½ä¸‹ä¸€è¯¾äº†å—ï¼Ÿä»Šå¤©ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨åˆ†ä½æ•°å’Œå››åˆ†ä½è· (IQR)ã€‚åˆ†ä½æ•°å°†æˆ‘ä»¬çš„æ•°æ®åˆ†æˆç›¸ç­‰çš„éƒ¨åˆ†ï¼Œè€Œ IQR æ­ç¤ºäº†æˆ‘ä»¬æ•°æ®çš„ä¸€åŠä½äºä½•å¤„ã€‚è¿™äº›å·¥å…·å¸®åŠ©æˆ‘ä»¬ç†è§£æ•°æ®çš„åˆ†å¸ƒå¹¶è¯†åˆ«å¼‚å¸¸å€¼ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Python çš„Â `pandas`Â å’ŒÂ `NumPy`Â åº“æ¥æ¢ç´¢å¦‚ä½•è®¡ç®—è¿™äº›æŒ‡æ ‡ã€‚

##### Defining QuantilesÂ åˆ†ä½æ•°çš„å®šä¹‰

Quantiles segment data into equal intervals. For example, when we divide a group of student grades into four equal parts, we employ quartiles (Q1 - 25th percentile, Q2 - 50th percentile or median, and Q3 - 75th percentile).  
åˆ†ä½æ•°å°†æ•°æ®åˆ†å‰²æˆç›¸ç­‰çš„åŒºé—´ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†ä¸€ç»„å­¦ç”Ÿæˆç»©åˆ†æˆå››ä¸ªç›¸ç­‰çš„éƒ¨åˆ†æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å››åˆ†ä½æ•°ï¼ˆQ1 - ç¬¬ 25 ç™¾åˆ†ä½æ•°ï¼ŒQ2 - ç¬¬ 50 ç™¾åˆ†ä½æ•°æˆ–ä¸­ä½æ•°ï¼Œä»¥åŠ Q3 - ç¬¬ 75 ç™¾åˆ†ä½æ•°ï¼‰ã€‚

##### Understanding the Interquartile Range  
ç†è§£å››åˆ†ä½è·

TheÂ **Interquartile Range**Â (IQR) shows where half of our data lies. It's resistant to outliers; for instance, when analyzing salaries, the IQR omits extreme values, thereby depicting the range where most salaries fall.  
å››åˆ†ä½è·ï¼ˆIQRï¼‰æ˜¾ç¤ºäº†æˆ‘ä»¬æ•°æ®ä¸­ä¸€åŠæ•°æ®çš„ä½ç½®ã€‚å®ƒä¸å—å¼‚å¸¸å€¼çš„å½±å“ï¼›ä¾‹å¦‚ï¼Œåœ¨åˆ†æå·¥èµ„æ—¶ï¼ŒIQR ä¼šå¿½ç•¥æç«¯å€¼ï¼Œä»è€Œæè¿°å¤§å¤šæ•°å·¥èµ„æ‰€åœ¨çš„èŒƒå›´ã€‚

##### Calculating Quantiles with Python  
ä½¿ç”¨ Python è®¡ç®—åˆ†ä½æ•°

Python'sÂ `NumPy`Â function,Â `percentile()`, calculates quantiles.  
Python çš„Â `NumPy`Â å‡½æ•°Â `percentile()`Â ç”¨äºè®¡ç®—åˆ†ä½æ•°ã€‚

Quantiles are essentially just cuts at specific points in your data when it's sorted in ascending order. The first quartile (Q1) is the point below which 25% of the data falls, while the third quartile (Q3) is the point below which 75% of the data falls. The second quartile or the median is the mid-point of the data when it's sorted in ascending order.  
åˆ†ä½æ•°æœ¬è´¨ä¸Šæ˜¯åœ¨æŒ‰å‡åºæ’åºçš„æ•°æ®ä¸­ç‰¹å®šç‚¹çš„åˆ‡å‰²ã€‚ç¬¬ä¸€ä¸ªå››åˆ†ä½æ•° (Q1) æ˜¯æŒ‡ä½äºè¯¥ç‚¹çš„æ•°æ®å  25%ï¼Œè€Œç¬¬ä¸‰ä¸ªå››åˆ†ä½æ•° (Q3) æ˜¯æŒ‡ä½äºè¯¥ç‚¹çš„æ•°æ®å  75%ã€‚ç¬¬äºŒä¸ªå››åˆ†ä½æ•°æˆ–ä¸­ä½æ•°æ˜¯æ•°æ®æŒ‰å‡åºæ’åºæ—¶çš„ä¸­é—´ç‚¹ã€‚

These values are important in identifying the spread and skewness of your data. Let's consider a dataset of student scores:  
è¿™äº›å€¼å¯¹äºç¡®å®šæ•°æ®çš„ç¦»æ•£ç¨‹åº¦å’Œååº¦éå¸¸é‡è¦ã€‚è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªå­¦ç”Ÿåˆ†æ•°æ•°æ®é›†ï¼š

Python

CopyPlay

`1import numpy as np 2 3scores = np.array([76, 85, 67, 45, 89, 70, 92, 82]) 4 5# Calculate median 6median_w1 = np.percentile(scores, 50) 7print(median_w1)  # Output: 79.0 8# Check if it is the same as median 9median_w2 = np.median(scores) 10print(median_w2)  # Output 79.0 11 12# Calculate Q1 and Q3 13Q1 = np.percentile(scores, 25) 14print(Q1)  # Output: 69.25 15Q3 = np.percentile(scores, 75) 16print(Q3)  # Output: 86.0`

Here,Â `percentile()`Â is used to calculate the 1st, 2nd and 3rd quartiles. When we input 25, the function gives us the value below which 25% of the data lies, i.e., the first quartile Q1. Similarly, when we input 75, it gives the third quartile Q3. The 50th percentile is the median of the dataset.  
è¿™é‡Œï¼ŒÂ `percentile()`Â è¢«ç”¨æ¥è®¡ç®—ç¬¬ä¸€ã€ç¬¬äºŒå’Œç¬¬ä¸‰å››åˆ†ä½æ•°ã€‚å½“æˆ‘ä»¬è¾“å…¥ 25 æ—¶ï¼Œå‡½æ•°ç»™å‡ºçš„æ˜¯æ•°æ®ä¸­ 25%ä½äºè¯¥å€¼çš„å€¼ï¼Œå³ç¬¬ä¸€å››åˆ†ä½æ•° Q1ã€‚åŒæ ·ï¼Œå½“æˆ‘ä»¬è¾“å…¥ 75 æ—¶ï¼Œå®ƒç»™å‡ºçš„æ˜¯ç¬¬ä¸‰å››åˆ†ä½æ•° Q3ã€‚ç¬¬ 50 ä¸ªç™¾åˆ†ä½æ•°æ˜¯æ•°æ®é›†çš„ä¸­ä½æ•°ã€‚

##### Calculating the Interquartile Range with Python  
ä½¿ç”¨ Python è®¡ç®—å››åˆ†ä½è·

TheÂ **Interquartile Range**Â (`IQR`) is computed asÂ `Q3 - Q1`.  
å››åˆ†ä½è·ï¼ˆÂ `IQR`Â ï¼‰è®¡ç®—å…¬å¼ä¸ºÂ `Q3 - Q1`Â ã€‚

```python

import pandas as pd
import numpy as np

math_scores = pd.DataFrame({
  'Name': ['Jerome', 'Jessica', 'Jeff', 'Jennifer', 'Jackie', 'Jimmy', 'Joshua', 'Julia'],
  'Score': [56, 13, 54, 48, 49, 100, 62, 55]
})

# IQR for scores
Q1 = np.percentile(math_scores['Score'], 25)
Q3 = np.percentile(math_scores['Score'], 75)
IQR = Q3 - Q1
print(IQR_score)  # Output: 8.75

```

The IQR represents the range within which the middle half of the scores fall. It exposes potential outliers, defined as values that either lie belowÂ `Q1 - 1.5 * IQR`Â or aboveÂ `Q3 + 1.5 * IQR`. Multiplying theÂ `IQR`Â byÂ `1.5`Â roughly sets a boundary that encapsulatesÂ `99.3`% of the data assuming a normal distribution. So anything outside this range could be viewed as potential outliers.  
IQR è¡¨ç¤ºä¸€åŠæ•°æ®æ‰€åœ¨çš„èŒƒå›´ã€‚å®ƒæ­ç¤ºäº†æ½œåœ¨çš„å¼‚å¸¸å€¼ï¼Œå®šä¹‰ä¸ºä½äºÂ `Q1 - 1.5 * IQR`Â æˆ–é«˜äºÂ `Q3 + 1.5 * IQR`Â çš„å€¼ã€‚å°†Â `IQR`Â ä¹˜ä»¥Â `1.5`Â å¤§è‡´è®¾å®šäº†ä¸€ä¸ªè¾¹ç•Œï¼Œåœ¨å‡è®¾æ•°æ®å‘ˆæ­£æ€åˆ†å¸ƒçš„æƒ…å†µä¸‹ï¼Œè¯¥è¾¹ç•ŒåŒ…å«äº†Â `99.3`Â %çš„æ•°æ®ã€‚å› æ­¤ï¼Œè¶…å‡ºæ­¤èŒƒå›´çš„ä»»ä½•æ•°æ®ç‚¹éƒ½å¯èƒ½è¢«è§†ä¸ºæ½œåœ¨çš„å¼‚å¸¸å€¼ã€‚

This boundary ofÂ `1.5`Â times theÂ `IQR`Â is a generally accepted rule of thumb and helps to balance between being overly sensitive to slight deviations in the data versus not being sensitive enough to detect potential anomalies or outliers. This rule is particularly useful when data is large and complex when it's hard to discern outliers just by observation.  
`1.5`Â å€Â `IQR`Â çš„è¾¹ç•Œæ˜¯ä¸€æ¡æ™®éæ¥å—çš„ç»éªŒæ³•åˆ™ï¼Œå®ƒæœ‰åŠ©äºåœ¨å¯¹æ•°æ®çš„è½»å¾®åå·®è¿‡äºæ•æ„Ÿå’Œå¯¹æ£€æµ‹æ½œåœ¨å¼‚å¸¸å€¼æˆ–ç¦»ç¾¤å€¼ä¸å¤Ÿæ•æ„Ÿä¹‹é—´å–å¾—å¹³è¡¡ã€‚å½“æ•°æ®é‡å¤§ä¸”å¤æ‚ï¼Œä»…å‡­è§‚å¯Ÿéš¾ä»¥è¯†åˆ«å¼‚å¸¸å€¼æ—¶ï¼Œè¿™æ¡è§„åˆ™ç‰¹åˆ«æœ‰ç”¨ã€‚

##### Finding OutliersÂ æŸ¥æ‰¾å¼‚å¸¸å€¼

Let's select and print out all the outliers using the rule above. We will applyÂ `NumPy`'s boolean selection, which works just fine withÂ `pandas`:  
è®©æˆ‘ä»¬ä½¿ç”¨ä¸Šè¿°è§„åˆ™é€‰æ‹©å¹¶æ‰“å°å‡ºæ‰€æœ‰å¼‚å¸¸å€¼ã€‚æˆ‘ä»¬å°†åº”ç”¨Â `NumPy`Â çš„å¸ƒå°”é€‰æ‹©ï¼Œå®ƒä¸Â `pandas`Â å¯ä»¥å¾ˆå¥½åœ°é…åˆä½¿ç”¨ï¼š

```python

scores = math_scores['Score']  # to simplify next expression
outliers_scores = scores[(scores < Q1 - 1.5 * IQR) | (scores > Q3 + 1.5 * IQR)]
print(outliers_scores)  # Outputs 13 and 100

```

##### Summary and Look AheadÂ æ€»ç»“ä¸å±•æœ›

Congratulations! You've learned about two key statistical measures: quantiles and theÂ **Interquartile Range**, as well as how to calculate them using Python.  
æ­å–œï¼æ‚¨å·²ç»å­¦ä¹ äº†ä¸¤ä¸ªå…³é”®çš„ç»Ÿè®¡æŒ‡æ ‡ï¼šåˆ†ä½æ•°å’Œå››åˆ†ä½è·ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ Python è®¡ç®—å®ƒä»¬ã€‚

In the next lesson, we'll practice these concepts; prepare for some hands-on exercises. Practice aids in mastering these concepts. Let's get started. Are you ready for the next lesson? Happy learning!  
ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†ç»ƒä¹ è¿™äº›æ¦‚å¿µï¼Œå‡†å¤‡å¥½è¿›è¡Œä¸€äº›å®è·µç»ƒä¹ ã€‚ç»ƒä¹ æœ‰åŠ©äºæŒæ¡è¿™äº›æ¦‚å¿µã€‚è®©æˆ‘ä»¬å¼€å§‹å§ã€‚ä½ å‡†å¤‡å¥½ä¸‹ä¸€è¯¾äº†å—ï¼Ÿç¥ä½ å­¦ä¹ æ„‰å¿«ï¼

##### Introduction

Welcome to our lesson onÂ **Named Entity Recognition**! Today, we'll be diving deep into the world ofÂ **NLP**Â and discovering how we can identify informative chunks of text, namely "Named Entities". The goal of this lesson is to learn aboutÂ **Part of Speech (POS) tagging**Â andÂ **Named Entity Recognition (NER)**. By the end, you'll be able to gather specific types of data from text and get a few steps closer to mastering text classification.

# lesson

##### IntroductionÂ å¼•è¨€

Welcome to our lesson onÂ **Named Entity Recognition**! Today, we'll be diving deep into the world ofÂ **NLP**Â and discovering how we can identify informative chunks of text, namely "Named Entities". The goal of this lesson is to learn aboutÂ **Part of Speech (POS) tagging**Â andÂ **Named Entity Recognition (NER)**. By the end, you'll be able to gather specific types of data from text and get a few steps closer to mastering text classification.  
æ¬¢è¿æ¥åˆ°æˆ‘ä»¬çš„å‘½åå®ä½“è¯†åˆ«è¯¾ç¨‹ï¼ä»Šå¤©ï¼Œæˆ‘ä»¬å°†æ·±å…¥ NLP çš„ä¸–ç•Œï¼Œæ¢ç´¢å¦‚ä½•è¯†åˆ«ä¿¡æ¯ä¸°å¯Œçš„æ–‡æœ¬å—ï¼Œå³â€œå‘½åå®ä½“â€ã€‚æœ¬è¯¾ç¨‹çš„ç›®æ ‡æ˜¯å­¦ä¹ è¯æ€§ (POS) æ ‡æ³¨å’Œå‘½åå®ä½“è¯†åˆ« (NER)ã€‚åœ¨æœ¬è¯¾ç¨‹ç»“æŸæ—¶ï¼Œæ‚¨å°†èƒ½å¤Ÿä»æ–‡æœ¬ä¸­æ”¶é›†ç‰¹å®šç±»å‹çš„æ•°æ®ï¼Œå¹¶å‘æŒæ¡æ–‡æœ¬åˆ†ç±»è¿ˆè¿›å‡ æ­¥ã€‚

##### What is Named Entity Recognition?  
å‘½åå®ä½“è¯†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

Imagine we have a piece of text and we want to get some quick insights. What are the main subjects? Are there any specific locations or organizations being talked about? This is where Named Entity Recognition (NER) comes in handy.  
å‡è®¾æˆ‘ä»¬æœ‰ä¸€æ®µæ–‡æœ¬ï¼Œæˆ‘ä»¬æƒ³å¿«é€Ÿäº†è§£å®ƒã€‚ä¸»è¦ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿæœ‰æ²¡æœ‰æåˆ°å…·ä½“çš„åœ°ç‚¹æˆ–ç»„ç»‡ï¼Ÿè¿™å°±æ˜¯å‘½åå®ä½“è¯†åˆ« (NER) çš„ç”¨æ­¦ä¹‹åœ°ã€‚
In natural language processing (NLP), NER is a subtask of information extraction that seeks to locate and classify named entities in text into pre-defined categories such as names of persons, organizations, locations, expressions of times, quantities, monetary values, and percentages.  
åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ˜¯ä¿¡æ¯æå–çš„ä¸€ä¸ªå­ä»»åŠ¡ï¼Œæ—¨åœ¨å®šä½æ–‡æœ¬ä¸­å‡ºç°çš„å‘½åå®ä½“ï¼Œå¹¶å°†å…¶åˆ†ç±»åˆ°é¢„å…ˆå®šä¹‰çš„ç±»åˆ«ä¸­ï¼Œä¾‹å¦‚äººåã€ç»„ç»‡æœºæ„åã€åœ°ç‚¹ã€æ—¶é—´è¡¨è¾¾å¼ã€æ•°é‡ã€è´§å¸å€¼å’Œç™¾åˆ†æ¯”ã€‚
For instance, consider the sentence: "Apple Inc. is planning to open a new store in San Francisco." Using NER, we could identify that "Apple Inc." is an organization and "San Francisco" is a location. Such information can be incredibly valuable for numerous NLP tasks.  
ä¾‹å¦‚ï¼Œè€ƒè™‘è¿™å¥è¯ï¼šâ€œè‹¹æœå…¬å¸è®¡åˆ’åœ¨æ—§é‡‘å±±å¼€è®¾ä¸€å®¶æ–°åº—ã€‚â€ ä½¿ç”¨ NERï¼Œæˆ‘ä»¬å¯ä»¥è¯†åˆ«å‡ºâ€œè‹¹æœå…¬å¸â€æ˜¯ä¸€ä¸ªç»„ç»‡ï¼Œâ€œæ—§é‡‘å±±â€æ˜¯ä¸€ä¸ªåœ°ç‚¹ã€‚ è¿™äº›ä¿¡æ¯å¯¹äºä¼—å¤š NLP ä»»åŠ¡æ¥è¯´éå¸¸å®è´µã€‚

##### Part of Speech (POS) Tagging  
è¯æ€§æ ‡æ³¨ (POS)

Every word in a sentence has a particular role. Some words are objects, some are verbs, some are adjectives, and so on. Tagging these parts of speech, or POS tagging, can be a critical component to many NLP tasks. It can help answer many questions, like what are the main objects in a sentence, what actions are being taken, and what's the context of these actions?  
å¥å­ä¸­çš„æ¯ä¸ªè¯éƒ½æœ‰ç‰¹å®šçš„è¯æ€§ã€‚æœ‰äº›è¯æ˜¯å®¾è¯­ï¼Œæœ‰äº›è¯æ˜¯åŠ¨è¯ï¼Œæœ‰äº›è¯æ˜¯å½¢å®¹è¯ï¼Œç­‰ç­‰ã€‚å¯¹è¿™äº›è¯æ€§è¿›è¡Œæ ‡è®°ï¼Œæˆ–è€…è¯´è¯æ€§æ ‡æ³¨ï¼Œæ˜¯è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚å®ƒå¯ä»¥å¸®åŠ©å›ç­”è®¸å¤šé—®é¢˜ï¼Œä¾‹å¦‚å¥å­ä¸­çš„ä¸»è¦å®¾è¯­æ˜¯ä»€ä¹ˆï¼Œæ­£åœ¨é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ï¼Œä»¥åŠè¿™äº›è¡ŒåŠ¨çš„èƒŒæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ
Let's start with a sentence example: "Apple Inc. is planning to open a new store in San Francisco." We are going to useÂ `NLTK`'sÂ `pos_tag`Â function to tag the part of speech for each word in this sentence.  
è®©æˆ‘ä»¬ä»ä¸€ä¸ªä¾‹å¥å¼€å§‹ï¼šâ€œè‹¹æœå…¬å¸è®¡åˆ’åœ¨æ—§é‡‘å±±å¼€è®¾ä¸€å®¶æ–°åº—ã€‚â€æˆ‘ä»¬å°†ä½¿ç”¨Â `NLTK`Â çš„Â `pos_tag`Â å‡½æ•°æ¥æ ‡è®°è¿™ä¸ªå¥å­ä¸­æ¯ä¸ªè¯çš„è¯æ€§ã€‚

```python

from nltk import pos_tag, word_tokenize

example_sentence = "Apple Inc. is planning to open a new store in San Francisco."
tokens = word_tokenize(example_sentence)
pos_tags = pos_tag(tokens)
print(f'The first 5 POS tags are: {pos_tags[:5]}')

```

The output of the above code will be:  
ä»¥ä¸Šä»£ç çš„è¾“å‡ºå°†æ˜¯ï¼š

```python

The first 5 POS tags are: [('Apple', 'NNP'), ('Inc.', 'NNP'), ('is', 'VBZ'), ('planning', 'VBG'), ('to', 'TO')]

```

Here, every word from our sentence gets tagged with a corresponding part of speech. This is the first step towards performing Named Entity Recognition.  
åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¥å­ä¸­çš„æ¯ä¸ªè¯éƒ½è¢«æ ‡è®°äº†ç›¸åº”çš„è¯æ€§ã€‚è¿™æ˜¯è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰çš„ç¬¬ä¸€æ­¥ã€‚

##### Named Entity Recognition with NLTK  
ä½¿ç”¨ NLTK è¿›è¡Œå‘½åå®ä½“è¯†åˆ«

Now, what about Named Entity Recognition? Well, Named Entity Recognition (or NER) can be considered a step beyond regular POS tagging. It groups together one or more words that signify a named entity such as "San Francisco" or "Apple Inc." into a single category, i.e., location or organization in this case.  
é‚£ä¹ˆï¼Œå‘½åå®ä½“è¯†åˆ«å‘¢ï¼Ÿå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å¯ä»¥è¢«è§†ä¸ºæ¯”å¸¸è§„è¯æ€§æ ‡æ³¨æ›´è¿›ä¸€æ­¥çš„æŠ€æœ¯ã€‚å®ƒå°†è¡¨ç¤ºå‘½åå®ä½“çš„ä¸€ä¸ªæˆ–å¤šä¸ªå•è¯ï¼ˆä¾‹å¦‚â€œæ—§é‡‘å±±â€æˆ–â€œè‹¹æœå…¬å¸â€ï¼‰å½’ç±»åˆ°å•ä¸ªç±»åˆ«ä¸­ï¼Œåœ¨æœ¬ä¾‹ä¸­åˆ†åˆ«æ˜¯åœ°ç‚¹æˆ–ç»„ç»‡ã€‚
We can use theÂ `ne_chunk`Â function in NLTK to perform NER on our POS-tagged sentence, like so:  
æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ NLTK ä¸­çš„Â `ne_chunk`Â å‡½æ•°å¯¹æˆ‘ä»¬ POS æ ‡æ³¨çš„å¥å­æ‰§è¡Œ NERï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python

from nltk import ne_chunk

named_entities = ne_chunk(pos_tags)
print(f'The named entities in our example sentences are:\n{named_entities}')

```

The output of the above code will be:  
ä»¥ä¸Šä»£ç çš„è¾“å‡ºå°†æ˜¯ï¼š

```python

The named entities in our example sentences are:
(S
  (PERSON Apple/NNP)
  (ORGANIZATION Inc./NNP)
  is/VBZ
  planning/VBG
  to/TO
  open/VB
  a/DT
  new/JJ
  store/NN
  in/IN
  (GPE San/NNP Francisco/NNP)
  ./.)  

```

Let's break down this output:  
è®©æˆ‘ä»¬åˆ†æä¸€ä¸‹è¿™ä¸ªè¾“å‡ºï¼š

- The 'S' at the beginning signifies the start of a sentence.  
    å¥é¦–çš„â€œSâ€è¡¨ç¤ºä¸€ä¸ªå¥å­çš„å¼€å§‹ã€‚
- Words inside paretheses, prefixed with labels such as PERSON, ORGANIZATION, or GPE are recognized named entities. For example, '(PERSON Apple/NNP)' indicates that 'Apple' is recognized as a named entity representing a Person and 'Apple' has been POS tagged as 'NNP' (Proper Noun, Singular).  
    æ‹¬å·ä¸­çš„è¯è¯­ï¼Œå¦‚æœå¸¦æœ‰è¯¸å¦‚ PERSONã€ORGANIZATION æˆ– GPE ç­‰æ ‡ç­¾ï¼Œåˆ™è¡¨ç¤ºè¯†åˆ«å‡ºçš„å‘½åå®ä½“ã€‚ä¾‹å¦‚ï¼Œ'(PERSON Apple/NNP)' è¡¨ç¤ºâ€œAppleâ€è¢«è¯†åˆ«ä¸ºä»£è¡¨äººç‰©çš„å‘½åå®ä½“ï¼Œå¹¶ä¸”â€œAppleâ€å·²è¢«è¯æ€§æ ‡æ³¨ä¸ºâ€œNNPâ€ï¼ˆä¸“æœ‰åè¯ï¼Œå•æ•°ï¼‰ã€‚
    - Words outside parentheses are not recognized as part of a named entity but are part of the sentence and each of them is associated with a POS tag. For instance, 'is/VBZ' means that 'is' is recognized as a verb in present tense, 3rd person singular form.  
    åœ†æ‹¬å·å¤–çš„å•è¯ä¸è¢«è¯†åˆ«ä¸ºå‘½åå®ä½“çš„ä¸€éƒ¨åˆ†ï¼Œè€Œæ˜¯å¥å­çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸”æ¯ä¸ªå•è¯éƒ½ä¸ä¸€ä¸ªè¯æ€§æ ‡ç­¾ç›¸å…³è”ã€‚ä¾‹å¦‚ï¼Œâ€œis/VBZâ€è¡¨ç¤ºâ€œisâ€è¢«è¯†åˆ«ä¸ºç°åœ¨æ—¶ã€ç¬¬ä¸‰äººç§°å•æ•°å½¢å¼çš„åŠ¨è¯ã€‚
    - '(GPE San/NNP Francisco/NNP)' indicates that 'San Francisco', a two-word entity, is recognized as a geopolitical entity, such as a city, state, or country.  
    '(GPE San/NNP Francisco/NNP)' è¡¨ç¤ºâ€œæ—§é‡‘å±±â€è¿™ä¸ªç”±ä¸¤ä¸ªè¯ç»„æˆçš„å®ä½“è¢«è¯†åˆ«ä¸ºä¸€ä¸ªåœ°ç¼˜æ”¿æ²»å®ä½“ï¼Œä¾‹å¦‚åŸå¸‚ã€å·æˆ–å›½å®¶ã€‚
    While Named Entity Recognition offers richer insights than simple POS tagging, it might not always be perfectly accurate due to the ambiguity and context-dependent nature of language. Despite this, it's a powerful tool in any NLP practitioner's arsenal.  
è™½ç„¶å‘½åå®ä½“è¯†åˆ«æ¯”ç®€å•çš„è¯æ€§æ ‡æ³¨æä¾›äº†æ›´ä¸°å¯Œçš„è§è§£ï¼Œä½†ç”±äºè¯­è¨€çš„æ­§ä¹‰æ€§å’Œè¯­å¢ƒä¾èµ–æ€§ï¼Œå®ƒå¯èƒ½å¹¶ä¸æ€»æ˜¯å®Œå…¨å‡†ç¡®ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå®ƒä»ç„¶æ˜¯ä»»ä½•è‡ªç„¶è¯­è¨€å¤„ç†ä»ä¸šè€…æ­¦å™¨åº“ä¸­çš„æœ‰åŠ›å·¥å…·ã€‚

##### Applying PoS Tagging and NER to a Real Dataset  
å°†è¯æ€§æ ‡æ³¨å’Œå‘½åå®ä½“è¯†åˆ«åº”ç”¨äºçœŸå®æ•°æ®é›†

Examining these NLP techniques in action on larger, more complex datasets allows us to understand the power of Natural Language Processing better. To this end, let's use POS tagging and Named Entity Recognition on a real-world dataset - the 20 Newsgroups dataset.  
åœ¨æ›´å¤§ã€æ›´å¤æ‚çš„æ•°æ®é›†ä¸Šè€ƒå¯Ÿè¿™äº› NLP æŠ€æœ¯çš„å®é™…åº”ç”¨ï¼Œå¯ä»¥è®©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£è‡ªç„¶è¯­è¨€å¤„ç†çš„å¼ºå¤§åŠŸèƒ½ã€‚ä¸ºæ­¤ï¼Œè®©æˆ‘ä»¬åœ¨çœŸå®ä¸–ç•Œçš„æ•°æ®é›†â€”â€”20 Newsgroups æ•°æ®é›†â€”â€”ä¸Šä½¿ç”¨è¯æ€§æ ‡æ³¨å’Œå‘½åå®ä½“è¯†åˆ«æŠ€æœ¯ã€‚

```python

from sklearn.datasets import fetch_20newsgroups
from nltk import pos_tag, ne_chunk, word_tokenize

# Loading the data with metadata removed
newsgroups_data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))

# Selecting the first document 
first_doc = newsgroups_data.data[0]

# Trimming the document's text down to the first 67 characters
first_doc = first_doc[:67]

# Tokenizing the text
tokens_first_doc = word_tokenize(first_doc)

# Applying POS tagging
pos_tags_first_doc = pos_tag(tokens_first_doc)

# Applying Named Entity Recognition
named_entities = ne_chunk(pos_tags_first_doc)

print(f'The first chunk of named entities in the first document are:\n{named_entities}')

```

Here's the output you can expect:  
è¯·æä¾›æ‚¨éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬å†…å®¹ã€‚æˆ‘å°†å°½åŠ›å°†å…¶å‡†ç¡®åœ°ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ï¼Œå¹¶ä¿æŒåŸæ–‡çš„å­¦æœ¯è¯­æ°”ï¼Œä¸æ·»åŠ ä»»ä½•è§£é‡Šã€‚

```python

The first chunk of named entities in the first document are:
(S
  I/PRP
  was/VBD
  wondering/VBG
  if/IN
  anyone/NN
  out/IN
  there/RB
  could/MD
  enlighten/VB
  me/PRP
  on/IN
  this/DT
  car/NN)

```

As you can see, even when we're working with a slimmed-down text input, both POS tagging and NER deliver valuable insights. We've applied these techniques to just a portion of a complex, real-world dataset, demonstrating how NLP can uncover important information from vast amounts of textual data. This highlights the critical role NLP plays in fields ranging from data analysis to AI and machine learning.  
æ­£å¦‚æ‚¨æ‰€è§ï¼Œå³ä½¿æˆ‘ä»¬å¤„ç†çš„æ˜¯ç²¾ç®€çš„æ–‡æœ¬è¾“å…¥ï¼Œè¯æ€§æ ‡æ³¨å’Œå‘½åå®ä½“è¯†åˆ«ä¹Ÿèƒ½æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬å·²å°†è¿™äº›æŠ€æœ¯åº”ç”¨äºå¤æ‚ã€çœŸå®çš„æ•°æ®åº“çš„ä¸€éƒ¨åˆ†ï¼Œå±•ç¤ºäº†è‡ªç„¶è¯­è¨€å¤„ç†å¦‚ä½•ä»æµ·é‡æ–‡æœ¬æ•°æ®ä¸­å‘ç°é‡è¦ä¿¡æ¯ã€‚è¿™å‡¸æ˜¾äº†è‡ªç„¶è¯­è¨€å¤„ç†åœ¨ä»æ•°æ®åˆ†æåˆ°äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ ç­‰å„ä¸ªé¢†åŸŸçš„å…³é”®ä½œç”¨ã€‚

##### Lesson Summary and Practice  
è¯¾ç¨‹æ€»ç»“ä¸ç»ƒä¹ 

In this lesson, we have coveredÂ **Part of Speech (POS) tagging**,Â **Named Entity Recognition (NER)**, and even applied these techniques to a real-world dataset! These concepts are fundamental to text preprocessing in Natural Language Processing (NLP). Having a grasp over these will allow you to approach more advanced topics in NLP with ease.  
åœ¨æœ¬è¯¾ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†è¯æ€§æ ‡æ³¨ (POS)ã€å‘½åå®ä½“è¯†åˆ« (NER)ï¼Œç”šè‡³å°†è¿™äº›æŠ€æœ¯åº”ç”¨åˆ°äº†çœŸå®çš„æ•°æ®é›†ï¼è¿™äº›æ¦‚å¿µæ˜¯è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ä¸­æ–‡æœ¬é¢„å¤„ç†çš„åŸºç¡€ã€‚æŒæ¡è¿™äº›å°†ä½¿æ‚¨èƒ½å¤Ÿè½»æ¾åœ°å¤„ç† NLP ä¸­æ›´é«˜çº§çš„ä¸»é¢˜ã€‚
You have the upcoming tasks to practice these techniques, reinforcing your understanding and improving your Natural Language Processing skills. Let's head onto them and keep learning! Practice is key when it comes to mastering these techniques. Enjoy the hands-on session. Keep Learning!  
ä½ æ¥ä¸‹æ¥å°†è¦å®Œæˆçš„ä»»åŠ¡å¯ä»¥å¸®åŠ©ä½ ç»ƒä¹ è¿™äº›æŠ€å·§ï¼Œå·©å›ºç†è§£å¹¶æå‡è‡ªç„¶è¯­è¨€å¤„ç†æŠ€èƒ½ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼ŒæŒç»­å­¦ä¹ ï¼ç†Ÿç»ƒæŒæ¡è¿™äº›æŠ€å·§çš„å…³é”®åœ¨äºç»ƒä¹ ã€‚äº«å—å®è·µçš„è¿‡ç¨‹å§ï¼æŒç»­å­¦ä¹ ï¼

## ã€ŒPracticeã€
Great job! Now, let's shift our focus to another set of student scores, this time inÂ **Math**. The given code calculates upper outlier bound, using theÂ **Interquartile Range (IQR)**Â method, described in the lesson.

Your goal is to update code to calculate lower outlier bound instead, using the same principle.

```python

import numpy as np

  
  

math_scores = np.array([92, 75, 88, 78, 90, 80, 88, 80])

# Calculate Q1 and Q3 for math scores

Q1_math = np.percentile(math_scores, 25)

Q3_math = np.percentile(math_scores, 75)

  

# Calculate the Interquartile Range for math scores

IQR_math = Q3_math - Q1_math

upper_outlier_bound = Q3_math + 1.5 * IQR_math

  

print(upper_outlier_bound)

```

è§£é‡Š
To update the given code to calculate the lower outlier bound using the Interquartile Range (IQR) method, we need to follow these steps:

1. Calculate the first quartile (Q1) and the third quartile (Q3) for the math scores.
2. Calculate the Interquartile Range (IQR) by subtracting Q1 from Q3.
3. Use the IQR to calculate the lower outlier bound by subtracting \(1.5 \times \text{IQR}\) from Q1.

Here is the updated code:

```python

import numpy as np

# Given math scores
math_scores = np.array([92, 75, 88, 78, 90, 80, 88, 80])

# Calculate Q1 and Q3 for math scores
Q1_math = np.percentile(math_scores, 25)
Q3_math = np.percentile(math_scores, 75)

# Calculate the Interquartile Range for math scores
IQR_math = Q3_math - Q1_math

# Calculate the lower outlier bound
lower_outlier_bound = Q1_math - 1.5 * IQR_math

print(lower_outlier_bound)

```

Explanation:
- We first calculate the first quartile (\(Q1\)) and the third quartile (\(Q3\)) using `np.percentile`.
- Then, we find the Interquartile Range (\(IQR\)) by subtracting \(Q1\) from \(Q3\).
- Finally, we calculate the lower outlier bound by subtracting \(1.5 \times \text{IQR}\) from \(Q1\), which is the standard method to detect lower outliers.

This code will output the lower outlier bound for the given math scores.

## ã€ŒPracticeã€
Great work, Space Voyager!

Now, change the sentence in the provided code to "Microsoft Corporation aims to reach 95% global coverage by 2025." and observe how the named entities identified by theÂ **Named Entity Recognition (NER)**Â change.

Keep advancing!

```python

from nltk import pos_tag, word_tokenize, ne_chunk

  

# Given sentence

sample_sentence = "Apple Inc. planned to open a new store in San Francisco in 82% of time by 2020."

  

# Get tokens

tokens = word_tokenize(sample_sentence)

  

# Get POS tags

pos_tags = pos_tag(tokens)

  

# Perform NER

named_entities = ne_chunk(pos_tags)

  

# Print the identified named entities

print(named_entities)

```

To change the sentence and observe how the named entities identified by the Named Entity Recognition (NER) change, we'll modify the given sentence to "Microsoft Corporation aims to reach 95% global coverage by 2025." and then run the NER process.

Here's the updated code:

```python

from nltk import pos_tag, word_tokenize, ne_chunk

# Updated sentence
sample_sentence = "Microsoft Corporation aims to reach 95% global coverage by 2025."

# Get tokens
tokens = word_tokenize(sample_sentence)

# Get POS tags
pos_tags = pos_tag(tokens)

# Perform NER
named_entities = ne_chunk(pos_tags)

# Print the identified named entities
print(named_entities)

```

The expected output will show the named entities identified by NER. Let's break down the process and what changes we can expect:

1. **Tokenization**: The sentence will be split into individual words (tokens).
2. **POS Tagging**: Each token will be tagged with its part of speech.
3. **NER**: The part-of-speech tagged tokens will be analyzed to identify named entities such as organizations, locations, and dates.

The initial sentence, "Apple Inc. planned to open a new store in San Francisco in 82% of time by 2020," identifies "Apple Inc." as an organization, "San Francisco" as a location, and "2020" as a date. 

For the updated sentence, "Microsoft Corporation aims to reach 95% global coverage by 2025," the expected named entities are:
- "Microsoft Corporation" as an organization.
- "2025" as a date.

Running the code will confirm the exact output. Here's a mock-up of what we can expect:

```

(S
  (ORGANIZATION Microsoft/NNP Corporation/NNP)
  aims/VBZ
  to/TO
  reach/VB
  95/CD
  %/NN
  global/JJ
  coverage/NN
  by/IN
  2025/CD)

```

This output indicates that "Microsoft Corporation" is recognized as an organization, and "2025" is recognized as a date, as expected. The NER process using `ne_chunk` from NLTK helps in identifying and categorizing these entities.

## ã€ŒPracticeã€
Excellent work, Space Explorer!

  

Let's reinforce your learning by inserting the missing code. Fill in the appropriate functions to perform tokenization and POS tagging. Use nltk's powerful tools to achieve this. Happy learning!from nltk import ____, word_tokenize

  

```python

# Given sentence

sample_sentence = "The quick brown fox jumps over the lazy dog."

  

# Get tokens

tokens = word_tokenize(sample_sentence)

  

# Get POS tags

pos_tags = ____(tokens)

  

# Print the POS tags

print(pos_tags)

```

è§£é‡Š
Certainly, here is the completed code with the appropriate functions from the NLTK library to perform tokenization and POS tagging:

```python

from nltk import pos_tag, word_tokenize

# Given sentence
sample_sentence = "The quick brown fox jumps over the lazy dog."

# Get tokens
tokens = word_tokenize(sample_sentence)

# Get POS tags
pos_tags = pos_tag(tokens)

# Print the POS tags
print(pos_tags)

```

### Explanation:
1. **Importing the necessary functions**:
   - `pos_tag`: This function is used to perform part-of-speech (POS) tagging.
   - `word_tokenize`: This function is used to split the sentence into individual words or tokens.

2. **Tokenization**:
   - `tokens = word_tokenize(sample_sentence)`: This line splits the given sentence into tokens.

3. **POS Tagging**:
   - `pos_tags = pos_tag(tokens)`: This line assigns POS tags to each token.

4. **Printing the POS tags**:
   - `print(pos_tags)`: This prints out the tokens along with their corresponding POS tags.

When you run this code, you should see an output where each word in the sentence is paired with its POS tag, like this:

```

[('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]

```

### Explanation of POS tags in the output:
- **DT**: Determiner
- **JJ**: Adjective
- **NN**: Noun, singular or mass
- **VBZ**: Verb, 3rd person singular present
- **IN**: Preposition or subordinating conjunction

This way, the code effectively tokenizes the sentence and performs POS tagging using NLTK's powerful tools.

# lesson

##  Introduction

Hello and welcome! Today's lesson will introduce a crucial component of text feature engineering:Â _tokenization_. Used in text classification, tokenization is a pre-processing step that transforms raw text into units of meaning known as tokens. By breaking down text into these consumable pieces, we can provide feeding material for machine learning models to understand the text better. Our goal in this lesson is to apply tokenization on a raw text dataset (_IMDB movie review dataset_) and understand how it can be beneficial in the process of text classification.

## Understanding the Concept and Importance of Text Tokenization

Text tokenization is a type of pre-processing step where a text string is split up into individual units (tokens). In most cases, these tokens are words, digits, or punctuation marks. For instance, consider this text: "I love Python." After tokenization, this sentence is split intoÂ `['I', 'love', 'Python', '.']`, with each word and punctuation mark becoming a separate token.

Text tokenization plays a foundational role in text classification and many Natural Language Processing (NLP) tasks. Consider the fact that most machine learning algorithms prefer numerical input. But when dealing with text data, we can't feed raw text directly into these algorithms. This is where tokenization steps in. It breaks down the text into individual tokens, which can then be transformed into some numerical form (via techniques like Bag-of-Words, TF-IDF, etc.). This transformed form can then be processed by the machine learning algorithms.

## Applying Tokenization on a Text Example Using NLTK

Before we tackle our dataset, let's understand how tokenization works with a simple example. Python and theÂ `NLTK`Â (Natural Language Toolkit) library, a comprehensive library built specifically for NLP tasks, make tokenization simple and efficient. For our example, suppose we have a sentence: "The cat is on the mat." Let's tokenize it:

```python

from nltk import word_tokenize
text = "The cat is on the mat."
tokens = word_tokenize(text)
print(tokens)

```

The output of the above code will be:

```

['The', 'cat', 'is', 'on', 'the', 'mat', '.']

```

##### Text Classification Dataset Overview

For the purpose of this lesson, we'll use theÂ **IMDB movie reviews dataset**Â (provided in the NLTK corpus). This dataset contains movie reviews along with their associated binary sentiment polarity labels. The core dataset has 50,000 reviews split evenly into 25k for training and 25k for testing. Each set has 12.5k positive and 12.5k negative reviews. However, for the purpose of these lessons, we will focus on using the first 100 reviews.

It's important to note that the IMDB dataset provided in the NLTK corpus has been preprocessed. The text is already lowercased, and common punctuation is typically separated from the words. This pre-cleaning makes the dataset well-suited for the tokenization process we'll be exploring.

Let's get these reviews and print a few of them:

```python

import nltk
from nltk.corpus import movie_reviews

nltk.download('movie_reviews')

movie_reviews_ids = movie_reviews.fileids()[:100]
review_texts = [movie_reviews.raw(fileid) for fileid in movie_reviews_ids]
print("First movie review:\n", review_texts[0][:260])

```

Note that we're only printing the first 260 characters of the first review to prevent lengthy output.

The output of the above code will be:

```python

First movie review:
 plot : two teen couples go to a church party , drink and then drive . 
they get into an accident . 
one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . 
what's the deal ? 
watch the movie and " sorta " find out . .

```

##### Applying Tokenization on the Dataset

Now it's time to transform our data. For this, we will apply tokenization on all our 100 movie reviews.

```python

from nltk import word_tokenize
tokenized_reviews = [word_tokenize(review) for review in review_texts]

```

So, what changes did tokenization bring to our data? Each review, which was initially a long string of text, is now a list of individual tokens (words, punctuation, etc), which collectively represent the review. In other words, our dataset evolved from being a list of strings to being a list of lists.

```python

for i, review in enumerate(tokenized_reviews[:3]):
  print(f"\n Review {i+1} first 10 tokens:\n", review[:10])

```

The output of the above code will be:

```python

 Review 1 first 10 tokens:
 ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party']

 Review 2 first 10 tokens:
 ['the', 'happy', 'bastard', "'s", 'quick', 'movie', 'review', 'damn', 'that', 'y2k']

 Review 3 first 10 tokens:
 ['it', 'is', 'movies', 'like', 'these', 'that', 'make', 'a', 'jaded', 'movie']

```

##### Lesson Summary and Next Steps

Well done! Today, you learned about the fundamental concept of text tokenization and its importance in text classification. You also applied tokenization to the IMDB movie reviews dataset using Python and NLTK. Your text data is now effectively transformed into a form that machine learning models can digest better.

As you advance in the course, you will refine this dataset further for your text classification objectives. We are laying the foundation one brick at a time, and tokenization was a sturdy one! Upcoming lessons will build upon this understanding. You'll harness this tokenized data to generate Bag-of-Words representations, implement TF-IDF representations, handle sparse features, and apply dimensionality reduction.

Remember, practice consolidates learning. Make sure to reinforce your knowledge by practicing the code samples and applying these concepts contextually. Don't forget to use your creativity to manipulate codes and see the outcomes. Happy learning!

## ã€ŒPracticeã€
Good job, Space Voyager! The existing code is intended to tokenize a movie review and filter out punctuation, leaving only word tokens. However, there's a bug causing it to act incorrectly.  
å¹²å¾—å¥½ï¼Œå¤ªç©ºæ—…è¡Œè€…ï¼ç°æœ‰ä»£ç æ—¨åœ¨å°†ç”µå½±è¯„è®ºæ ‡è®°åŒ–å¹¶è¿‡æ»¤æ‰æ ‡ç‚¹ç¬¦å·ï¼Œä»…ä¿ç•™å•è¯æ ‡è®°ã€‚ä½†æ˜¯ï¼Œå…¶ä¸­å­˜åœ¨ä¸€ä¸ªé”™è¯¯ï¼Œå¯¼è‡´å…¶è¡Œä¸ºä¸æ­£ç¡®ã€‚

Can you locate the error and adjust the code to correctly execute the task? After your revision, the code should tokenize the first movie review from the dataset, omit any punctuation, and print the first 10 word tokens.

```python

import nltk

from nltk.corpus import movie_reviews

from nltk import word_tokenize

  

nltk.download('movie_reviews', quiet=True)

  

# Load the first movie review

review_text = movie_reviews.raw(movie_reviews.fileids()[0])

  

# Apply tokenization on the review

tokenized_review = word_tokenize(review_text)

  

# Should keep only word tokens

current_tokens = [token for token in tokenized_review if not token.isalpha()]

  

# Print the first 10 tokens

print("\nThe first 10 tokens are:\n", current_tokens[:10])

```

è§£é‡Š
ä½ å·²ç»å¾ˆæ¥è¿‘æ­£ç¡®ç­”æ¡ˆäº†ï¼ä»£ç ä¸­åªæœ‰ä¸€ä¸ªå°é”™è¯¯éœ€è¦ä¿®æ­£ã€‚è®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹è¿™è¡Œä»£ç ï¼š

```python

current_tokens = [token for token in tokenized_review if not token.isalpha()]

```

è¿™è¡Œä»£ç ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼æ¥è¿‡æ»¤æ ‡è®°ï¼Œä½†æ¡ä»¶ `if not token.isalpha()` çš„é€»è¾‘æ˜¯ç›¸åçš„ã€‚`isalpha()` æ–¹æ³•æ£€æŸ¥ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦åªåŒ…å«å­—æ¯å­—ç¬¦ã€‚é€šè¿‡ä½¿ç”¨ `not`ï¼Œå®ƒå®é™…ä¸Šä¿ç•™äº†éå­—æ¯çš„æ ‡è®°ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œè€Œä¸¢å¼ƒäº†æ‰€æœ‰å•è¯æ ‡è®°ã€‚

ä¸ºäº†ä¿®å¤è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦ç§»é™¤ `not` å…³é”®å­—ï¼Œåƒè¿™æ ·ï¼š

```python

current_tokens = [token for token in tokenized_review if token.isalpha()]

```

ç°åœ¨ï¼Œæ¡ä»¶ `if token.isalpha()` å°†åªä¿ç•™å­—æ¯å­—ç¬¦çš„æ ‡è®°ï¼Œæœ‰æ•ˆåœ°è¿‡æ»¤æ‰æ ‡ç‚¹ç¬¦å·ã€‚

ä¿®æ”¹åçš„å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

```python

import nltk
from nltk.corpus import movie_reviews
from nltk import word_tokenize

nltk.download('movie_reviews', quiet=True)

# Load the first movie review
review_text = movie_reviews.raw(movie_reviews.fileids()[0])

# Apply tokenization on the review 
tokenized_review = word_tokenize(review_text)

# Should keep only word tokens
current_tokens = [token for token in tokenized_review if token.isalpha()]

# Print the first 10 tokens
print("\nThe first 10 tokens are:\n", current_tokens[:10])

```

è¿è¡Œä¿®æ”¹åçš„ä»£ç ï¼Œä½ åº”è¯¥ä¼šçœ‹åˆ°å‰10ä¸ªå•è¯æ ‡è®°æ­£ç¡®åœ°æ‰“å°å‡ºæ¥ï¼Œä¸åŒ…å«ä»»ä½•æ ‡ç‚¹ç¬¦å·ã€‚è¿™æ ·å°±æˆåŠŸåœ°æ ‡è®°åŒ–äº†ç”µå½±è¯„è®ºï¼Œå¹¶è¿‡æ»¤æ‰äº†éå•è¯æ ‡è®°ã€‚

```

The first 10 tokens are:  
 ['plot', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', 'drink']

```

## ã€ŒPracticeã€
Hey Space Voyager, you're doing great!  
å˜¿ï¼Œå¤ªç©ºæ—…è¡Œè€…ï¼Œä½ åšå¾—å¾ˆå¥½ï¼

In the provided starter code, fill in the blanks (`____`) to convert a given sentence into individual tokens using theÂ `nltk`Â library. Then, filter out the word tokens, leaving out the punctuation marks.Â è¯·æä¾›éœ€è¦ç¿»è¯‘çš„å®Œæ•´å¥å­ï¼Œæˆ‘æ‰èƒ½å¸®æ‚¨ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ã€‚ç›®å‰æ‚¨åªæä¾›äº†å¥å­çš„å¼€å¤´ "In the p"ï¼Œä¿¡æ¯ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘ã€‚  
åœ¨æä¾›çš„åˆå§‹ä»£ç ä¸­ï¼Œå¡«å†™ç©ºæ ¼ (Â `____`Â )ï¼Œä½¿ç”¨Â `nltk`Â åº“å°†ç»™å®šçš„å¥å­è½¬æ¢ä¸ºå•ç‹¬çš„æ ‡è®°ã€‚ç„¶åï¼Œè¿‡æ»¤æ‰å•è¯æ ‡è®°ï¼Œå»æ‰æ ‡ç‚¹ç¬¦å·ã€‚

```python

from nltk import ____

  

# Given text

text = "Who doesn't enjoy studying Feature Engineering for Text Classification?"

  

# Tokenize the text

tokens = ____(text)

  

# Filter out the words

words = [token for token in tokens if token.____()]

  

print("\n Word Tokens:\n", words)

```

## ã€ŒPracticeã€
Hey Space Voyager, you're doing great!

In the provided starter code, fill in the blanks (`____`) to convert a given sentence into individual tokens using theÂ `nltk`Â library. Then, filter out the word tokens, leaving out the punctuation marks.

```python

from nltk import ____

  

# Given text

text = "Who doesn't enjoy studying Feature Engineering for Text Classification?"

  

# Tokenize the text

tokens = ____(text)

  

# Filter out the words

words = [token for token in tokens if token.____()]

  

print("\n Word Tokens:\n", words)

```

è§£é‡Š

```

You're diving into natural language processing (NLP) with the NLTK library, which is a powerful tool for text analysis! To complete the provided Python code, you need to fill in the blanks to tokenize a given sentence and filter out non-word tokens such as punctuation. Here's how you can do it:

1. **Import Tokenizer**: First, you need to import the appropriate tokenizer from NLTK. A commonly used tokenizer is `word_tokenize`.

2. **Tokenize the Text**: Use the `word_tokenize` function to split the text into tokens.

3. **Filter Non-Word Tokens**: You'll need to check if each token is alphabetic to filter out punctuation. The string method `isalpha()` is useful here, as it returns `True` if all characters in the string are alphabetic.

Here's the completed script:

```python

from nltk import word_tokenize

# Given text
text = "Who doesn't enjoy studying Feature Engineering for Text Classification?"

# Tokenize the text
tokens = word_tokenize(text)

# Filter out the words
words = [token for token in tokens if token.isalpha()]

print("\n Word Tokens:\n", words)

```

**Explanation**:
- **`word_tokenize`**: This function from NLTK breaks the text into individual words and punctuation.
- **`token.isalpha()`**: This method checks if the token consists only of alphabetic characters, thus effectively filtering out punctuation and numbers.

When you run this script, it will print the list of word tokens from the given text, excluding any punctuation marks.

```

## ã€ŒPracticeã€
Great job, Voyager! Now, you're tasked with fine-tuning our weather simulation. Could you modify the initial parameters in theÂ `np.random.normal()`Â function to simulate temperature values that have an average ofÂ `35`Â and a standard deviation ofÂ `10`?

Observe how this alteration impacts theÂ _skewness_Â andÂ _kurtosis_Â of the weather data distribution. Keep up the good work!

```

```python
from scipy import stats

import numpy as np

  

weather_data = np.random.normal(loc=30, scale=5, size=1000) # A populated weather data (secured from a suitable weather database)

  

# Compute skewness - representing the direction of skew (departure from horizontal symmetry)

weather_skewness = stats.skew(weather_data)

  

# Compute kurtosis - shows the height and sharpness of the data at the center

weather_kurtosis = stats.kurtosis(weather_data)

  

print(f"Weather Data Skewness: {weather_skewness}\nWeather Data Kurtosis: {weather_kurtosis}")

```

æ ¹æ®ä½ çš„è¦æ±‚ï¼Œæˆ‘å·²ç»ä¿®æ”¹äº†ä»£ç ï¼Œå°†æ­£æ€åˆ†å¸ƒçš„å‚æ•°ä» `loc=30`, `scale=5` è°ƒæ•´åˆ° `loc=35`, `scale=10`ã€‚ä¸‹é¢æ˜¯ä¿®æ­£åçš„å®Œæ•´ä»£ç æ®µï¼š

```python

from scipy import stats
import numpy as np

# æ›´æ–°å‚æ•°ä»¥æ¨¡æ‹Ÿå¹³å‡å€¼ä¸º35ï¼Œæ ‡å‡†å·®ä¸º10çš„æ¸©åº¦å€¼
weather_data = np.random.normal(loc=35, scale=10, size=1000)

# è®¡ç®—ååº¦ - è¡¨ç¤ºåç¦»æ°´å¹³å¯¹ç§°çš„æ–¹å‘
weather_skewness = stats.skew(weather_data)

# è®¡ç®—å³°åº¦ - æ˜¾ç¤ºæ•°æ®ä¸­å¿ƒçš„é«˜åº¦å’Œå°–é”åº¦
weather_kurtosis = stats.kurtosis(weather_data)

print(f"Weather Data Skewness: {weather_skewness}\nWeather Data Kurtosis: {weather_kurtosis}")

```

è¿™æ®µä»£ç å°†ç”Ÿæˆæ–°çš„å¤©æ°”æ•°æ®ï¼Œå¹¶è®¡ç®—å…¶ååº¦å’Œå³°åº¦ï¼Œä»¥åˆ†ææ•°æ®çš„åˆ†å¸ƒç‰¹æ€§ã€‚å¦‚éœ€è¿›ä¸€æ­¥åˆ†ææˆ–è°ƒæ•´ï¼Œè¯·å‘ŠçŸ¥ï¼

## ã€ŒPraticeã€
Great job so far, Space Voyager!

Your task now is to fill in the blanks with the correct code to load the IMDB movie reviews dataset. After loading the data, tokenize the first movie review, and print the first 10 tokens of the tokenized review.

```python

import nltk

from ____ import ____

from nltk import word_tokenize

  

nltk.____('movie_reviews', quiet=True)

  

# Load the first movie review

review_text = ____.raw(____.fileids()[0])

  

# Apply tokenization on the review

tokenized_review = word_tokenize(review_text)

  

# Print the first 10 tokens

print("\nThe first 10 tokens are:\n", tokenized_review[:10])

```

è§£é‡Š

```python

Here's the completed Python code snippet that correctly fills in the blanks to load the IMDB movie reviews dataset, tokenize the first movie review, and print the first 10 tokens of the tokenized review:

```python

import nltk
from nltk.corpus import movie_reviews
from nltk import word_tokenize

nltk.download('movie_reviews', quiet=True)
nltk.download('punkt', quiet=True)  # Ensure the tokenizer is available

# Load the first movie review
review_text = movie_reviews.raw(movie_reviews.fileids()[0])

# Apply tokenization on the review
tokenized_review = word_tokenize(review_text)

# Print the first 10 tokens
print("\nThe first 10 tokens are:\n", tokenized_review[:10])

```

This script downloads the `movie_reviews` dataset and the necessary tokenizer (`punkt`). It then loads the text of the first movie review, tokenizes it, and prints the first 10 tokens. If you have any further questions or need more assistance, feel free to ask!

# lesson

##### Introducing Bag-of-Words Representation

In the world of text analysis, transforming raw data into a format that is both computer-friendly and preserves the essential information for further processing is crucial. One of the simplest yet versatile methods to do this is theÂ _Bag-of-Words Representation_, or BoW for short.

BoW is essentially a method to extract features from text. Imagine you have a big bag filled with words. These words can come from anywhere: a book, a website, or, in our case, movie reviews from the IMDB dataset. For each document or sentence, the BoW representation will contain the count of how many times each word appears. Most importantly, in this "bag," we don't care about the order of words, only their occurrence.

Consider this simple example with three sentences:

1. `The cat sat on the mat.`
2. `The cat sat near the mat.`
3. `The cat played with a ball.`

Using a BoW representation, our table would look like this:

||the|cat|sat|on|mat|near|played|with|a|ball|
|---|---|---|---|---|---|---|---|---|---|---|
|1|2|1|1|1|1|0|0|0|0|0|
|2|2|1|1|0|1|1|0|0|0|0|
|3|1|1|0|0|0|0|1|1|1|1|

Each sentence (document) corresponds to a row, and each unique word is a column. The values in the cells represent the word count in the given sentence.

##### Illustrating Bag-of-Words with a Simple Example

We can start practising the Bag-of-Words model by using Scikit-learnÂ `CountVectorizer`Â on the exact same three sentences:

```python

from sklearn.feature_extraction.text import CountVectorizer

# Simple example sentences
sentences = ['The cat sat on the mat.',
             'The cat sat near the mat.',
             'The cat played with a ball.']

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(sentences)

print('Feature names:')
print(vectorizer.get_feature_names_out())
print('Bag of Words Representation:')
print(X.toarray())

```

The output of the above code will be:

```

Feature names:
['ball' 'cat' 'mat' 'near' 'on' 'played' 'sat' 'the' 'with']
Bag of Words Representation:
[[0 1 1 0 1 0 1 2 0]
 [0 1 1 1 0 0 1 2 0]
 [1 1 0 0 0 1 0 1 1]]

```

From the output, you'll notice that Scikit-learnÂ `CountVectorizer`Â has done the exact thing as our previous manual process. It's created a Bag-of-Words representation for our sentences where each row corresponds to a sentence and each column to a unique word.

##### Applying Bag-of-Words to Our Dataset

Now that we know what Bag-of-Words is and what it does, let's apply it to our dataset:

```python

import nltk
from nltk.corpus import movie_reviews
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('movie_reviews')  
reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]

```

In the code snippet above, we utilize Python'sÂ `NLTK`Â module to download and import theÂ `IMDB movie reviews dataset`.

Next, we'll again use Scikit-learn'sÂ `CountVectorizer`Â to apply the BoW method to our reviews:

```python

vectorizer = CountVectorizer()
bag_of_words = vectorizer.fit_transform(reviews)

print(f"The shape of our Bag-of-Words is: {bag_of_words.shape}")

```

The output of the above code will be:

```

The shape of our Bag-of-Words is: (2000, 39659)

```

The output indicates that the result is a matrix where each row corresponds to a movie review and each column to a unique word. The entries in this matrix are word counts.

##### Understanding the Bag-of-Words Matrix and Most Used Word

Let's decode what's inside theÂ `bag_of_words`Â matrix:

```python

feature_names = vectorizer.get_feature_names_out()
first_review_word_counts = bag_of_words[0].toarray()[0]

```

Here, we retrieve the feature names (which are unique words in the reviews) from ourÂ `CountVectorizer`Â model. Then we get the word counts for a specific review - in our case, we chose the first one.

Subsequently, let's find out which word in the first review occurs the most:

```python

max_count_index = first_review_word_counts.argmax()
most_used_word = feature_names[max_count_index]

print(f"The most used word is '{most_used_word}' with a count of {first_review_word_counts[max_count_index]}")

```

Running the above code would output something like:

```python

The most used word is 'the' with a count of 38

```

The output gives away the most used word in the first review and its count. The script finds the index of the word with the highest count in the first review. Then, it uses this index to identify the corresponding word in theÂ `feature_names`. This demonstrates how we can identify the most used word in a specific review using the Bag-of-Words model.

##### Lesson Summary

Congratulations! You've successfully made it through this lesson. Today, you've learned how to implement a significant concept in the world of text classification, theÂ `Bag-of-Words`Â method. You've not only understood the theoretical aspect of it, but you also applied it on a real-world dataset using Python. You even used it to extract insights about word frequency, a crucial aspect of many text classification problems.

As we move forward in the upcoming lessons, we'll take what you've learned today, build on top of it, and continue our journey to understand and apply more advanced text classification techniques. Remember, practice makes perfect, so try to apply what you've learned today on different text data on your own. Happy coding, and see you in the next lesson!

Start practice

## ã€ŒPracticeã€
Great job! Now, let's play around with theÂ **text**. Change the provided sentences to any three sentences of your choice. Then, observe how this affects theÂ **Bag-of-Words**Â representation. Remember, the output should be aÂ **matrix**, with each row representing one of your sentences and each column representing a unique word.

```python

from sklearn.feature_extraction.text import CountVectorizer

  

# Custom sentences

sentences = ['The cat sat on the mat.',

'The cat sat near the mat.',

'The cat played with a ball.']

  

vectorizer = CountVectorizer()

X = vectorizer.fit_transform(sentences)

  

print('Feature names:')

print(vectorizer.get_feature_names_out())

print('Bag of Words Representation:')

print(X.toarray())

```

è§£é‡Š
Certainly! I'll modify the sentences and show you how it affects the Bag-of-Words representation. Here's the updated code with new sentences:

```python

from sklearn.feature_extraction.text import CountVectorizer

  

# Simple example sentences

sentences = ['Machine learning is fascinating.',

'Deep learning is a subset of machine learning.',

'We use Python for machine learning.']

  

vectorizer = CountVectorizer()

X = vectorizer.fit_transform(sentences)

  

print('Feature names:')

print(vectorizer.get_feature_names_out())

print('Bag of Words Representation:')

print(X.toarray())

```

Let's break down the filled-in parts:

1. We import `CountVectorizer` from `sklearn.feature_extraction.text`.
    
2. We create an instance of `CountVectorizer` with `vectorizer = CountVectorizer()`.
    
3. We use `fit_transform(sentences)` to both fit the vectorizer to our sentences (learning the vocabulary) and transform the sentences into a Bag-of-Words representation in one step.
    

When you run this code, you'll see output similar to this:

```

Feature names:
['deep' 'fascinating' 'for' 'is' 'learning' 'machine' 'of' 'python' 'subset' 'use' 'we']

Bag of Words Representation:
[[0 1 0 1 1 1 0 0 0 0 0]
 [1 0 0 1 2 1 1 0 1 0 0]
 [0 0 1 0 1 1 0 1 0 1 1]]

```

This output shows:

1. The unique words (features) extracted from all sentences.
2. A matrix where each row represents a sentence, and each column represents a word. The numbers in the matrix indicate how many times each word appears in each sentence.

For example, in the second sentence, 'learning' appears twice, which is reflected in the matrix.

This Bag-of-Words representation is a fundamental technique in natural language processing and is often used as a starting point for more complex text analysis tasks. Great job on exploring this concept!

# Introduction to Sparse Matrices
Hello and welcome to this lesson onÂ **"Efficient Text Data Representation with Sparse Matrices"**. As you recall, in our previous lessons, we transformed raw text data into numerical features, for example, using theÂ _Bag-of-Words_Â (BoW) orÂ _Term Frequency-Inverse Document Frequency_Â (TF-IDF) techniques. These transformation methods often create what we call "Sparse Matrices," an incredibly memory-efficient way of storing high-dimensional data.

Let's break this down a bit. In the context of text data, each unique word across all documents could be treated as a distinct feature. However, each document will only include a small subset of these available features or unique words. Meaning, most entries in our feature matrix end up being 0s, hence resulting in a sparse matrix.

We'll begin with a simple non-text matrix to illustrate sparse matrices and later connect this knowledge to our journey on text data transformation.

```python

import numpy as np
from scipy.sparse import csr_matrix, csc_matrix, coo_matrix

# Simple example matrix
vectors = np.array([
    [0, 0, 2, 3, 0],
    [4, 0, 0, 0, 6],
    [0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0],
    [0, 7, 0, 8, 0]
])

```

##### Sparse Matrix Formats: CSR

In this section, we'll investigate how we can handle sparse matrices in different formats including:Â _Compressed Sparse Row_Â (CSR),Â _Compressed Sparse Column_Â (CSC), and theÂ _Coordinate_Â (COO) formats.

We'll start with the CSR format, a common format for sparse matrices that is excellent for quick arithmetic operations and matrix vector calculations.

```python

# CSR format
sparse_csr = csr_matrix(vectors)
print("Compressed Sparse Row (CSR) Matrix:\n", sparse_csr)

```

The output of the above code will be:

```

Compressed Sparse Row (CSR) Matrix:
   (0, 2)	2
  (0, 3)	3
  (1, 0)	4
  (1, 4)	6
  (4, 1)	7
  (4, 3)	8
Observe that in the output of the Compressed Sparse Row representation, it records the values in the matrix row-wise, starting from the top. Each entry (0, 2), for example, tells us that the element in the 0th row and 2nd column is 2.

```

##Sparse Matrix Formats: CSC

Next, let's convert ourÂ `vectors`Â matrix to the CSC format. This format, like the CSR format, also forms the backbone of many operations we perform on sparse matrices. But it stores the non-zero entries column-wise, and is especially efficient for column slicing operations.

```

# CSC format
sparse_csc = csc_matrix(vectors)
print("Compressed Sparse Column (CSC) Matrix:\n", sparse_csc)

```

The output of the above code will be:

```

Compressed Sparse Column (CSC) Matrix:
   (1, 0)	4
  (4, 1)	7
  (0, 2)	2
  (0, 3)	3
  (4, 3)	8
  (1, 4)	6

```

In this Compressed Sparse Column output, the non-zero entries are stored column-wise. Essentially, CSC format is a transpose of the CSR format.

##### Sparse Matrix Formats: COO

Lastly, let's convert our example to the COO format or Coordinate List format. The COO format is another useful way to represent a sparse matrix and is simpler compared to CSR or CSC formats.

```python

# COO format
sparse_coo = coo_matrix(vectors)
print("Coordinate Format (COO) Matrix:\n", sparse_coo)

```

The output of the above code will be:

```

Coordinate Format (COO) Matrix:
   (0, 2)	2
  (0, 3)	3
  (1, 0)	4
  (1, 4)	6
  (4, 1)	7
  (4, 3)	8

```

In the COO format, or Coordinate format, the non-zero entries are represented by their own coordinates (row, column). Unlike CSC or CSR, the COO format can contain duplicate entries. This can be particularly useful when data is being accumulated in several passes and there might be instances where duplicate entries are generated. These duplicates are not immediately merged in the COO format, providing you with flexibility for subsequent processing like duplicate resolution.

##### Vectorized Operations: CSR and CSC

Sparse matrices are not just memory-efficient storage mechanisms, but they also allow us to conduct operations directly on them. Specifically, the CSR and CSC formats support these operations directly, whereas the COO format requires converting back to CSR or CSC first.

Let's see this in practice when performing a multiplication operation.

```python

Running operations on CSR and CSC matrices
weighted_csr = sparse_csr.multiply(0.5)
print("Weighted CSR:\n", weighted_csr.toarray())

```

The output of the code block above will be:

```

Weighted CSR:
 [[0.  0.  1.  1.5 0. ]
 [2.  0.  0.  0.  3. ]
 [0.  3.5 0.  4.  0. ]
 [0.  0.  0.  0.  0. ]
 [0.  0.  0.  0.  0. ]]

```

We can see the impressive CSR format efficiency in vectorized operations, which becomes crucial when performing calculations with large text data.

##### Vectorized Operations: COO

And now let's demonstrate the process of performing the same multiplication operation on the COO format, but this time requiring conversion to CSR or CSC first.

```python

# Operation on COO requires conversion to CSR or CSC first
weighted_coo = sparse_coo.tocsr().multiply(0.5)
print("Weighted COO:\n", weighted_coo.toarray())

```

The output of the above code will be:

```

Weighted COO:
 [[0.  0.  1.  1.5 0. ]
 [2.  0.  0.  0.  3. ]
 [0.  3.5 0.  4.  0. ]
 [0.  0.  0.  0.  0. ]
 [0.  0.  0.  0.  0. ]]

```

##### The Connection Between Sparse Matrices and NLP

After going through the concepts and code, you might ask - what does all this have to do with NLP? Well, remember when we transformed raw text data into either aÂ _Bag-of-Words_Â or aÂ _TF-IDF_Â representation in the previous lessons? Each unique word across all documents was treated as a distinct feature. Given the high dimensionality and inherent sparsity of the resulting feature representation, we used sparse matrices for efficient storage.

Handling of sparse matrices becomes crucial in large NLP tasks, as they allow us to operate on large datasets while maintaining computational efficiency and optimal memory usage. Therefore, understanding these different formats of sparse matrices is an essential part of your feature engineering skills for text classification.

##### Lesson Summary

Congratulations! Today, you gained an insight into sparse matrices and their different formats, how they help efficiently storing and operating on high dimensional data like that of text records in NLP. You also explored the implications of implementing vectorized operations on different sparse matrix formats. Structuring your learning and understanding these formats is paramount to efficiently handle large datasets in NLP and other machine learning tasks. In the upcoming exercises, you'll get hands-on experience with these concepts, reinforcing your understanding further. Keep up the momentum and dive into practice!

##### Topic Overview and Actualization  
ä¸»é¢˜æ¦‚è¿°ä¸ç°å®åŒ–

Today, we target duplicates and outliers to clean our data for more accurate analysis.  
ä»Šå¤©ï¼Œæˆ‘ä»¬å°†é’ˆå¯¹é‡å¤æ•°æ®å’Œå¼‚å¸¸å€¼è¿›è¡Œæ¸…ç†ï¼Œä»¥ä¾¿è¿›è¡Œæ›´å‡†ç¡®çš„åˆ†æã€‚

##### Understanding Duplicates in Data  
ç†è§£æ•°æ®ä¸­çš„é‡å¤é¡¹

Let's consider a dataset from a school containing students' details. If a student's information appears more than once, that is regarded as a duplicate. Duplicates distort data, leading to inaccurate statistics.  
è€ƒè™‘ä¸€ä¸ªåŒ…å«å­¦ç”Ÿè¯¦ç»†ä¿¡æ¯çš„å­¦æ ¡æ•°æ®é›†ã€‚å¦‚æœä¸€ä¸ªå­¦ç”Ÿçš„èµ„è®¯å‡ºç°å¤šæ¬¡ï¼Œåˆ™è¢«è§†ä¸ºé‡å¤æ•°æ®ã€‚é‡å¤æ•°æ®ä¼šæ‰­æ›²æ•°æ®ï¼Œå¯¼è‡´ç»Ÿè®¡æ•°æ®ä¸å‡†ç¡®ã€‚

## ã€ŒPracticeã€
Greetings, Stellar Navigator! For this assignment, we're focusing on model initialization and training. You will find a TODO comment in the provided starter code. Fill it in to define the Naive Bayes model and train it! You'll be able to see the difference between your model's prediction and the actual results visually on a scatter plot. Let's dive in!
ä½ å¥½ï¼Œæ˜Ÿé™…é¢†èˆªå‘˜ï¼æœ¬æ¬¡ä½œä¸šçš„é‡ç‚¹æ˜¯æ¨¡å‹åˆå§‹åŒ–å’Œè®­ç»ƒã€‚ä½ ä¼šåœ¨æä¾›çš„åˆå§‹ä»£ç ä¸­æ‰¾åˆ°ä¸€ä¸ª TODO æ³¨é‡Šã€‚è¯·å®Œæˆæ³¨é‡Šå†…å®¹ï¼Œå®šä¹‰æœ´ç´ è´å¶æ–¯æ¨¡å‹å¹¶å¯¹å…¶è¿›è¡Œè®­ç»ƒï¼ä½ å°†èƒ½å¤Ÿåœ¨æ•£ç‚¹å›¾ä¸Šç›´è§‚åœ°çœ‹åˆ°ä½ çš„æ¨¡å‹é¢„æµ‹ç»“æœä¸å®é™…ç»“æœä¹‹é—´çš„å·®å¼‚ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

```python

# Import the necessary libraries

import pandas as pd

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.naive_bayes import MultinomialNB

from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt

import datasets

  

# Load the dataset

spam_dataset = datasets.load_dataset('codesignal/sms-spam-collection', split='train')

spam_dataset = pd.DataFrame(spam_dataset)

  

# Define X (input features) and Y (output labels)

X = spam_dataset["message"]

Y = spam_dataset["label"]

  

# Perform the train test split using stratified cross-validation

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)

  

# Initialize the CountVectorizer

count_vectorizer = CountVectorizer()

  

# Fit and transform the training data

X_train_count = count_vectorizer.fit_transform(X_train)

  

# Transform the test data

X_test_count = count_vectorizer.transform(X_test)

  

# TODO: Initialize the MultinomialNB model and fit it on the training data

  

naive_bayes_model = MultinomialNB()

naive_bayes_model.fit(X_train_count, Y_train)

# Make predictions on the test data

Y_pred = naive_bayes_model.predict(X_test_count)

  

# Create a DataFrame with actual and predicted labels

results_df = pd.DataFrame({"Actual": Y_test, "Predicted": Y_pred})

  

# We now generate indices for our scatter plot for clarity

indices = range(1, 51)

  

# Plotting the comparison scatter plot for the first 50 messages

plt.figure(figsize=(10, 5))

  

# Plot actual labels

plt.scatter(indices, results_df["Actual"].values[:50], edgecolor='b', facecolors='none', label='Actual')

  

# Plot predicted labels

plt.scatter(indices, results_df["Predicted"].values[:50], edgecolor='none',color='r', label='Predicted', marker='x')

  

plt.yticks([0, 1], ['Ham', 'Spam'])

plt.ylabel('Category')

plt.xlabel('Message Number')

plt.title('Actual vs Predicted Labels for First 50 Messages')

plt.legend()

plt.show()

```

## ã€ŒPracticeã€
äº²çˆ±çš„å¤ªç©ºæ—…è¡Œè€…ï¼Œä½ çš„æŠ€èƒ½å†æ¬¡è¢«éœ€è¦ï¼åˆ©ç”¨ä½ æ‰€å­¦åˆ°çš„å…³äºæœ´ç´ è´å¶æ–¯æ¨¡å‹çš„çŸ¥è¯†ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä½¿ç”¨æ··æ·†çŸ©é˜µè¯„ä¼°ä½ çš„æ¨¡å‹ã€‚å®ç°æœ´ç´ è´å¶æ–¯æ¨¡å‹ï¼Œè¿›è¡Œé¢„æµ‹ï¼Œç„¶åä½¿ç”¨æµ‹è¯•æ•°æ®ä¸ºæ¨¡å‹ç”Ÿæˆæ··æ·†çŸ©é˜µã€‚ç»˜åˆ¶æ··æ·†çŸ©é˜µçš„ç»“æœä»¥è¿›è¡Œè§†è§‰è¯„ä¼°ã€‚Dear Space Voyager, your skills are needed once again! Using what you've learned about theÂ **Naive Bayes Model**, your mission is to evaluate your model using aÂ **confusion matrix**. Implement theÂ **Naive Bayes model**, make predictions, and then generate a confusion matrix for the model using the test data. Plot the results of the confusion matrix for visual assessment.

```python

# Import the necessary libraries

import pandas as pd

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.naive_bayes import MultinomialNB

from sklearn.model_selection import train_test_split

from sklearn import metrics

import datasets

import seaborn as sns

import matplotlib.pyplot as plt

  

# Load the dataset

spam_dataset = datasets.load_dataset('codesignal/sms-spam-collection', split='train')

spam_dataset = pd.DataFrame(spam_dataset)

  

# Define X (input features) and Y (output labels)

X = spam_dataset["message"]

Y = spam_dataset["label"]

  

# Perform the train test split using stratified cross-validation

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)

  

# TODO: Initialize the CountVectorizer and fit and transform the training data

  

# TODO: Transform the test data

  

# TODO: Initialize the MultinomialNB model

  

# TODO: Fit the model on the training data

  

# TODO: Make predictions on the test data

  

# Generate confusion matrix

confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)

  

# Create a DataFrame with confusion matrix

confusion_df = pd.DataFrame(confusion_matrix, index=['ham', 'spam'], columns=['ham', 'spam'])

  

# Print the confusion DataFrame

print(confusion_df)

  

# Plot the confusion matrix

plt.figure(figsize=(6,6))

sns.heatmap(confusion_df, annot=True, fmt='d', cmap='Blues')

plt.show()

```

è§£é‡Š

```python

# Import the necessary libraries

import pandas as pd

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.naive_bayes import MultinomialNB

from sklearn.model_selection import train_test_split

from sklearn import metrics

import datasets

import seaborn as sns

import matplotlib.pyplot as plt

  

# Load the dataset

spam_dataset = datasets.load_dataset('codesignal/sms-spam-collection', split='train')

spam_dataset = pd.DataFrame(spam_dataset)

  

# Define X (input features) and Y (output labels)

X = spam_dataset["message"]

Y = spam_dataset["label"]

  

# Perform the train test split using stratified cross-validation

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)

# TODO: Initialize the CountVectorizer and fit and transform the training data

count_vectorizer = CountVectorizer()

X_train_count = count_vectorizer.fit_transform(X_train)

  

# TODO: Transform the test data

X_test_count = count_vectorizer.transform(X_test)

  

# TODO: Initialize the MultinomialNB model

naive_bayes_model = MultinomialNB()

  

# TODO: Fit the model on the training data

naive_bayes_model.fit(X_train_count, Y_train)

  

# TODO: Make predictions on the test data

Y_pred = naive_bayes_model.predict(X_test_count)

  
  

# Generate confusion matrix

confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)

  

# Create a DataFrame with confusion matrix

confusion_df = pd.DataFrame(confusion_matrix, index=['ham', 'spam'], columns=['ham', 'spam'])

  

# Print the confusion DataFrame

print(confusion_df)

  

# Plot the confusion matrix

plt.figure(figsize=(6,6))

sns.heatmap(confusion_df, annot=True, fmt='d', cmap='Blues')

plt.show()

```

å¥½çš„,è®©æˆ‘ä»¬ç”¨æ›´ç®€å•çš„æ–¹å¼,ç»™5å²çš„å°æœ‹å‹è§£é‡Šè¿™ä¸ªæœ‰è¶£çš„æ¸¸æˆ: 1. æƒ³è±¡ä½ æœ‰ä¸€å¤§å †ä¿¡ä»¶ã€‚æœ‰äº›æ˜¯å¥½æœ‹å‹å†™çš„,æœ‰äº›æ˜¯åäººå¯„æ¥çš„éª—äººçš„ä¿¡ã€‚ 2. æˆ‘ä»¬è¦æ•™ä¸€ä¸ªæœºå™¨äººæœ‹å‹åˆ†è¾¨å“ªäº›æ˜¯å¥½ä¿¡,å“ªäº›æ˜¯åä¿¡ã€‚ 3. é¦–å…ˆ,æˆ‘ä»¬ç»™æœºå™¨äººä¸€äº›ä¿¡çœ‹,å‘Šè¯‰å®ƒå“ªäº›æ˜¯å¥½çš„,å“ªäº›æ˜¯åçš„ã€‚è¿™å°±åƒæ•™ä½ åˆ†è¾¨è‹¹æœå’Œæ©˜å­ã€‚ 4. ç„¶å,æˆ‘ä»¬ç»™æœºå™¨äººä¸€äº›æ–°çš„ä¿¡,çœ‹å®ƒèƒ½ä¸èƒ½çŒœå¯¹æ˜¯å¥½ä¿¡è¿˜æ˜¯åä¿¡ã€‚ 5. æœ€å,æˆ‘ä»¬ç”»äº†ä¸€ä¸ªæœ‰é¢œè‰²çš„å›¾ç”»: - è“è‰²çš„æ ¼å­è¶Šæ·±,è¯´æ˜æœºå™¨äººçŒœå¯¹çš„æ¬¡æ•°è¶Šå¤šã€‚ - æµ…è‰²çš„æ ¼å­è¯´æ˜æœºå™¨äººæœ‰æ—¶å€™ä¼šçŒœé”™ã€‚ 6. å¦‚æœå›¾ç”»ä¸Šæ·±è“è‰²çš„æ ¼å­å¤š,æµ…è‰²çš„æ ¼å­å°‘,å°±è¯´æ˜æˆ‘ä»¬çš„æœºå™¨äººæœ‹å‹å­¦å¾—å¾ˆå¥½! è¿™å°±åƒæ•™ä½ çš„ç©å…·æœºå™¨äººåˆ†è¾¨å¥½ä¸œè¥¿å’Œåä¸œè¥¿ã€‚å¦‚æœå®ƒå­¦å¾—å¥½,å°±èƒ½å¸®æˆ‘ä»¬æ‰¾å‡ºé‚£äº›éª—äººçš„åä¿¡,ä¿æŠ¤æˆ‘ä»¬ä¸å—éª—ã€‚æ˜¯ä¸æ˜¯å¾ˆç¥å¥‡å‘€?

```

å¥½çš„,æˆ‘æ¥å°è¯•ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šè¿™æ®µä»£ç ,è®©ä¸‰å¹´çº§çš„å°æœ‹å‹ä¹Ÿèƒ½ç†è§£: 1. é¦–å…ˆ,æˆ‘ä»¬æœ‰ä¸€å †çŸ­ä¿¡ã€‚æœ‰äº›æ˜¯æ­£å¸¸çš„çŸ­ä¿¡,æœ‰äº›æ˜¯åƒåœ¾çŸ­ä¿¡ã€‚æˆ‘ä»¬æƒ³æ•™ç”µè„‘åˆ†è¾¨å“ªäº›æ˜¯åƒåœ¾çŸ­ä¿¡ã€‚ 2. æˆ‘ä»¬æŠŠè¿™äº›çŸ­ä¿¡åˆ†æˆä¸¤ç»„:ä¸€ç»„ç”¨æ¥æ•™ç”µè„‘,å¦ä¸€ç»„ç”¨æ¥æµ‹è¯•ç”µè„‘å­¦å¾—æ€ä¹ˆæ ·ã€‚ 3. æ¥ä¸‹æ¥,æˆ‘ä»¬è¦æŠŠçŸ­ä¿¡å˜æˆæ•°å­—,å› ä¸ºç”µè„‘æ›´æ“…é•¿å¤„ç†æ•°å­—ã€‚æˆ‘ä»¬æ•°ä¸€æ•°æ¯ä¸ªçŸ­ä¿¡é‡Œæœ‰å“ªäº›è¯,å‡ºç°äº†å¤šå°‘æ¬¡ã€‚ 4. ç„¶å,æˆ‘ä»¬ç”¨ä¸€ç§å«"æœ´ç´ è´å¶æ–¯"çš„æ–¹æ³•æ¥æ•™ç”µè„‘ã€‚è¿™å°±åƒæ•™ç”µè„‘ç©ä¸€ä¸ªçŒœè°œæ¸¸æˆã€‚ 5. æ•™å®Œä¹‹å,æˆ‘ä»¬è®©ç”µè„‘çŒœæµ‹é‚£äº›æµ‹è¯•ç”¨çš„çŸ­ä¿¡æ˜¯ä¸æ˜¯åƒåœ¾çŸ­ä¿¡ã€‚ 6. æœ€å,æˆ‘ä»¬çœ‹çœ‹ç”µè„‘çŒœå¾—å¯¹ä¸å¯¹ã€‚æˆ‘ä»¬ç”»äº†ä¸€ä¸ªè¡¨æ ¼: - å·¦ä¸Šè§’çš„æ•°å­—æ˜¯ç”µè„‘æ­£ç¡®åœ°è¯´"è¿™ä¸æ˜¯åƒåœ¾çŸ­ä¿¡"çš„æ¬¡æ•°ã€‚ - å³ä¸Šè§’æ˜¯ç”µè„‘é”™è¯¯åœ°è¯´"è¿™æ˜¯åƒåœ¾çŸ­ä¿¡"çš„æ¬¡æ•°ã€‚ - å·¦ä¸‹è§’æ˜¯ç”µè„‘é”™è¯¯åœ°è¯´"è¿™ä¸æ˜¯åƒåœ¾çŸ­ä¿¡"çš„æ¬¡æ•°ã€‚ - å³ä¸‹è§’æ˜¯ç”µè„‘æ­£ç¡®åœ°è¯´"è¿™æ˜¯åƒåœ¾çŸ­ä¿¡"çš„æ¬¡æ•°ã€‚ 7. æˆ‘ä»¬è¿˜ç”»äº†ä¸€ä¸ªå½©è‰²çš„å›¾,é¢œè‰²è¶Šæ·±çš„åœ°æ–¹,æ•°å­—è¶Šå¤§ã€‚è¿™æ ·æˆ‘ä»¬ä¸€çœ¼å°±èƒ½çœ‹å‡ºç”µè„‘å­¦å¾—å¥½ä¸å¥½ã€‚ é€šè¿‡è¿™ä¸ªæ¸¸æˆ,æˆ‘ä»¬æ•™ä¼šäº†ç”µè„‘åˆ†è¾¨åƒåœ¾çŸ­ä¿¡,å°±åƒæ•™å°æœ‹å‹åˆ†è¾¨å¥åº·é£Ÿç‰©å’Œåƒåœ¾é£Ÿå“ä¸€æ ·!

##A Brief Introduction to Support Vector Machines (SVM)  
æ”¯æŒå‘é‡æœº (SVM) ç®€ä»‹

In machine learning,Â **Support Vector Machines**Â (SVMs) are classification algorithms that you can use to label data into different classes. TheÂ `SVM`Â algorithm segregates data into two groups by finding a hyperplane in a high-dimensional space (or surface, in case of more than two features) that distinctly classifies the data points. The algorithm chooses the hyperplane that represents the largest separation, or margin, between classes.  
åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæ”¯æŒå‘é‡æœº (SVM) æ˜¯ä¸€ç§åˆ†ç±»ç®—æ³•ï¼Œå¯ç”¨äºå°†æ•°æ®æ ‡è®°åˆ°ä¸åŒçš„ç±»åˆ«ä¸­ã€‚SVM ç®—æ³•é€šè¿‡åœ¨é«˜ç»´ç©ºé—´ï¼ˆå¦‚æœç‰¹å¾è¶…è¿‡ä¸¤ä¸ªï¼Œåˆ™ä¸ºæ›²é¢ï¼‰ä¸­æ‰¾åˆ°ä¸€ä¸ªèƒ½å¤Ÿæ¸…æ™°åœ°åŒºåˆ†æ•°æ®ç‚¹çš„è¶…å¹³é¢ï¼Œå°†æ•°æ®åˆ†æˆä¸¤ç»„ã€‚è¯¥ç®—æ³•é€‰æ‹©èƒ½å¤Ÿä»£è¡¨ç±»åˆ«ä¹‹é—´æœ€å¤§é—´éš”ï¼ˆæˆ–ç§°â€œè¾¹é™…â€ï¼‰çš„è¶…å¹³é¢ã€‚

`SVM`Â is extremely useful for solving nonlinear text classification problems. It can efficiently perform a non-linear classification using theÂ _"kernel trick,"_Â implicitly mapping the inputs into high-dimensional feature spaces.  
æ”¯æŒå‘é‡æœº (SVM) å¯¹äºè§£å†³éçº¿æ€§æ–‡æœ¬åˆ†ç±»é—®é¢˜éå¸¸æœ‰æ•ˆã€‚å®ƒå¯ä»¥é€šè¿‡â€œæ ¸æŠ€å·§â€æœ‰æ•ˆåœ°æ‰§è¡Œéçº¿æ€§åˆ†ç±»ï¼Œå°†è¾“å…¥éšå¼æ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ã€‚

In summary,Â `SVM`'s distinguishing factors are:  
ç»¼ä¸Šæ‰€è¿°ï¼Œæ”¯æŒå‘é‡æœºçš„æ˜¾è‘—ç‰¹ç‚¹æ˜¯ï¼š

- **Hyperplanes**: These are decision boundaries that helpÂ `SVM`Â separate data into different classes.  
    è¶…å¹³é¢ï¼šè¿™äº›æ˜¯å¸®åŠ© SVM å°†æ•°æ®åˆ†æˆä¸åŒç±»åˆ«çš„å†³ç­–è¾¹ç•Œã€‚
- **Support Vectors**: These are the data points that lie closest to the decision surface (or hyperplane). They are critical elements ofÂ `SVM`Â because they help maximize the margin of the classifier.  
    æ”¯æŒå‘é‡ï¼šè¿™äº›æ•°æ®ç‚¹æœ€æ¥è¿‘å†³ç­–é¢ï¼ˆæˆ–è¶…å¹³é¢ï¼‰ã€‚å®ƒä»¬æ˜¯æ”¯æŒå‘é‡æœºçš„å…³é”®è¦ç´ ï¼Œå› ä¸ºå®ƒä»¬æœ‰åŠ©äºæœ€å¤§åŒ–åˆ†ç±»å™¨çš„è¾¹ç•Œã€‚
- **Kernel Trick**: The kernel helpsÂ `SVM`Â to deal with non-linear input spaces by using a higher dimension space.  
    æ ¸æŠ€å·§ï¼šæ ¸å‡½æ•°é€šè¿‡å°†æ•°æ®æ˜ å°„åˆ°æ›´é«˜ç»´ç©ºé—´ï¼Œå¸®åŠ©æ”¯æŒå‘é‡æœºå¤„ç†éçº¿æ€§è¾“å…¥ç©ºé—´ã€‚
- **Soft Margin**:Â `SVM`Â allows some misclassifications in its model for better performance. This flexibility is introduced through a concept called Soft Margin.  
    è½¯é—´éš”ï¼šä¸ºäº†è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼ŒSVM å…è®¸æ¨¡å‹ä¸­å­˜åœ¨ä¸€äº›é”™è¯¯åˆ†ç±»ã€‚è¿™ç§çµæ´»æ€§æ˜¯é€šè¿‡ç§°ä¸ºè½¯é—´éš”çš„æ¦‚å¿µå¼•å…¥çš„ã€‚```
    ### Loading and Preprocessing the Data

This section is a quick revisit of the code you are already familiar with. We are just loading and preprocessing theÂ _SMS Spam Collection_Â dataset.

```

```python

# Import the necessary libraries
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import metrics
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
import datasets

# Load the dataset
spam_dataset = datasets.load_dataset('codesignal/sms-spam-collection', split='train')
spam_dataset = pd.DataFrame(spam_dataset)

# Define X (input features) and Y (output labels)
X = spam_dataset["message"]
Y = spam_dataset["label"]

# Perform the train test split using stratified cross-validation
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)

# Initialize the CountVectorizer
count_vectorizer = CountVectorizer()

# Fit and transform the training data 
X_train_count = count_vectorizer.fit_transform(X_train)

# Transform the test data
X_test_count = count_vectorizer.transform(X_test)

```

##### Implementing Support Vector Machines for Text Classification

Let's delve into the practical implementation ofÂ `SVM`Â for text classification using theÂ `Scikit-learn`Â library. We are going to introduce a newÂ `Scikit-learn`Â function,Â `SVC()`. This function is used to fit theÂ `SVM`Â model according to the given training data.

In the following Python code, we initialize theÂ `SVC`Â model, fit it with our training data, and then make predictions on the test dataset.

```python

# Initialize the SVC model
svm_model = SVC()

# Fit the model on the training data
svm_model.fit(X_train_count, Y_train)

# Make predictions on the test data
y_pred = svm_model.predict(X_test_count)

```

TheÂ `SVC`Â function takes several parameters, with the key ones being:

- `C`: This is the penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying training points correctly.
- `kernel`: Specifies the kernel type to be used in the algorithm. It can be 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable.
- `degree`: Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.
- ##### Making Predictions and Evaluating the SVM Model  
è¿›è¡Œé¢„æµ‹å’Œè¯„ä¼°æ”¯æŒå‘é‡æœºæ¨¡å‹

After building the model, the next step is to use it on unseen data and evaluate its performance. The python code for this step is shown below:  
æ¨¡å‹æ„å»ºå®Œæˆåï¼Œä¸‹ä¸€æ­¥æ˜¯åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šä½¿ç”¨è¯¥æ¨¡å‹å¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚ä¸‹é¢å±•ç¤ºäº†æ­¤æ­¥éª¤çš„ Python ä»£ç ï¼š

```python

# Make predictions on the test data
y_pred = svm_model.predict(X_test_count)

# Calculate the accuracy of the model
accuracy = metrics.accuracy_score(Y_test, y_pred)

# Print the accuracy
print(f"Accuracy of Support Vector Machines Classifier: {accuracy:.2f}")

```

The output of the above code will be:  
ä»¥ä¸Šä»£ç çš„è¾“å‡ºç»“æœä¸ºï¼š

```python

Accuracy of Support Vector Machines Classifier: 0.98

```

This output signifies that ourÂ `SVM`Â model has achieved a high accuracy, specifically 98%, in classifying messages as spam or ham, highlighting its effectiveness in text classification tasks.  
æ­¤ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ”¯æŒå‘é‡æœº (SVM) æ¨¡å‹åœ¨å°†é‚®ä»¶åˆ†ç±»ä¸ºåƒåœ¾é‚®ä»¶æˆ–éåƒåœ¾é‚®ä»¶æ–¹é¢è¾¾åˆ°äº† 98% çš„é«˜å‡†ç¡®ç‡ï¼Œå‡¸æ˜¾äº†å…¶åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

##### Lesson Summary and Upcoming Practice  
è¯¾ç¨‹æ€»ç»“å’Œå³å°†è¿›è¡Œçš„ç»ƒä¹ 

Congratulations on making it to the end of this lesson! You have now learned the theory behindÂ **Support Vector Machines**Â (SVMs) and how to use them to perform text classification in Python. You've also learned to load and preprocess the data, build theÂ `SVM`Â model, and evaluate its accuracy.  
æ­å–œä½ å®Œæˆäº†æœ¬è¯¾çš„å­¦ä¹ ï¼ä½ ç°åœ¨å·²ç»å­¦ä¹ äº†æ”¯æŒå‘é‡æœº (SVM) èƒŒåçš„ç†è®ºï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒä»¬åœ¨ Python ä¸­æ‰§è¡Œæ–‡æœ¬åˆ†ç±»ã€‚ä½ è¿˜å­¦ä¹ äº†å¦‚ä½•åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ã€æ„å»º SVM æ¨¡å‹ä»¥åŠè¯„ä¼°å…¶å‡†ç¡®æ€§ã€‚
Remember, like any other skill, programming requires practice. The upcoming practice exercises will allow you to reinforce the knowledge you've acquired in this lesson. They have been carefully designed to give you further expertise inÂ `SVM`Â and text classification. Good luck! You're doing a great job, and I'm excited to see you in the next lesson on Decision Trees for text classification.  
è®°ä½ï¼Œç¼–ç¨‹å’Œå…¶ä»–æŠ€èƒ½ä¸€æ ·ï¼Œéƒ½éœ€è¦ç»ƒä¹ ã€‚æ¥ä¸‹æ¥çš„ç»ƒä¹ å°†å¸®åŠ©ä½ å·©å›ºåœ¨æœ¬èŠ‚è¯¾ä¸­å­¦åˆ°çš„çŸ¥è¯†ã€‚è¿™äº›ç»ƒä¹ ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œæ—¨åœ¨è®©ä½ è¿›ä¸€æ­¥æŒæ¡æ”¯æŒå‘é‡æœºå’Œæ–‡æœ¬åˆ†ç±»æ–¹é¢çš„çŸ¥è¯†ã€‚ç¥ä½ å¥½è¿ï¼ä½ åšå¾—å¾ˆå¥½ï¼Œæˆ‘æœŸå¾…åœ¨ä¸‹ä¸€èŠ‚å…³äºå†³ç­–æ ‘æ–‡æœ¬åˆ†ç±»çš„è¯¾ç¨‹ä¸­è§åˆ°ä½ ã€‚

Certainly! I'll modify the code to use the polynomial kernel ('poly') instead of the linear kernel for the SVM model. Here's the adjusted code:

```python

# Import the necessary libraries 
import pandas as pd 
from sklearn.feature_extraction.text import CountVectorizer from sklearn import metrics from sklearn.svm import SVC from sklearn.model_selection import train_test_split import datasets # Load the dataset spam_dataset = datasets.load_dataset('codesignal/sms-spam-collection', split='train') spam_dataset = pd.DataFrame(spam_dataset) # Define X (input features) and Y (output labels) X = spam_dataset["message"] Y = spam_dataset["label"] # Perform the train test split using stratified cross-validation X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y) # Initialize the CountVectorizer count_vectorizer = CountVectorizer() # Fit and transform the training data X_train_count = count_vectorizer.fit_transform(X_train) # Transform the test data X_test_count = count_vectorizer.transform(X_test) # Initialize the SVC model with 'poly' kernel svm_model = SVC(kernel='poly') # Fit the model on the training data svm_model.fit(X_train_count, Y_train) # Make predictions on the test data y_pred = svm_model.predict(X_test_count) # Calculate the accuracy of the model accuracy = metrics.accuracy_score(Y_test, y_pred) # Print the accuracy print(f"Accuracy of Support Vector Machines Classifier with polynomial kernel: {accuracy:.2f}")

```

å¥½çš„,æˆ‘ä¼šå°è¯•ç”¨ç®€å•çš„è¯­è¨€æ¥è§£é‡Šè¿™æ®µä»£ç ,å°±åƒæ˜¯åœ¨è·Ÿ6å²çš„å°æœ‹å‹è®²æ•…äº‹ä¸€æ ·ã€‚ æƒ³è±¡ä¸€ä¸‹,æˆ‘ä»¬æœ‰ä¸€ä¸ªç¥å¥‡çš„æœºå™¨äººæœ‹å‹,å®ƒå¯ä»¥å¸®æˆ‘ä»¬åˆ†è¾¨çŸ­ä¿¡æ˜¯ä¸æ˜¯åƒåœ¾çŸ­ä¿¡ã€‚æˆ‘ä»¬è¦æ•™è¿™ä¸ªæœºå™¨äººæ€ä¹ˆåšåˆ°è¿™ä¸€ç‚¹ã€‚è¿™å°±æ˜¯è¿™æ®µä»£ç è¦åšçš„äº‹æƒ…ã€‚ 1. é¦–å…ˆ,æˆ‘ä»¬éœ€è¦ç»™æœºå™¨äººä¸€äº›å·¥å…·ã€‚è¿™äº›å·¥å…·å°±åƒæ˜¯æœºå™¨äººçš„çœ¼ç›å’Œå¤§è„‘,å¸®å®ƒçœ‹ä¸œè¥¿å’Œæ€è€ƒã€‚ 2. ç„¶å,æˆ‘ä»¬ç»™æœºå™¨äººä¸€å¤§å †çŸ­ä¿¡ã€‚æœ‰äº›æ˜¯å¥½çŸ­ä¿¡,æœ‰äº›æ˜¯åçŸ­ä¿¡(åƒåœ¾çŸ­ä¿¡)ã€‚ 3. æˆ‘ä»¬æŠŠè¿™äº›çŸ­ä¿¡åˆ†æˆä¸¤éƒ¨åˆ†:ä¸€éƒ¨åˆ†ç”¨æ¥æ•™æœºå™¨äºº,å¦ä¸€éƒ¨åˆ†ç”¨æ¥æµ‹è¯•æœºå™¨äººå­¦å¾—æ€ä¹ˆæ ·ã€‚ 4. æ¥ä¸‹æ¥,æˆ‘ä»¬æ•™æœºå™¨äººçœ‹çŸ­ä¿¡ã€‚æˆ‘ä»¬å‘Šè¯‰å®ƒè¦æ³¨æ„çŸ­ä¿¡é‡Œçš„æ¯ä¸ªå­—,å°±åƒä½ åœ¨å­¦ä¹ è®¤å­—ä¸€æ ·ã€‚ 5. ç°åœ¨,æˆ‘ä»¬è¦æ•™æœºå™¨äººä¸€ä¸ªç‰¹æ®Šçš„æŠ€èƒ½,å«åš"å¤šé¡¹å¼æ ¸"ã€‚è¿™ä¸ªæŠ€èƒ½å¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£æ›´å¤æ‚çš„çŸ­ä¿¡å†…å®¹ã€‚ 6. æœºå™¨äººå¼€å§‹å­¦ä¹ äº†!å®ƒä»”ç»†çœ‹æ¯ä¸€æ¡æ•™å®ƒçš„çŸ­ä¿¡,åŠªåŠ›è®°ä½å“ªäº›æ˜¯å¥½çŸ­ä¿¡,å“ªäº›æ˜¯åçŸ­ä¿¡ã€‚ 7. å­¦ä¹ å®Œå,æˆ‘ä»¬ç»™æœºå™¨äººä¸€äº›æ–°çš„çŸ­ä¿¡,çœ‹å®ƒèƒ½ä¸èƒ½æ­£ç¡®åˆ†è¾¨å‡ºå“ªäº›æ˜¯åƒåœ¾çŸ­ä¿¡ã€‚ 8. æœ€å,æˆ‘ä»¬è¦ç»™æœºå™¨äººæ‰“åˆ†ã€‚å¦‚æœå®ƒçŒœå¯¹äº†å¾ˆå¤š,å°±è¯´æ˜å®ƒå­¦å¾—å¾ˆå¥½! 9. æˆ‘ä»¬æŠŠæœºå™¨äººçš„åˆ†æ•°æ‰“å°å‡ºæ¥,çœ‹çœ‹å®ƒå­¦å¾—æ€ä¹ˆæ ·ã€‚ è¿™å°±æ˜¯æ•´ä¸ªè¿‡ç¨‹å•¦!æˆ‘ä»¬æ•™ä¼šäº†æœºå™¨äººæœ‹å‹ä¸€ä¸ªæ–°æŠ€èƒ½,è®©å®ƒå¯ä»¥å¸®æˆ‘ä»¬åˆ†è¾¨åƒåœ¾çŸ­ä¿¡ã€‚æ˜¯ä¸æ˜¯å¾ˆç¥å¥‡å‘€?

### Topic Overview and Actualization

Hello and welcome! In today's lesson, we dive into the world ofÂ **Decision Trees**Â in text classification. Decision Trees are simple yet powerful supervised learning algorithms used for classification and regression problems. In this lesson, our focus will be on understanding the Decision Tree algorithm and implementing it for a text classification problem. Let's get started!

### Understanding Decision Trees for Classification

**Decision Trees**Â are a type of flowchart-like structure in which each internal node represents a feature, each branch represents a decision rule, and each leaf node represents an outcome or a class label. The topmost node in a Decision Tree is known as the root node, which best splits the dataset.

Splitting is a process of dividing a node into two or more sub-nodes, and a Decision Tree uses certain metrics during this training phase to find the best split. These includeÂ **Entropy**,Â **Gini Index**, andÂ **Information Gain**.

The advantage of Decision Trees is that they require relatively little effort for data preparation yet can handle both categorical and numeric data. They are visually intuitive and easy to interpret.

Let's see how this interprets to our spam detection problem.

##### Loading and Preprocessing the Data

Before we dive into implementing Decision Trees, let's quickly load and preprocess our text dataset. This step will transform our dataset into a format that can be input into our machine learning models. This code block is being included for completeness:

```python

# Import the necessary libraries
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import tree
import datasets

# Load the dataset
spam_dataset = datasets.load_dataset('codesignal/sms-spam-collection', split='train')
spam_dataset = pd.DataFrame(spam_dataset)

# Define X (input features) and Y (output labels)
X = spam_dataset["message"]
Y = spam_dataset["label"]

# Perform the train test split using stratified cross-validation
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)

# Initialize the CountVectorizer
count_vectorizer = CountVectorizer()

# Fit and transform the training data 
X_train_count = count_vectorizer.fit_transform(X_train)

# Transform the test data
X_test_count = count_vectorizer.transform(X_test)

```

With our data now prepared, let's move on to implementing Decision Trees using the Scikit-learn library.

##### Implementing Decision Trees for Text Classification

In this section, we create our Decision Trees model using the Scikit-learn library:

```python

# Initialize the DecisionTreeClassifier model
decision_tree_model = tree.DecisionTreeClassifier()

# Fit the model on the training data
decision_tree_model.fit(X_train_count, Y_train)

```

Here, we initialize the model using theÂ `DecisionTreeClassifier()`Â class and then fit it to our training data with theÂ `fit()`Â method.

##### Prediction and Model Evaluation

After our model has been trained, it's time to make predictions on the test data and evaluate the model's performance:

```python

# Make predictions on the test data
y_pred = decision_tree_model.predict(X_test_count)

```

Lastly, we calculate the accuracy score, which is the ratio of the number of correct predictions to the total number of predictions. The closer this number is to 1, the better our model:

```python

# Calculate the accuracy of the model
accuracy = metrics.accuracy_score(Y_test, y_pred)

# Print the accuracy
print(f"Accuracy of Decision Tree Classifier: {accuracy:.2f}")

```

The output of the above code will be:

```python

Accuracy of Decision Tree Classifier: 0.97

```

This high accuracy score indicates that our Decision Tree model is performing exceptionally well in classifying messages as spam or not spam.

##### Lesson Summary and Practice

Great job! You've learned the theory of Decision Trees, successfully applied it to a text classification problem, and evaluated the performance of your model. Understanding and mastering Decision Trees is an essential step in your journey to becoming skilled inÂ **Natural Language Processing and Machine Learning**.
To reinforce what we've learned, the next step is to tackle some exercises that will give you hands-on experience with Decision Trees. This practical experience will reinforce your learning and deepen your understanding.

Looking forward to delving even deeper into natural language processing? Let's proceed to our next lesson:Â **Random Forest for Text Classification**. Happy Learning!

## ã€ŒPractice1ã€
Great work! Now, let's adjust theÂ `max_depth`Â of our Decision Tree model and observe any potential changes in accuracy. Change the value of theÂ `max_depth`Â parameter inÂ `DecisionTreeClassifier`Â fromÂ `5`Â toÂ `10`. After adjusting your code, run it and compare the accuracy score to that of the original model. Good luck, Stellar Navigator!

```python

# Import necessary libraries

from sklearn import tree

from sklearn import metrics

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.model_selection import train_test_split

import datasets

import pandas as pd

  

# Load the dataset

spam_dataset = datasets.load_dataset('codesignal/sms-spam-collection', split='train')

spam_dataset = pd.DataFrame(spam_dataset)

  

# Define X (input features) and Y (output labels)

X = spam_dataset["message"]

Y = spam_dataset["label"]

  

# Perform the train test split using stratified cross-validation

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)

  

# Initialize the CountVectorizer

count_vectorizer = CountVectorizer()

  

# Fit and transform the training data

X_train_count = count_vectorizer.fit_transform(X_train)

  

# Transform the test data

X_test_count = count_vectorizer.transform(X_test)

  

# Initialize the DecisionTreeClassifier model with a max_depth parameter

decision_tree_model_v2 = tree.DecisionTreeClassifier(max_depth=5)

  

# Fit the model on the training data

decision_tree_model_v2.fit(X_train_count, Y_train)

  

# Make predictions on the test data

y_pred_v2 = decision_tree_model_v2.predict(X_test_count)

  

# Calculate the accuracy of the model

accuracy_v2 = metrics.accuracy_score(Y_test, y_pred_v2)

  

# Print the accuracy

print(f"Accuracy of Decision Tree Classifier with max_depth=5: {accuracy_v2:.2f}")

```

Certainly! I'll adjust the `max_depth` parameter of the DecisionTreeClassifier from 5 to 10 and run the code to compare the accuracy scores. Here's the modified code: 

```python 

# Import necessary libraries

from sklearn import tree

from sklearn import metrics

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.model_selection import train_test_split

import datasets

import pandas as pd

  

# Load the dataset

spam_dataset = datasets.load_dataset('codesignal/sms-spam-collection', split='train')

spam_dataset = pd.DataFrame(spam_dataset)

  

# Define X (input features) and Y (output labels)

X = spam_dataset["message"]

Y = spam_dataset["label"]

  

# Perform the train test split using stratified cross-validation

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)

  

# Initialize the CountVectorizer

count_vectorizer = CountVectorizer()

  

# Fit and transform the training data

X_train_count = count_vectorizer.fit_transform(X_train)

  

# Transform the test data

X_test_count = count_vectorizer.transform(X_test)

  

# Initialize the DecisionTreeClassifier model

decision_tree_model = ____(max_depth=5)

  

# Fit the model on the training data

____.fit(____, ____)

  

# Make predictions on the test data

y_pred = decision_tree_model.___(____)

  

# Calculate the accuracy of the model

accuracy = metrics.accuracy_score(Y_test, y_pred)

  

# Print the accuracy

print(f"Accuracy of Decision Tree Classifier: {accuracy:.2f}")

``` 

Now, let's run this code and compare the accuracy to the original model with `max_depth=5`. After running the code, we should see the accuracy score for the Decision Tree Classifier with `max_depth=10`. Let's compare this to the previous accuracy score: 1. Original model (max_depth=5): 0.96 (from the previous output) 2. New model (max_depth=10): [The new accuracy score will be displayed here] By increasing the `max_depth` from 5 to 10, we've allowed the decision tree to create more complex decision boundaries. This can potentially lead to better accuracy on the training data, but it may also increase the risk of overfitting. If the new accuracy is higher, it suggests that the model benefited from the increased complexity and was able to capture more nuanced patterns in the data. However, if the accuracy is similar or lower, it might indicate that the additional depth didn't provide significant improvements or possibly led to overfitting. To further analyze the impact of changing the `max_depth`, you could consider: 1. Comparing the training accuracy to the test accuracy to check for overfitting. 2. Trying different `max_depth` values to find the optimal depth for this dataset. 3. Implementing cross-validation to get a more robust estimate of the model's performance. Remember, the goal is to find the right balance between model complexity and generalization ability. The optimal `max_depth` will depend on the specific characteristics of your dataset. Execution Error: ModuleNotFoundError Traceback (most recent call last) Cell In[3], line 6 4 from sklearn.feature_extraction.text import CountVectorizer 5 from sklearn.model_selection import train_test_split ----> 6 import datasets 7 import pandas as pd 9 # Load the dataset ModuleNotFoundError: No module named 'datasets'

å“¦,æˆ‘æ˜ç™½äº†ã€‚è®©æˆ‘ä»¬è¯•ç€ç”¨æ›´ç®€å•çš„æ–¹å¼æ¥è§£é‡Šè¿™ä¸ªæƒ…å†µ: æƒ³è±¡ä½ æœ‰ä¸€ä¸ªå¾ˆé…·çš„ç©å…·æœºå™¨äººã€‚è¿™ä¸ªæœºå™¨äººå¯ä»¥å­¦ä¹ åšä¸åŒçš„äº‹æƒ…,æ¯”å¦‚åˆ†è¾¨è‹¹æœå’Œæ©˜å­ã€‚ æœ‰æ—¶å€™,æˆ‘ä»¬æƒ³è®©æœºå™¨äººå˜å¾—æ›´èªæ˜ã€‚æ‰€ä»¥æˆ‘ä»¬ç»™å®ƒä¸€ä¸ªæ–°çš„"å¤§è„‘"ã€‚è¿™ä¸ªæ–°"å¤§è„‘"å¯ä»¥è®©æœºå™¨äººæ€è€ƒå¾—æ›´å¤šã€‚ ä½†æ˜¯,æœ‰è¶£çš„æ˜¯,è®©æœºå™¨äººæ€è€ƒæ›´å¤šå¹¶ä¸æ€»æ˜¯è®©å®ƒå˜å¾—æ›´èªæ˜ã€‚æœ‰æ—¶å€™,å®ƒå¯èƒ½ä¼šå˜å¾—æœ‰ç‚¹ç³Šæ¶‚,å› ä¸ºå®ƒæƒ³å¾—å¤ªå¤šäº†! å°±åƒä½ ç©æ¸¸æˆä¸€æ ·,æœ‰æ—¶å€™ç®€å•çš„è§„åˆ™åè€Œæ›´å®¹æ˜“èµ¢ã€‚æœºå™¨äººä¹Ÿæ˜¯è¿™æ ·,æœ‰æ—¶å€™ç®€å•ä¸€ç‚¹åè€Œèƒ½åšå¾—æ›´å¥½ã€‚ æ‰€ä»¥,æˆ‘ä»¬æ€»æ˜¯è¦è¯•è¯•çœ‹,åˆ°åº•æ˜¯è®©æœºå™¨äººæ€è€ƒå¤šä¸€ç‚¹å¥½,è¿˜æ˜¯æ€è€ƒå°‘ä¸€ç‚¹å¥½ã€‚è¿™å°±æ˜¯å¤§äººä»¬åœ¨åšçš„äº‹æƒ… - ä»–ä»¬åœ¨å¸®åŠ©æœºå™¨äººæ‰¾åˆ°æœ€å¥½çš„"æ€è€ƒæ–¹å¼",è®©å®ƒå˜å¾—æ—¢èªæ˜åˆä¸ä¼šç³Šæ¶‚ã€‚ ä½ è§‰å¾—æ€ä¹ˆæ ·?è¿™æ ·è§£é‡Šæ˜¯ä¸æ˜¯æ›´å®¹æ˜“ç†è§£äº†å‘¢?

# lesson

##### Introduction to the Random Forest for Text Classification Lesson

Welcome to the lesson onÂ **Random Forest for Text Classification**. As we continue our journey into the world of text classification techniques in Natural Language Processing (NLP), this lesson brings us to the powerful ensemble learning method - the Random Forest algorithm.

In this lesson, we will:

- Broaden our understanding of the Random Forest algorithm.
- Apply it using Python's scikit-learn package, on the SMS Spam Collection dataset.
- Evaluate our model's accuracy in classifying whether a text message is spam or not.

By the end of this lesson, you will have gained hands-on experience in implementing a Random Forest classifier, equipping you with another versatile tool in your NLP modeling toolkit.

Let the learning begin!

##### Introduction to the Random Forest for Text Classification Lesson

Welcome to the lesson onÂ **Random Forest for Text Classification**. As we continue our journey into the world of text classification techniques in Natural Language Processing (NLP), this lesson brings us to the powerful ensemble learning method - the Random Forest algorithm.

In this lesson, we will:

- Broaden our understanding of the Random Forest algorithm.
- Apply it using Python's scikit-learn package, on the SMS Spam Collection dataset.
- Evaluate our model's accuracy in classifying whether a text message is spam or not.

By the end of this lesson, you will have gained hands-on experience in implementing a Random Forest classifier, equipping you with another versatile tool in your NLP modeling toolkit.

Let the learning begin!

```python

# Import the necessary libraries
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import datasets

# Load the dataset
spam_dataset = datasets.load_dataset('codesignal/sms-spam-collection', split='train')
spam_dataset = pd.DataFrame(spam_dataset)

# Define X (input features) and Y (output labels)
X = spam_dataset["message"]
Y = spam_dataset["label"]

# Perform the train test split using stratified cross-validation
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)

# Initialize the CountVectorizer
count_vectorizer = CountVectorizer()

# Fit and transform the training data 
X_train_count = count_vectorizer.fit_transform(X_train)

# Transform the test data
X_test_count = count_vectorizer.transform(X_test)

```

Remember, theÂ `CountVectorizer`Â transforms the text data into vectors of token occurrence counts (also known as bag of words), which is required for processing by machine learning models. We also use a stratified train-test split to ensure a balanced representation of different classes within both our training and test data.

##### Random Forest Classification: Overview

**Random Forest**Â is a type of ensemble learning method, where a group of weak models work together to form a stronger predictive model. A Random Forest operates by constructing numerous decision trees during training time and outputting the class that is the mode of the classes (classification) of the individual trees.

Random Forest has several advantages over a single decision tree. Most significant among these is that by building and averaging multiple deep decision trees trained on different parts of the same training data, the Random Forest algorithm reduces the problem of overfitting.

Random Forests also handle imbalanced data well, making them a good option for our text classification task.

##### Implementing Random Forest Classifier with Scikit-learn

Now that we have a basic understanding of the Random Forest algorithm, let's train our model.

```python

# Initialize the RandomForestClassifier model
random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model on the training data
random_forest_model.fit(X_train_count, Y_train)

```

Here, the parameterÂ `n_estimators`Â defines the number of trees in the forest of the model whileÂ `random_state`Â sets a seed to the random generator, ensuring that the split you generate is replicable. The random forest model inherently handles multi-class tasks, hence we don't have to use the 'one-vs-all' method to extend it to multi-class.
è¿™é‡Œï¼Œå‚æ•° n_estimators å®šä¹‰äº†æ¨¡å‹æ£®æ—ä¸­æ ‘çš„æ•°é‡ï¼Œè€Œ random_state ä¸ºéšæœºç”Ÿæˆå™¨è®¾ç½®äº†ä¸€ä¸ªç§å­ï¼Œç¡®ä¿ç”Ÿæˆçš„åˆ’åˆ†æ˜¯å¯å¤åˆ¶çš„ã€‚éšæœºæ£®æ—æ¨¡å‹æœ¬èº«å°±èƒ½å¤„ç†å¤šåˆ†ç±»ä»»åŠ¡ï¼Œå› æ­¤æˆ‘ä»¬ä¸å¿…ä½¿ç”¨â€œä¸€å¯¹å¤šâ€æ–¹æ³•å°†å…¶æ‰©å±•åˆ°å¤šåˆ†ç±»ã€‚

##### Evaluating the ModelÂ æ¨¡å‹è¯„ä¼°

Once our model is trained, we can use it to make predictions on our test data. By comparing these predictions against the actual labels in the test set, we can evaluate how well our model is performing. One of the most straightforward metrics we can use to achieve this is accuracy, calculated as the proportion of true results among the total number of cases examined.  
æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒå¯¹æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚é€šè¿‡å°†è¿™äº›é¢„æµ‹ç»“æœä¸æµ‹è¯•é›†ä¸­å®é™…æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚å‡†ç¡®ç‡æ˜¯æœ€ç›´è§‚çš„è¯„ä¼°æŒ‡æ ‡ä¹‹ä¸€ï¼Œå®ƒæŒ‡çš„æ˜¯åœ¨æ‰€æœ‰æ ·æœ¬ä¸­é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ã€‚

```python

# Make predictions on the test data
y_pred = random_forest_model.predict(X_test_count)

# Calculate the accuracy of the model
accuracy = metrics.accuracy_score(Y_test, y_pred)

# Print the accuracy
print(f"Accuracy of Random Forest Classifier: {accuracy:.2f}")

```

The output of the above code will be:  
ä»¥ä¸Šä»£ç çš„è¾“å‡ºç»“æœä¸ºï¼š

```python

Accuracy of Random Forest Classifier: 0.97

```

This indicates that our Random Forest model was able to accurately classify 97% of the messages in the test set as spam or ham, showcasing a high level of performance.  
è¿™è¡¨æ˜æˆ‘ä»¬çš„éšæœºæ£®æ—æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°å°†æµ‹è¯•é›†ä¸­ 97% çš„æ¶ˆæ¯åˆ†ç±»ä¸ºåƒåœ¾é‚®ä»¶æˆ–éåƒåœ¾é‚®ä»¶ï¼Œå±•ç°å‡ºå¾ˆé«˜çš„æ€§èƒ½æ°´å¹³ã€‚

##### Lesson Summary and Next Steps  
è¯¾ç¨‹æ€»ç»“å’Œåç»­æ­¥éª¤

We successfully explored the Random Forest algorithm, learned how it works, and implemented it in Python to classify messages as spam or ham. Remember, choosing and training a model is just part of the machine learning pipeline. Evaluating your model's performance, and selecting the best one, is also integral to any successfulÂ **Machine Learning project**.  
æˆ‘ä»¬æˆåŠŸæ¢ç´¢äº†éšæœºæ£®æ—ç®—æ³•ï¼Œå­¦ä¹ äº†å®ƒçš„å·¥ä½œåŸç†ï¼Œå¹¶åœ¨ Python ä¸­å®ç°äº†å®ƒï¼Œä»¥å°†æ¶ˆæ¯åˆ†ç±»ä¸ºåƒåœ¾é‚®ä»¶æˆ–éåƒåœ¾é‚®ä»¶ã€‚è¯·è®°ä½ï¼Œé€‰æ‹©å’Œè®­ç»ƒæ¨¡å‹åªæ˜¯æœºå™¨å­¦ä¹ æµç¨‹çš„ä¸€éƒ¨åˆ†ã€‚è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å¹¶é€‰æ‹©æœ€ä½³æ¨¡å‹ä¹Ÿæ˜¯ä»»ä½•æˆåŠŸçš„æœºå™¨å­¦ä¹ é¡¹ç›®çš„ç»„æˆéƒ¨åˆ†ã€‚
In our upcoming exercises, you will get the opportunity to apply the concepts you've learned and further familiarize yourself with the Random Forest algorithm. These tasks will help you solidify your understanding and ensure you are able to apply these techniques to your future data science projects. Happy learning!  
åœ¨æ¥ä¸‹æ¥çš„ç»ƒä¹ ä¸­ï¼Œæ‚¨å°†æœ‰æœºä¼šè¿ç”¨æ‰€å­¦æ¦‚å¿µï¼Œå¹¶è¿›ä¸€æ­¥ç†Ÿæ‚‰éšæœºæ£®æ—ç®—æ³•ã€‚è¿™äº›ä»»åŠ¡å°†å¸®åŠ©æ‚¨å·©å›ºç†è§£ï¼Œç¡®ä¿æ‚¨èƒ½å¤Ÿå°†è¿™äº›æŠ€æœ¯åº”ç”¨åˆ°æœªæ¥çš„æ•°æ®ç§‘å­¦é¡¹ç›®ä¸­ã€‚ç¥å­¦ä¹ æ„‰å¿«ï¼