<!DOCTYPE html>
<html lang="zh-cn">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>O&#39;Reillyç³»åˆ—ã€ŠPrompt Engineering for Generative AIã€‹By James Phoenix Mike Taylor | Mengxi&#39;s Blog</title>


<link href="https://fonts.googleapis.com" rel="preconnect" />
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect" />
<link
    href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;600;700&family=Noto+Sans+SC:wght@300;400;500;600;700&display=swap"
    rel="stylesheet" />


<link rel="stylesheet" href="/css/unified-styles.css">
</head>

<body class="theme-knowledge">
    <nav class="u-nav">
    <div class="u-container u-nav__container">
        <a href="http://localhost:1313/" class="u-nav__brand">
            <span class="u-nav__logo-emoji">ğŸŒ¿</span> Mengxi&#39;s Blog
        </a>
        <ul class="u-nav__links">
            
            <li><a href="/" class="u-nav__link ">Home</a></li>
            
            <li><a href="/posts" class="u-nav__link ">Posts</a></li>
            
            <li><a href="/search" class="u-nav__link ">Search</a></li>
            
            <li><a href="/timeline/" class="u-nav__link ">Timeline</a></li>
            
            <li><a href="/guide/" class="u-nav__link ">Guide</a></li>
            
            <li><a href="/upload/" class="u-nav__link ">Upload</a></li>
            
        </ul>
    </div>
</nav>

    <main class="u-container u-py-xl">
        
<header class="u-text-center u-mb-xl">
    <h1>O&#39;Reillyç³»åˆ—ã€ŠPrompt Engineering for Generative AIã€‹By James Phoenix Mike Taylor</h1>
    <div class="u-flex u-justify-center u-gap-md u-text-light">
        <span>Dec 15, 2025</span>
        <span>â€¢</span>
        <span>181 min read</span>
    </div>
</header>

<article class="u-card" style="max-width: 800px; margin: 0 auto;">
    <div class="content">
        <h1 id="preface"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/preface01.html">Preface</a></h1>
<pre><code>The rapid pace of innovation in generative AI promises to change how we live and work, but itâ€™s getting increasingly difficult to keep up. The number ofÂ [AI papers published on arXiv is growing exponentially](https://oreil.ly/EN5ay),Â [Stable Diffusion](https://oreil.ly/QX-yy)Â has been among the fastest growing open source projects in history, and AI art toolÂ [Midjourneyâ€™s Discord server](https://oreil.ly/ZVZ5o)Â has tens of millions of members, surpassing even the largest gaming communities. What most captured the publicâ€™s imagination was OpenAIâ€™s release of ChatGPT,Â [which reached 100 million users in two months](https://oreil.ly/FbYWk), making it the fastest-growing consumer app in history. Learning to work with AI has quickly become one of the most in-demand skills.  
</code></pre>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¿«é€Ÿåˆ›æ–°æœ‰æœ›æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼ï¼Œä½†è·Ÿä¸Šå®ƒå˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚ arXiv ä¸Šå‘è¡¨çš„ AI è®ºæ–‡æ•°é‡å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼ŒStable Diffusion å·²æˆä¸ºå†å²ä¸Šå¢é•¿æœ€å¿«çš„å¼€æºé¡¹ç›®ä¹‹ä¸€ï¼ŒAI è‰ºæœ¯å·¥å…· Midjourney çš„ Discord æœåŠ¡å™¨æ‹¥æœ‰æ•°åƒä¸‡ä¼šå‘˜ï¼Œç”šè‡³è¶…è¿‡äº†æœ€å¤§çš„æ¸¸æˆç¤¾åŒºã€‚æœ€æ¿€å‘å…¬ä¼—æƒ³è±¡åŠ›çš„æ˜¯OpenAIå‘å¸ƒçš„ChatGPTï¼Œä¸¤ä¸ªæœˆå†…ç”¨æˆ·æ•°é‡å°±è¾¾åˆ°1äº¿ï¼Œæˆä¸ºå†å²ä¸Šå¢é•¿æœ€å¿«çš„æ¶ˆè´¹ç±»åº”ç”¨ç¨‹åºã€‚å­¦ä¹ ä½¿ç”¨äººå·¥æ™ºèƒ½å·²è¿…é€Ÿæˆä¸ºæœ€å—æ¬¢è¿çš„æŠ€èƒ½ä¹‹ä¸€ã€‚</p>
<p>Everyone using AI professionally quickly learns that the quality of the output depends heavily on what you provide as input. The discipline ofÂ <em>prompt engineering</em>Â has arisen as a set of best practices for improving the reliability, efficiency, and accuracy of AI models. â€œIn ten years, half of the worldâ€™s jobs will be in prompt engineering,â€Â <a href="https://oreil.ly/IdIfO">claims Robin Li</a>, the cofounder and CEO of Chinese tech giant Baidu. However, we expect prompting to be a skill required of many jobs, akin to proficiency in Microsoft Excel, rather than a popular job title in itself. This new wave of disruption is changing everything we thought we knew about computers. Weâ€™re used to writing algorithms that return the same result every timeâ€”not so for AI, where the responses are non-deterministic. Cost and latency are real factors again, after decades of Mooreâ€™s law making us complacent in expecting real-time computation at negligible cost. The biggest hurdle is the tendency of these models to confidently make things up, dubbedÂ <em>hallucination</em>, causing us to rethink the way we evaluate the accuracy of our work.<br>
æ¯ä¸ªä¸“ä¸šä½¿ç”¨äººå·¥æ™ºèƒ½çš„äººéƒ½ä¼šå¾ˆå¿«äº†è§£åˆ°ï¼Œè¾“å‡ºçš„è´¨é‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ‚¨æä¾›çš„è¾“å…¥å†…å®¹ã€‚å³æ—¶å·¥ç¨‹å­¦ç§‘ä½œä¸ºä¸€å¥—æé«˜äººå·¥æ™ºèƒ½æ¨¡å‹å¯é æ€§ã€æ•ˆç‡å’Œå‡†ç¡®æ€§çš„æœ€ä½³å®è·µè€Œå‡ºç°ã€‚ä¸­å›½ç§‘æŠ€å·¨å¤´ç™¾åº¦è”åˆåˆ›å§‹äººå…¼é¦–å¸­æ‰§è¡Œå®˜æå½¦å®è¡¨ç¤ºï¼šâ€œåå¹´å†…ï¼Œä¸–ç•Œä¸Šä¸€åŠçš„å·¥ä½œå²—ä½å°†æ¥è‡ªå³æ—¶å·¥ç¨‹ã€‚â€ç„¶è€Œï¼Œæˆ‘ä»¬é¢„è®¡æç¤ºå°†æˆä¸ºè®¸å¤šå·¥ä½œæ‰€éœ€çš„ä¸€é¡¹æŠ€èƒ½ï¼Œç±»ä¼¼äºç†Ÿç»ƒæŒæ¡ Microsoft Excelï¼Œè€Œä¸æ˜¯å…¶æœ¬èº«æ˜¯ä¸€ä¸ªæµè¡Œçš„èŒä½åç§°ã€‚è¿™æ³¢æ–°çš„é¢ è¦†æµªæ½®æ­£åœ¨æ”¹å˜æˆ‘ä»¬å¯¹è®¡ç®—æœºçš„ä¸€åˆ‡è®¤è¯†ã€‚æˆ‘ä»¬ä¹ æƒ¯äºç¼–å†™æ¯æ¬¡è¿”å›ç›¸åŒç»“æœçš„ç®—æ³•ï¼Œä½†å¯¹äºäººå·¥æ™ºèƒ½æ¥è¯´å´å¹¶éå¦‚æ­¤ï¼Œå› ä¸ºäººå·¥æ™ºèƒ½çš„å“åº”æ˜¯ä¸ç¡®å®šçš„ã€‚å‡ åå¹´æ¥ï¼Œæ‘©å°”å®šå¾‹è®©æˆ‘ä»¬æ²¾æ²¾è‡ªå–œåœ°æœŸæœ›ä»¥å¯å¿½ç•¥ä¸è®¡çš„æˆæœ¬è¿›è¡Œå®æ—¶è®¡ç®—ï¼Œæˆæœ¬å’Œå»¶è¿Ÿå†æ¬¡æˆä¸ºçœŸæ­£çš„å› ç´ ã€‚æœ€å¤§çš„éšœç¢æ˜¯è¿™äº›æ¨¡å‹å€¾å‘äºè‡ªä¿¡åœ°ç¼–é€ äº‹å®ï¼Œè¿™è¢«ç§°ä¸ºå¹»è§‰ï¼Œå¯¼è‡´æˆ‘ä»¬é‡æ–°æ€è€ƒè¯„ä¼°å·¥ä½œå‡†ç¡®æ€§çš„æ–¹å¼ã€‚</p>
<p>Weâ€™ve been working with generative AI since the GPT-3 beta in 2020, and as we saw the models progress, many early prompting tricks and hacks became no longer necessary. Over time a consistent set of principles emerged that were still useful with the newer models, and worked across both text and image generation. We have written this book based on these timeless principles, helping you learn transferable skills that will continue to be useful no matter what happens with AI over the next five years. The key to working with AI isnâ€™t â€œfiguring out how to hack the prompt by adding one magic word to the end that changes everything else,â€ asÂ <a href="https://oreil.ly/oo262">OpenAI cofounder Sam Altman asserts</a>, but what will always matter is the â€œquality of ideas and the understanding of what you want.â€ While we donâ€™t know if weâ€™ll call it â€œprompt engineeringâ€ in five years, working effectively with generative AI will only become more important.<br>
è‡ª 2020 å¹´ GPT-3 æµ‹è¯•ç‰ˆä»¥æ¥ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨ç ”ç©¶ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼Œéšç€æˆ‘ä»¬çœ‹åˆ°æ¨¡å‹çš„è¿›æ­¥ï¼Œè®¸å¤šæ—©æœŸçš„æç¤ºæŠ€å·§å’ŒæŠ€å·§å˜å¾—ä¸å†å¿…è¦ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œå‡ºç°äº†ä¸€å¥—ä¸€è‡´çš„åŸåˆ™ï¼Œè¿™äº›åŸåˆ™å¯¹äºæ–°æ¨¡å‹ä»ç„¶æœ‰ç”¨ï¼Œå¹¶ä¸”é€‚ç”¨äºæ–‡æœ¬å’Œå›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬æ ¹æ®è¿™äº›æ°¸æ’çš„åŸåˆ™ç¼–å†™äº†è¿™æœ¬ä¹¦ï¼Œå¸®åŠ©æ‚¨å­¦ä¹ å¯è½¬ç§»çš„æŠ€èƒ½ï¼Œæ— è®ºæœªæ¥äº”å¹´äººå·¥æ™ºèƒ½å‘ç”Ÿä»€ä¹ˆï¼Œè¿™äº›æŠ€èƒ½éƒ½å°†ç»§ç»­æœ‰ç”¨ã€‚æ­£å¦‚ OpenAI è”åˆåˆ›å§‹äººè¨å§†Â·å¥¥å°”ç‰¹æ›¼ (Sam Altman) æ‰€è¨€ï¼Œä½¿ç”¨äººå·¥æ™ºèƒ½çš„å…³é”®å¹¶ä¸åœ¨äºâ€œå¼„æ¸…æ¥šå¦‚ä½•é€šè¿‡åœ¨æœ«å°¾æ·»åŠ ä¸€ä¸ªç¥å¥‡çš„å•è¯æ¥æ”¹å˜å…¶ä»–ä¸€åˆ‡æ¥ç ´è§£æç¤ºâ€ï¼Œä½†æ°¸è¿œé‡è¦çš„æ˜¯â€œæƒ³æ³•çš„è´¨é‡å’Œç†è§£ä½ æƒ³è¦ä»€ä¹ˆã€‚â€è™½ç„¶æˆ‘ä»¬ä¸çŸ¥é“äº”å¹´åæ˜¯å¦ä¼šç§°ä¹‹ä¸ºâ€œå³æ—¶å·¥ç¨‹â€ï¼Œä½†æœ‰æ•ˆåœ°ä½¿ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åªä¼šå˜å¾—æ›´åŠ é‡è¦ã€‚</p>
<h1 id="software-requirements-for-this-book">Software Requirements for This Book</h1>
<p>æœ¬ä¹¦çš„è½¯ä»¶è¦æ±‚</p>
<p>All of the code in this book is in Python and was designed to be run in aÂ <a href="https://jupyter.org/">Jupyter Notebook</a>Â orÂ <a href="https://colab.research.google.com/">Google Colab notebook</a>. The concepts taught in the book are transferable to JavaScript or any other coding language if preferred, though the primary focus of this book is on prompting techniques rather than traditional coding skills. The code can all beÂ <a href="https://oreil.ly/BrightPool">found on GitHub</a>, and we will link to the relevant notebooks throughout. Itâ€™s highly recommended that you utilize theÂ <a href="https://oreil.ly/BrightPool">GitHub repository</a>Â and run the provided examples while reading the book.<br>
æœ¬ä¹¦ä¸­çš„æ‰€æœ‰ä»£ç å‡é‡‡ç”¨ Python ç¼–å†™ï¼Œæ—¨åœ¨åœ¨ Jupyter Notebook æˆ– Google Colab Notebook ä¸­è¿è¡Œã€‚ä¹¦ä¸­æ•™æˆçš„æ¦‚å¿µå¯ä»¥è½¬ç§»åˆ° JavaScript æˆ–ä»»ä½•å…¶ä»–ç¼–ç è¯­è¨€ï¼ˆå¦‚æœæ„¿æ„ï¼‰ï¼Œå°½ç®¡æœ¬ä¹¦çš„ä¸»è¦é‡ç‚¹æ˜¯æç¤ºæŠ€æœ¯è€Œä¸æ˜¯ä¼ ç»Ÿçš„ç¼–ç æŠ€èƒ½ã€‚ä»£ç éƒ½å¯ä»¥åœ¨ GitHub ä¸Šæ‰¾åˆ°ï¼Œæˆ‘ä»¬å°†åœ¨å…¨æ–‡ä¸­é“¾æ¥åˆ°ç›¸å…³ç¬”è®°æœ¬ã€‚å¼ºçƒˆå»ºè®®æ‚¨åœ¨é˜…è¯»æœ¬ä¹¦æ—¶ä½¿ç”¨ GitHub å­˜å‚¨åº“å¹¶è¿è¡Œæä¾›çš„ç¤ºä¾‹ã€‚</p>
<p>For non-notebook examples, you can run the script with the formatÂ <code>python content/chapter_x/script.py</code>Â in your terminal, whereÂ <code>x</code>Â is the chapter number andÂ <code>script.py</code>Â is the name of the script. In some instances, API keys need to be set as environment variables, and we will make that clear. The packages used update frequently, so install ourÂ <a href="https://oreil.ly/BPreq"><em>requirements.txt</em></a>Â in a virtual environment before running code examples.<br>
å¯¹äºéç¬”è®°æœ¬ç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥åœ¨ç»ˆç«¯ä¸­è¿è¡Œæ ¼å¼ä¸ºÂ <code>python content/chapter_x/script.py</code>Â çš„è„šæœ¬ï¼Œå…¶ä¸­Â <code>x</code>Â æ˜¯ç« èŠ‚ç¼–å·ï¼ŒÂ <code>script.py</code>Â æ˜¯ç« èŠ‚åç§°è„šæœ¬ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒAPI å¯†é’¥éœ€è¦è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ï¼Œæˆ‘ä»¬å°†æ˜ç¡®è¿™ä¸€ç‚¹ã€‚ä½¿ç”¨çš„è½¯ä»¶åŒ…ç»å¸¸æ›´æ–°ï¼Œå› æ­¤åœ¨è¿è¡Œä»£ç ç¤ºä¾‹ä¹‹å‰åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…æˆ‘ä»¬çš„requirements.txtã€‚</p>
<p>TheÂ <em>requirements.txt</em>Â file is generated for Python 3.9. If you want to use a different version of Python, you can generate a newÂ <em>requirements.txt</em>Â from thisÂ <a href="https://oreil.ly/YRwP7"><em>requirements.in</em></a>Â file found within the GitHub repository, by running these commands:<br>
requests.txt æ–‡ä»¶æ˜¯ä¸º Python 3.9 ç”Ÿæˆçš„ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨ä¸åŒç‰ˆæœ¬çš„ Pythonï¼Œå¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤ä» GitHub å­˜å‚¨åº“ä¸­æ‰¾åˆ°çš„requirements.in æ–‡ä»¶ç”Ÿæˆæ–°çš„requirements.txtï¼š</p>
<pre tabindex="0"><code>`pip install pip-tools`
`pip-compile requirements.in`
</code></pre><p>For Mac users:Â å¯¹äº Mac ç”¨æˆ·ï¼š</p>
<ol>
<li>
<p>Open Terminal: You can find the Terminal application in your Applications folder, under Utilities, or use Spotlight to search for it.<br>
æ‰“å¼€ç»ˆç«¯ï¼šæ‚¨å¯ä»¥åœ¨â€œåº”ç”¨ç¨‹åºâ€æ–‡ä»¶å¤¹ä¸­çš„â€œå®ç”¨ç¨‹åºâ€ä¸‹æ‰¾åˆ°ç»ˆç«¯åº”ç”¨ç¨‹åºï¼Œæˆ–ä½¿ç”¨ Spotlight è¿›è¡Œæœç´¢ã€‚</p>
</li>
<li>
<p>Navigate to your project folder: Use theÂ <code>cd</code>Â command to change the directory to your project folder. For example:Â <code>cd path/to/your/project</code>.<br>
å¯¼èˆªåˆ°æ‚¨çš„é¡¹ç›®æ–‡ä»¶å¤¹ï¼šä½¿ç”¨Â <code>cd</code>Â å‘½ä»¤å°†ç›®å½•æ›´æ”¹ä¸ºæ‚¨çš„é¡¹ç›®æ–‡ä»¶å¤¹ã€‚ä¾‹å¦‚ï¼šÂ <code>cd path/to/your/project</code>Â ã€‚</p>
</li>
<li>
<p>Create the virtual environment: Use the following command to create a virtual environment namedÂ <code>venv</code>Â (you can name it anything):Â <code>python3 -m venv venv</code>.<br>
åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼šä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºåä¸ºÂ <code>venv</code>Â çš„è™šæ‹Ÿç¯å¢ƒï¼ˆæ‚¨å¯ä»¥å°†å…¶å‘½åä¸ºä»»ä½•åç§°ï¼‰ï¼šÂ <code>python3 -m venv venv</code>Â ã€‚</p>
</li>
<li>
<p>Activate the virtual environment: Before you install packages, you need to activate the virtual environment. Do this with the commandÂ <code>source venv/bin/activate</code>.<br>
æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼šåœ¨å®‰è£…è½¯ä»¶åŒ…ä¹‹å‰ï¼Œæ‚¨éœ€è¦æ¿€æ´»è™šæ‹Ÿç¯å¢ƒã€‚ä½¿ç”¨å‘½ä»¤Â <code>source venv/bin/activate</code>Â æ‰§è¡Œæ­¤æ“ä½œã€‚</p>
</li>
<li>
<p>Install packages: Now that your virtual environment is active, you can install packages usingÂ <code>pip</code>. To install packages from theÂ <em>requirements.txt</em>Â file, useÂ <code>pip install -r requirements.txt</code>.<br>
å®‰è£…è½¯ä»¶åŒ…ï¼šç°åœ¨æ‚¨çš„è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨Â <code>pip</code>Â å®‰è£…è½¯ä»¶åŒ…ã€‚è¦ä»requirements.txt æ–‡ä»¶å®‰è£…è½¯ä»¶åŒ…ï¼Œè¯·ä½¿ç”¨Â <code>pip install -r requirements.txt</code>Â ã€‚</p>
</li>
<li>
<p>Deactivate virtual environment: When youâ€™re done, you can deactivate the virtual environment by typingÂ <strong><code>deactivate</code></strong>.<br>
åœç”¨è™šæ‹Ÿç¯å¢ƒï¼šå®Œæˆåï¼Œæ‚¨å¯ä»¥é€šè¿‡é”®å…¥Â <code>deactivate</code>Â æ¥åœç”¨è™šæ‹Ÿç¯å¢ƒã€‚</p>
</li>
</ol>
<p>For Windows users:Â å¯¹äº Windows ç”¨æˆ·ï¼š</p>
<ol>
<li>
<p>Open Command Prompt: You can search forÂ <code>cmd</code>Â in the Start menu.<br>
æ‰“å¼€å‘½ä»¤æç¤ºç¬¦ï¼šæ‚¨å¯ä»¥åœ¨â€œå¼€å§‹â€èœå•ä¸­æœç´¢Â <code>cmd</code>Â ã€‚</p>
</li>
<li>
<p>Navigate to your project folder: Use theÂ <code>cd</code>Â command to change the directory to your project folder. For example:Â <code>cd path\to\your\project</code>.<br>
å¯¼èˆªåˆ°æ‚¨çš„é¡¹ç›®æ–‡ä»¶å¤¹ï¼šä½¿ç”¨Â <code>cd</code>Â å‘½ä»¤å°†ç›®å½•æ›´æ”¹ä¸ºæ‚¨çš„é¡¹ç›®æ–‡ä»¶å¤¹ã€‚ä¾‹å¦‚ï¼šÂ <code>cd path\to\your\project</code>Â ã€‚</p>
</li>
<li>
<p>Create the virtual environment: Use the following command to create a virtual environment namedÂ <code>venv</code>:Â <code>python -m venv venv</code>.<br>
åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼šä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºåä¸ºÂ <code>venv</code>Â çš„è™šæ‹Ÿç¯å¢ƒï¼šÂ <code>python -m venv venv</code>Â ã€‚</p>
</li>
<li>
<p>Activate the virtual environment: To activate the virtual environment on Windows, useÂ <code>.\venv\Scripts\activate</code>.<br>
æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼šè¦åœ¨ Windows ä¸Šæ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼Œè¯·ä½¿ç”¨Â <code>.\venv\Scripts\activate</code>Â ã€‚</p>
</li>
<li>
<p>Install packages: With the virtual environment active, install the required packages:Â <code>pip install -r requirements.txt</code>.<br>
å®‰è£…è½¯ä»¶åŒ…ï¼šåœ¨è™šæ‹Ÿç¯å¢ƒå¤„äºæ´»åŠ¨çŠ¶æ€çš„æƒ…å†µä¸‹ï¼Œå®‰è£…æ‰€éœ€çš„è½¯ä»¶åŒ…ï¼šÂ <code>pip install -r requirements.txt</code>Â ã€‚</p>
</li>
<li>
<p>Deactivate the virtual environment: To exit the virtual environment, simply type:Â <code>deactivate</code>.<br>
åœç”¨è™šæ‹Ÿç¯å¢ƒï¼šè¦é€€å‡ºè™šæ‹Ÿç¯å¢ƒï¼Œåªéœ€é”®å…¥ï¼šÂ <code>deactivate</code>Â ã€‚</p>
</li>
</ol>
<p>Here are some additional tips on setup:<br>
ä»¥ä¸‹æ˜¯æœ‰å…³è®¾ç½®çš„ä¸€äº›é™„åŠ æç¤ºï¼š</p>
<ul>
<li>
<p>Always ensure your Python is up-to-date to avoid compatibility issues.<br>
å§‹ç»ˆç¡®ä¿æ‚¨çš„ Python æ˜¯æœ€æ–°çš„ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜ã€‚</p>
</li>
<li>
<p>Remember to activate your virtual environment whenever you work on the project.<br>
æ¯å½“æ‚¨å¤„ç†é¡¹ç›®æ—¶ï¼Œè¯·è®°ä½æ¿€æ´»æ‚¨çš„è™šæ‹Ÿç¯å¢ƒã€‚</p>
</li>
<li>
<p>TheÂ <em>requirements.txt</em>Â file should be in the same directory where you create your virtual environment, or you should specify the path to it when usingÂ <code>pip install -r</code>.<br>
requirements.txt æ–‡ä»¶åº”è¯¥ä½äºæ‚¨åˆ›å»ºè™šæ‹Ÿç¯å¢ƒçš„åŒä¸€ç›®å½•ä¸­ï¼Œæˆ–è€…æ‚¨åº”è¯¥åœ¨ä½¿ç”¨Â <code>pip install -r</code>Â æ—¶æŒ‡å®šå®ƒçš„è·¯å¾„ã€‚</p>
</li>
</ul>
<p>Access to an OpenAI developer account is assumed, as yourÂ <code>OPENAI_API_KEY</code>Â must be set as an environment variable in any examples importing the OpenAI library, for which we use version 1.0. Quick-start instructions for setting up your development environment can be found inÂ <a href="https://oreil.ly/YqbrY">OpenAIâ€™s documentation</a>Â on their website.<br>
å‡è®¾å¯ä»¥è®¿é—® OpenAI å¼€å‘è€…å¸æˆ·ï¼Œå› ä¸ºåœ¨å¯¼å…¥ OpenAI åº“çš„ä»»ä½•ç¤ºä¾‹ä¸­ï¼Œæ‚¨çš„Â <code>OPENAI_API_KEY</code>Â å¿…é¡»è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨ç‰ˆæœ¬ 1.0ã€‚æœ‰å…³è®¾ç½®å¼€å‘ç¯å¢ƒçš„å¿«é€Ÿå…¥é—¨è¯´æ˜ï¼Œè¯·å‚é˜… OpenAI ç½‘ç«™ä¸Šçš„æ–‡æ¡£ã€‚</p>
<p>You must also ensure thatÂ <em>billing is enabled</em>Â on your OpenAI account and that a valid payment method is attached to run some of the code within the book. The examples in the book use GPT-4 where not stated, though we do briefly cover Anthropicâ€™s competingÂ <a href="https://oreil.ly/jY8Ai">Claude 3 model</a>, as well as Metaâ€™s open sourceÂ <a href="https://oreil.ly/BbXZ3">Llama 3</a>Â andÂ <a href="https://oreil.ly/KYgij">Google Gemini</a>.<br>
æ‚¨è¿˜å¿…é¡»ç¡®ä¿æ‚¨çš„ OpenAI å¸æˆ·å¯ç”¨äº†è®¡è´¹åŠŸèƒ½ï¼Œå¹¶ä¸”é™„åŠ äº†æœ‰æ•ˆçš„ä»˜æ¬¾æ–¹å¼æ¥è¿è¡Œä¹¦ä¸­çš„æŸäº›ä»£ç ã€‚ä¹¦ä¸­çš„ç¤ºä¾‹ä½¿ç”¨ GPT-4ï¼ˆæœªè¯´æ˜ï¼‰ï¼Œä½†æˆ‘ä»¬ç¡®å®ç®€è¦ä»‹ç»äº† Anthropic çš„ç«äº‰ Claude 3 æ¨¡å‹ï¼Œä»¥åŠ Meta çš„å¼€æº Llama 3 å’Œ Google Geminiã€‚</p>
<p>For image generation we useÂ <a href="https://www.midjourney.com/">Midjourney</a>, for which you need a Discord account to sign up, though these principles apply equally to DALL-E 3 (available with a ChatGPT Plus subscription or via the API) or Stable Diffusion (available as anÂ <a href="https://oreil.ly/cmTtW">API</a>Â or it canÂ <a href="https://oreil.ly/Ha0T5">run locally</a>Â on your computer if it has a GPU). The image generation examples in this book use Midjourney v6, Stable Diffusion v1.5 (as many extensions are still only compatible with this version), orÂ <a href="https://oreil.ly/S0P4s">Stable Diffusion XL</a>, and we specify the differences when this is important.<br>
å¯¹äºå›¾åƒç”Ÿæˆï¼Œæˆ‘ä»¬ä½¿ç”¨ Midjourneyï¼Œæ‚¨éœ€è¦æ³¨å†Œä¸€ä¸ª Discord å¸æˆ·ï¼Œå°½ç®¡è¿™äº›åŸåˆ™åŒæ ·é€‚ç”¨äº DALL-E 3ï¼ˆé€šè¿‡ ChatGPT Plus è®¢é˜…æˆ–é€šè¿‡ API æä¾›ï¼‰æˆ– Stable Diffusionï¼ˆä½œä¸º API æä¾›æˆ–å®ƒï¼‰å¦‚æœæ‚¨çš„è®¡ç®—æœºæœ‰ GPUï¼Œåˆ™å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œï¼‰ã€‚æœ¬ä¹¦ä¸­çš„å›¾åƒç”Ÿæˆç¤ºä¾‹ä½¿ç”¨ Midjourney v6ã€Stable Diffusion v1.5ï¼ˆå› ä¸ºè®¸å¤šæ‰©å±•ä»ç„¶åªä¸æ­¤ç‰ˆæœ¬å…¼å®¹ï¼‰æˆ– Stable Diffusion XLï¼Œå¹¶ä¸”å½“è¿™å¾ˆé‡è¦æ—¶æˆ‘ä»¬ä¼šæŒ‡å®šå·®å¼‚ã€‚</p>
<p>We provide examples using open source libraries wherever possible, though we do include commercial vendors where appropriateâ€”for example,Â <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch05.html#vector_databases_05">ChapterÂ 5</a>Â on vector databases demonstrates both FAISS (an open source library) and Pinecone (a paid vendor). The examples demonstrated in the book should be easily modifiable for alternative models and vendors, and the skills taught are transferable.Â <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch04.html#advanced_text_04">ChapterÂ 4</a>Â on advanced text generation is focused on the LLM framework LangChain, andÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch09.html#advanced_image_09">ChapterÂ 9</a>Â on advanced image generation is built on AUTOMATIC1111â€™s open source Stable Diffusion Web UI.<br>
æˆ‘ä»¬å°½å¯èƒ½æä¾›ä½¿ç”¨å¼€æºåº“çš„ç¤ºä¾‹ï¼Œå°½ç®¡æˆ‘ä»¬ç¡®å®åœ¨é€‚å½“çš„æƒ…å†µä¸‹åŒ…å«äº†å•†ä¸šä¾›åº”å•†ï¼Œä¾‹å¦‚ï¼Œå…³äºçŸ¢é‡æ•°æ®åº“çš„ç¬¬ 5 ç« æ¼”ç¤ºäº† FAISSï¼ˆå¼€æºåº“ï¼‰å’Œ Pineconeï¼ˆä»˜è´¹ä¾›åº”å•†ï¼‰ã€‚ä¹¦ä¸­æ¼”ç¤ºçš„ç¤ºä¾‹åº”è¯¥å¯ä»¥è½»æ¾ä¿®æ”¹ä»¥é€‚åº”æ›¿ä»£æ¨¡å‹å’Œä¾›åº”å•†ï¼Œå¹¶ä¸”æ‰€æ•™æˆçš„æŠ€èƒ½æ˜¯å¯ä»¥è½¬ç§»çš„ã€‚ç¬¬ 4 ç« å…³äºé«˜çº§æ–‡æœ¬ç”Ÿæˆçš„é‡ç‚¹æ˜¯ LLM æ¡†æ¶ LangChainï¼Œç¬¬ 9 ç« å…³äºé«˜çº§å›¾åƒç”Ÿæˆçš„å†…å®¹åŸºäº AUTOMATIC1111 çš„å¼€æº Stable Diffusion Web UIã€‚</p>
<h1 id="conventions-used-in-this-book">Conventions Used in This Book</h1>
<p>æœ¬ä¹¦ä¸­ä½¿ç”¨çš„çº¦å®š</p>
<p>The following typographical conventions are used in this book:<br>
æœ¬ä¹¦ä½¿ç”¨ä»¥ä¸‹å°åˆ·çº¦å®šï¼š</p>
<p><em>ItalicÂ æ–œä½“</em></p>
<p>Indicates new terms, URLs, email addresses, filenames, and file extensions.<br>
è¡¨ç¤ºæ–°æœ¯è¯­ã€URLã€ç”µå­é‚®ä»¶åœ°å€ã€æ–‡ä»¶åå’Œæ–‡ä»¶æ‰©å±•åã€‚</p>
<p><code>Constant width</code></p>
<p>Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.<br>
ç”¨äºç¨‹åºåˆ—è¡¨ï¼Œä»¥åŠåœ¨æ®µè½ä¸­å¼•ç”¨ç¨‹åºå…ƒç´ ï¼Œä¾‹å¦‚å˜é‡æˆ–å‡½æ•°åç§°ã€æ•°æ®åº“ã€æ•°æ®ç±»å‹ã€ç¯å¢ƒå˜é‡ã€è¯­å¥å’Œå…³é”®å­—ã€‚</p>
<p><strong><code>Constant width bold</code></strong></p>
<p>Shows commands or other text that should be typed literally by the user.<br>
æ˜¾ç¤ºåº”ç”±ç”¨æˆ·é€å­—é”®å…¥çš„å‘½ä»¤æˆ–å…¶ä»–æ–‡æœ¬ã€‚</p>
<p><em><code>Constant width italic</code></em></p>
<p>Shows text that should be replaced with user-supplied values or by values determined by context.<br>
æ˜¾ç¤ºåº”æ›¿æ¢ä¸ºç”¨æˆ·æä¾›çš„å€¼æˆ–ä¸Šä¸‹æ–‡ç¡®å®šçš„å€¼çš„æ–‡æœ¬ã€‚</p>
<h6 id="tip">TIP</h6>
<p>This element signifies a tip or suggestion.<br>
è¯¥å…ƒç´ è¡¨ç¤ºæç¤ºæˆ–å»ºè®®ã€‚</p>
<h6 id="noteç¬”è®°">NOTEÂ ç¬”è®°</h6>
<p>This element signifies a general note.<br>
è¯¥å…ƒç´ è¡¨ç¤ºä¸€èˆ¬æ³¨é‡Šã€‚</p>
<h6 id="warningè­¦å‘Š">WARNINGÂ è­¦å‘Š</h6>
<p>This element indicates a warning or caution.<br>
è¯¥å…ƒç´ è¡¨ç¤ºè­¦å‘Šæˆ–è­¦å‘Šã€‚</p>
<p>Throughout the book we reinforce what we call the Five Principles of Prompting, identifying which principle is most applicable to the example at hand. You may want to refer toÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#five_principles_01">ChapterÂ 1</a>, which describes the principles in detail.<br>
åœ¨æ•´æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬å¼ºåŒ–äº†æ‰€è°“çš„â€œæç¤ºäº”é¡¹åŸåˆ™â€ï¼Œç¡®å®šå“ªé¡¹åŸåˆ™æœ€é€‚ç”¨äºå½“å‰çš„ç¤ºä¾‹ã€‚æ‚¨å¯èƒ½éœ€è¦å‚è€ƒç¬¬ 1 ç« ï¼Œå…¶ä¸­è¯¦ç»†æè¿°äº†è¿™äº›åŸåˆ™ã€‚</p>
<h1 id="principle-nameåŸç†åç§°">PRINCIPLE NAMEÂ åŸç†åç§°</h1>
<p>This will explain how the principle is applied to the current example or section of text.<br>
è¿™å°†è§£é‡Šå¦‚ä½•å°†è¯¥åŸç†åº”ç”¨äºå½“å‰çš„ç¤ºä¾‹æˆ–æ–‡æœ¬éƒ¨åˆ†ã€‚</p>
<h1 id="using-code-examplesä½¿ç”¨ä»£ç ç¤ºä¾‹">Using Code ExamplesÂ ä½¿ç”¨ä»£ç ç¤ºä¾‹</h1>
<p>Supplemental material (code examples, exercises, etc.) is available for download atÂ <a href="https://oreil.ly/prompt-engineering-for-generative-ai"><em>https://oreil.ly/prompt-engineering-for-generative-ai</em></a>.<br>
è¡¥å……ææ–™ï¼ˆä»£ç ç¤ºä¾‹ã€ç»ƒä¹ ç­‰ï¼‰å¯åœ¨ <a href="https://oreil.ly/prompt-engineering-for-generative-ai">https://oreil.ly/prompt-engineering-for-generative-ai</a> ä¸‹è½½ã€‚</p>
<p>If you have a technical question or a problem using the code examples, please send email toÂ <a href="mailto:bookquestions@oreilly.com"><em>bookquestions@oreilly.com</em></a>.<br>
å¦‚æœæ‚¨æœ‰æŠ€æœ¯é—®é¢˜æˆ–ä½¿ç”¨ä»£ç ç¤ºä¾‹æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·å‘é€ç”µå­é‚®ä»¶è‡³ <a href="mailto:bookquestions@oreilly.com">bookquestions@oreilly.com</a>ã€‚</p>
<p>This book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless youâ€™re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from Oâ€™Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your productâ€™s documentation does require permission.<br>
æœ¬ä¹¦æ—¨åœ¨å¸®åŠ©æ‚¨å®Œæˆå·¥ä½œã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœæœ¬ä¹¦æä¾›äº†ç¤ºä¾‹ä»£ç ï¼Œæ‚¨å°±å¯ä»¥åœ¨æ‚¨çš„ç¨‹åºå’Œæ–‡æ¡£ä¸­ä½¿ç”¨å®ƒã€‚é™¤éæ‚¨è¦å¤åˆ¶å¤§éƒ¨åˆ†ä»£ç ï¼Œå¦åˆ™æ‚¨æ— éœ€è”ç³»æˆ‘ä»¬ä»¥è·å¾—è®¸å¯ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨æœ¬ä¹¦ä¸­çš„å‡ æ®µä»£ç ç¼–å†™ä¸€ä¸ªç¨‹åºä¸éœ€è¦è®¸å¯ã€‚é”€å”®æˆ–åˆ†å‘ Oâ€™Reilly ä¹¦ç±ä¸­çš„ç¤ºä¾‹ç¡®å®éœ€è¦è®¸å¯ã€‚é€šè¿‡å¼•ç”¨æœ¬ä¹¦å’Œç¤ºä¾‹ä»£ç æ¥å›ç­”é—®é¢˜ä¸éœ€è¦è®¸å¯ã€‚å°†æœ¬ä¹¦ä¸­çš„å¤§é‡ç¤ºä¾‹ä»£ç åˆå¹¶åˆ°æ‚¨çš„äº§å“æ–‡æ¡£ä¸­ç¡®å®éœ€è¦è®¸å¯ã€‚</p>
<p>We appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: â€œ<em>Prompt Engineering for Generative AI</em>Â by James Phoenix and Mike Taylor (Oâ€™Reilly). Copyright 2024 Saxifrage, LLC and Just Understanding Data LTD, 978-1-098-15343-4.â€<br>
æˆ‘ä»¬èµèµä½†é€šå¸¸ä¸è¦æ±‚å½’å±ã€‚å½’å±é€šå¸¸åŒ…æ‹¬æ ‡é¢˜ã€ä½œè€…ã€å‡ºç‰ˆå•†å’Œ ISBNã€‚ä¾‹å¦‚ï¼šâ€œJames Phoenix å’Œ Mike Taylor (Oâ€™Reilly) çš„ã€Šç”Ÿæˆå¼ AI å¿«é€Ÿå·¥ç¨‹ã€‹ã€‚ç‰ˆæƒæ‰€æœ‰ 2024 Saxifrage, LLC å’Œ Just Understanding Data LTDï¼Œ978-1-098-15343-4ã€‚â€</p>
<p>If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us atÂ <a href="mailto:permissions@oreilly.com"><em>permissions@oreilly.com</em></a>.<br>
å¦‚æœæ‚¨è®¤ä¸ºæ‚¨å¯¹ä»£ç ç¤ºä¾‹çš„ä½¿ç”¨ä¸ç¬¦åˆåˆç†ä½¿ç”¨æˆ–ä¸Šè¿°è®¸å¯çš„èŒƒå›´ï¼Œè¯·éšæ—¶é€šè¿‡permissions@oreilly.com ä¸æˆ‘ä»¬è”ç³»ã€‚</p>
<h1 id="oreilly-online-learning">Oâ€™Reilly Online Learning</h1>
<p>å¥¥è±åˆ©åœ¨çº¿å­¦ä¹ </p>
<h6 id="noteç¬”è®°-1">NOTEÂ ç¬”è®°</h6>
<p>For more than 40 years,Â <a href="https://oreilly.com/"><em>Oâ€™Reilly Media</em></a>Â has provided technology and business training, knowledge, and insight to help companies succeed.<br>
40 å¤šå¹´æ¥ï¼ŒOâ€™Reilly Media ä¸€ç›´æä¾›æŠ€æœ¯å’Œä¸šåŠ¡åŸ¹è®­ã€çŸ¥è¯†å’Œè§è§£æ¥å¸®åŠ©å…¬å¸å–å¾—æˆåŠŸã€‚</p>
<p>Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. Oâ€™Reillyâ€™s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from Oâ€™Reilly and 200+ other publishers. For more information, visitÂ <a href="https://oreilly.com/"><em>https://oreilly.com</em></a>.<br>
æˆ‘ä»¬ç‹¬ç‰¹çš„ä¸“å®¶å’Œåˆ›æ–°è€…ç½‘ç»œé€šè¿‡ä¹¦ç±ã€æ–‡ç« å’Œæˆ‘ä»¬çš„åœ¨çº¿å­¦ä¹ å¹³å°åˆ†äº«ä»–ä»¬çš„çŸ¥è¯†å’Œä¸“ä¸šçŸ¥è¯†ã€‚ Oâ€™Reilly çš„åœ¨çº¿å­¦ä¹ å¹³å°è®©æ‚¨å¯ä»¥æŒ‰éœ€è®¿é—®å®æ—¶åŸ¹è®­è¯¾ç¨‹ã€æ·±å…¥å­¦ä¹ è·¯å¾„ã€äº¤äº’å¼ç¼–ç ç¯å¢ƒä»¥åŠæ¥è‡ª Oâ€™Reilly å’Œ 200 å¤šå®¶å…¶ä»–å‡ºç‰ˆå•†çš„å¤§é‡æ–‡æœ¬å’Œè§†é¢‘ã€‚æ¬²äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—® <a href="https://oreilly.com">https://oreilly.com</a>ã€‚</p>
<h1 id="how-to-contact-us">How to Contact Us</h1>
<p>å¦‚ä½•è”ç³»æˆ‘ä»¬</p>
<p>Please address comments and questions concerning this book to the publisher:<br>
è¯·å‘å‡ºç‰ˆå•†æå‡ºæœ‰å…³æœ¬ä¹¦çš„æ„è§å’Œé—®é¢˜ï¼š</p>
<ul>
<li>Oâ€™Reilly Media, Inc.Â å¥¥è±åˆ©åª’ä½“å…¬å¸</li>
<li>1005 Gravenstein Highway North<br>
æ ¼é›·æ–‡æ–¯å¦å…¬è·¯åŒ—1005å·</li>
<li>Sebastopol, CA 95472Â å¡ç“¦æ–¯æ‰˜æ³¢å°”, CA 95472</li>
<li>800-889-8969 (in the United States or Canada)<br>
800-889-8969ï¼ˆç¾å›½æˆ–åŠ æ‹¿å¤§ï¼‰</li>
<li>707-827-7019 (international or local)<br>
707-827-7019ï¼ˆå›½é™…æˆ–æœ¬åœ°ï¼‰</li>
<li>707-829-0104 (fax)Â 707-829-0104ï¼ˆä¼ çœŸï¼‰</li>
<li><a href="mailto:support@oreilly.com"><em>support@oreilly.com</em></a>
<a href="https://www.oreilly.com/about/contact.html">https://www.oreilly.com/about/contact.html</a>
We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at <a href="https://oreil.ly/prompt-engineering-generativeAI">https://oreil.ly/prompt-engineering-generativeAI</a>.
æˆ‘ä»¬æœ‰æœ¬ä¹¦çš„ç½‘é¡µï¼Œå…¶ä¸­åˆ—å‡ºäº†å‹˜è¯¯è¡¨ã€ç¤ºä¾‹å’Œä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚æ‚¨å¯ä»¥é€šè¿‡ <a href="https://oreil.ly/prompt-engineering-generativeAI">https://oreil.ly/prompt-engineering-generativeAI</a> è®¿é—®æ­¤é¡µé¢ã€‚</li>
</ul>
<p>For news and information about our books and courses, visit <a href="https://oreilly.com">https://oreilly.com</a>.
æœ‰å…³æˆ‘ä»¬çš„ä¹¦ç±å’Œè¯¾ç¨‹çš„æ–°é—»å’Œä¿¡æ¯ï¼Œè¯·è®¿é—® <a href="https://oreilly.com">https://oreilly.com</a>ã€‚</p>
<p>Find us on LinkedIn: <a href="https://linkedin.com/company/oreilly-media">https://linkedin.com/company/oreilly-media</a>.
åœ¨ LinkedIn ä¸Šæ‰¾åˆ°æˆ‘ä»¬ï¼šhttps://linkedin.com/company/oreilly-mediaã€‚</p>
<p>Watch us on YouTube: <a href="https://youtube.com/oreillymedia">https://youtube.com/oreillymedia</a>.
åœ¨ YouTube ä¸Šè§‚çœ‹æˆ‘ä»¬çš„è§†é¢‘ï¼šhttps://youtube.com/oreillymediaã€‚</p>
<p>AcknowledgmentsÂ è‡´è°¢
Weâ€™d like to thank the following people for their contribution in conducting a technical review of the book and their patience in correcting a fast-moving target:
æˆ‘ä»¬è¦æ„Ÿè°¢ä»¥ä¸‹äººå‘˜å¯¹æœ¬ä¹¦è¿›è¡ŒæŠ€æœ¯å®¡æŸ¥æ‰€åšçš„è´¡çŒ®ä»¥åŠä»–ä»¬åœ¨çº æ­£å¿«é€Ÿå˜åŒ–çš„ç›®æ ‡æ–¹é¢çš„è€å¿ƒï¼š</p>
<p>Mayo Oshin, early LangChain contributor and founder at SeinnAI Analytics
Mayo Oshinï¼ŒLangChain æ—©æœŸè´¡çŒ®è€…å’Œ SeinnAI Analytics åˆ›å§‹äºº</p>
<p>Ellis Crosby, founder at Scarlett Panda and AI agency Incremen.to
åŸƒåˆ©æ–¯Â·å…‹ç½—æ–¯æ¯” (Ellis Crosby)ï¼ŒScarlett Panda å’Œäººå·¥æ™ºèƒ½æœºæ„ Incremen.to çš„åˆ›å§‹äºº</p>
<p>Dave Pawson, Oâ€™Reilly author of XSL-FO
Dave Pawsonï¼ŒOâ€™Reilly XSL-FO çš„ä½œè€…</p>
<p>Mark Phoenix, a senior software engineer
é©¬å…‹Â·è²å°¼å…‹æ–¯ (Mark Phoenix)ï¼Œé«˜çº§è½¯ä»¶å·¥ç¨‹å¸ˆ</p>
<p>Aditya Goel, GenAI consultant
Aditya Goelï¼ŒGenAI é¡¾é—®</p>
<p>We are also grateful to our families for their patience and understanding and would like to reassure them that we still prefer talking to them over ChatGPT.
æˆ‘ä»¬è¿˜æ„Ÿè°¢å®¶äººçš„è€å¿ƒå’Œç†è§£ï¼Œå¹¶å‘ä»–ä»¬ä¿è¯æˆ‘ä»¬ä»ç„¶æ›´å–œæ¬¢ä¸ä»–ä»¬äº¤è°ˆè€Œä¸æ˜¯ ChatGPTã€‚</p>
<h1 id="1-the-five-principles-of-prompting"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html">1. The Five Principles Of Prompting</a></h1>
<pre><code># Chapter 1.Â The Five Principles of Prompting  
</code></pre>
<p>ç¬¬ä¸€ç«  æç¤ºçš„äº”é¡¹åŸåˆ™</p>
<p><em>Prompt engineering</em>Â is the process of discoveringÂ prompts that reliably yield useful or desired results.<br>
æç¤ºå·¥ç¨‹æ˜¯å‘ç°èƒ½å¤Ÿå¯é åœ°äº§ç”Ÿæœ‰ç”¨æˆ–æœŸæœ›ç»“æœçš„æç¤ºçš„è¿‡ç¨‹ã€‚</p>
<p>AÂ <em>prompt</em>Â is the input you provide, typicallyÂ text, when interfacing with an AI model like ChatGPT or Midjourney. The prompt serves as a set of instructions the model uses to predict theÂ desired response: text fromÂ <em>large language models</em>Â (LLMs) likeÂ <a href="https://chat.openai.com/">ChatGPT</a>, or images fromÂ <em>diffusion models</em>Â likeÂ <a href="https://www.midjourney.com/">Midjourney</a>.<br>
æç¤ºæ˜¯æ‚¨åœ¨ä¸ ChatGPT æˆ– Midjourney ç­‰ AI æ¨¡å‹äº¤äº’æ—¶æä¾›çš„è¾“å…¥ï¼Œé€šå¸¸æ˜¯æ–‡æœ¬ã€‚æç¤ºå……å½“æ¨¡å‹ç”¨äºé¢„æµ‹æ‰€éœ€å“åº”çš„ä¸€ç»„æŒ‡ä»¤ï¼šæ¥è‡ª ChatGPT ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„æ–‡æœ¬ï¼Œæˆ–æ¥è‡ª Midjourney ç­‰æ‰©æ•£æ¨¡å‹çš„å›¾åƒã€‚</p>
<p>Here is a simple example of a prompt input forÂ a product name generator (inspired by one ofÂ <a href="https://oreil.ly/Fc8cq">OpenAIâ€™s examples</a>), and the resulting output from ChatGPT.<br>
ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„äº§å“åç§°ç”Ÿæˆå™¨æç¤ºè¾“å…¥ç¤ºä¾‹ï¼ˆå—åˆ° OpenAI ç¤ºä¾‹ä¹‹ä¸€çš„å¯å‘ï¼‰ï¼Œä»¥åŠ ChatGPT çš„ç»“æœè¾“å‡ºã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<pre tabindex="0"><code>Can I have a list of product names for a pair of shoes that
can fit any foot size?
</code></pre><p>Output:Â è¾“å‡ºï¼š</p>
<pre tabindex="0"><code>Certainly! Here&#39;s a list of product names for a pair of shoes
that can fit any foot size:
</code></pre><p>UniFit SoleStrides
FlexiSize All-Fit Shoes
OmniFit FootFlex
&hellip;
AnyStep AdaptiWalkers
FlexiRange UniversalSteps
PerfectGlide MultiFit Shoes</p>
<p>This is already a remarkable response for a naive prompt, which feels like magic because we got here with very little effort. As the state-of-the-art models improve, the likelihood you will getÂ <em>good enough</em>Â results on your first try goes up. For any throwaway interactions with an AI, where you donâ€™t plan to do the same task again, the naive approach is all you need.<br>
å¯¹äºä¸€ä¸ªå¤©çœŸçš„æç¤ºæ¥è¯´ï¼Œè¿™å·²ç»æ˜¯ä¸€ä¸ªäº†ä¸èµ·çš„ååº”ï¼Œè¿™æ„Ÿè§‰å°±åƒé­”æœ¯ä¸€æ ·ï¼Œå› ä¸ºæˆ‘ä»¬å‡ ä¹ä¸è´¹å¹ç°ä¹‹åŠ›å°±åˆ°è¾¾äº†è¿™é‡Œã€‚éšç€æœ€å…ˆè¿›æ¨¡å‹çš„æ”¹è¿›ï¼Œæ‚¨åœ¨ç¬¬ä¸€æ¬¡å°è¯•ä¸­è·å¾—è¶³å¤Ÿå¥½çš„ç»“æœçš„å¯èƒ½æ€§å°±ä¼šå¢åŠ ã€‚å¯¹äºä»»ä½•ä¸äººå·¥æ™ºèƒ½çš„ä¸€æ¬¡æ€§äº¤äº’ï¼Œä½ ä¸æ‰“ç®—å†æ¬¡æ‰§è¡Œç›¸åŒçš„ä»»åŠ¡ï¼Œç®€å•çš„æ–¹æ³•å°±æ˜¯ä½ æ‰€éœ€è¦çš„ã€‚</p>
<p>However, if you planned to put this prompt into production, youâ€™d benefit from investing more work into getting it right. Mistakes cost you money in terms of the fees OpenAI charges based on the length of the prompt and response, as well as the time spent fixing mistakes. If you were building a product name generator with thousands of users, there are some obvious issues youâ€™d want attempt to fix:<br>
ä½†æ˜¯ï¼Œå¦‚æœæ‚¨è®¡åˆ’å°†æ­¤æç¤ºæŠ•å…¥ç”Ÿäº§ï¼Œé‚£ä¹ˆæŠ•å…¥æ›´å¤šçš„å·¥ä½œæ¥ä½¿å…¶æ­£ç¡®ï¼Œæ‚¨å°†ä¼šå—ç›ŠåŒªæµ…ã€‚é”™è¯¯ä¼šå¯¼è‡´æ‚¨æŸå¤±é‡‘é’±ï¼ŒOpenAI æ ¹æ®æç¤ºå’Œå“åº”çš„é•¿åº¦ä»¥åŠä¿®å¤é”™è¯¯æ‰€èŠ±è´¹çš„æ—¶é—´æ”¶å–è´¹ç”¨ã€‚å¦‚æœæ‚¨æ­£åœ¨æ„å»ºä¸€ä¸ªæ‹¥æœ‰æ•°åƒåç”¨æˆ·çš„äº§å“åç§°ç”Ÿæˆå™¨ï¼Œé‚£ä¹ˆæ‚¨éœ€è¦å°è¯•ä¿®å¤ä¸€äº›æ˜æ˜¾çš„é—®é¢˜ï¼š</p>
<p>Vague directionÂ æ–¹å‘æ¨¡ç³Š</p>
<p>Youâ€™re not briefing the AI on what style ofÂ name you want, or what attributes it should have. Do you want a single word or a concatenation? Can the words be made up, or is it important that theyâ€™re in real English? Do you want the AI to emulate somebody you admire who is famous for great product names?<br>
ä½ ä¸ä¼šå‘äººå·¥æ™ºèƒ½ä»‹ç»ä½ æƒ³è¦ä»€ä¹ˆé£æ ¼çš„åå­—ï¼Œæˆ–è€…å®ƒåº”è¯¥å…·æœ‰ä»€ä¹ˆå±æ€§ã€‚æ‚¨æƒ³è¦å•ä¸ªå•è¯è¿˜æ˜¯ä¸²è”å•è¯ï¼Ÿè¿™äº›å•è¯å¯ä»¥æ˜¯è™šæ„çš„å—ï¼Ÿæˆ–è€…å®ƒä»¬æ˜¯çœŸæ­£çš„è‹±è¯­å¾ˆé‡è¦å—ï¼Ÿæ‚¨æ˜¯å¦å¸Œæœ›äººå·¥æ™ºèƒ½æ¨¡ä»¿æ‚¨æ‰€é’¦ä½©çš„ä»¥ä¼Ÿå¤§äº§å“åç§°è€Œé—»åçš„äººï¼Ÿ</p>
<p>Unformatted outputÂ æ— æ ¼å¼è¾“å‡º</p>
<p>Youâ€™re getting back a list of separated names line by line, of unspecified length. When you run this prompt multiple times, youâ€™ll see sometimes it comes back with a numbered list, and often it has text at the beginning, which makes it hard to parse programmatically.<br>
æ‚¨å°†é€è¡Œè¿”å›ä¸€ä¸ªæœªæŒ‡å®šé•¿åº¦çš„åˆ†éš”åç§°åˆ—è¡¨ã€‚å½“æ‚¨å¤šæ¬¡è¿è¡Œæ­¤æç¤ºæ—¶ï¼Œæ‚¨ä¼šçœ‹åˆ°æœ‰æ—¶å®ƒä¼šè¿”å›ä¸€ä¸ªç¼–å·åˆ—è¡¨ï¼Œå¹¶ä¸”é€šå¸¸åœ¨å¼€å¤´æœ‰æ–‡æœ¬ï¼Œè¿™ä½¿å¾—ä»¥ç¼–ç¨‹æ–¹å¼è§£æå˜å¾—å›°éš¾ã€‚</p>
<p>Missing examplesÂ ç¼ºå°‘ç¤ºä¾‹</p>
<p>You havenâ€™t given the AI any examples ofÂ whatÂ <em>good</em>Â names look like. Itâ€™s autocompleting using an average of its training data, i.e., the entire internet (with all its inherent bias), but is that what you want? Ideally youâ€™d feed it examples of successful names, common names in an industry, or even just other names you like.<br>
ä½ è¿˜æ²¡æœ‰ç»™äººå·¥æ™ºèƒ½ä»»ä½•å¥½åå­—çš„ä¾‹å­ã€‚å®ƒä½¿ç”¨å…¶è®­ç»ƒæ•°æ®çš„å¹³å‡å€¼ï¼ˆå³æ•´ä¸ªäº’è”ç½‘ï¼ˆåŠå…¶æ‰€æœ‰å›ºæœ‰çš„åè§ï¼‰ï¼‰è‡ªåŠ¨å®Œæˆï¼Œä½†è¿™å°±æ˜¯æ‚¨æƒ³è¦çš„å—ï¼Ÿç†æƒ³æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥å‘å…¶æä¾›æˆåŠŸåç§°ã€è¡Œä¸šä¸­å¸¸è§åç§°çš„ç¤ºä¾‹ï¼Œç”šè‡³åªæ˜¯æ‚¨å–œæ¬¢çš„å…¶ä»–åç§°ã€‚</p>
<p>Limited evaluationÂ æœ‰é™è¯„ä»·</p>
<p>You have no consistent or scalable way toÂ define which names are good or bad, so you have to manually review each response. If you can institute a rating system or other form of measurement, you can optimize the prompt to get better results and identify how many times it fails.<br>
æ‚¨æ²¡æœ‰ä¸€è‡´æˆ–å¯æ‰©å±•çš„æ–¹æ³•æ¥å®šä¹‰å“ªäº›åç§°å¥½æˆ–åï¼Œå› æ­¤æ‚¨å¿…é¡»æ‰‹åŠ¨æ£€æŸ¥æ¯ä¸ªå“åº”ã€‚å¦‚æœæ‚¨å¯ä»¥å»ºç«‹è¯„çº§ç³»ç»Ÿæˆ–å…¶ä»–å½¢å¼çš„æµ‹é‡ï¼Œæ‚¨å¯ä»¥ä¼˜åŒ–æç¤ºä»¥è·å¾—æ›´å¥½çš„ç»“æœå¹¶ç¡®å®šå¤±è´¥çš„æ¬¡æ•°ã€‚</p>
<p>No task divisionÂ æ²¡æœ‰ä»»åŠ¡åˆ’åˆ†</p>
<p>Youâ€™re asking a lot of aÂ single prompt here: there are lots of factors that go into product naming, and this important task is being naively outsourced to the AI all in one go, with no task specialization or visibility into how itâ€™s handling this task for you.<br>
ä½ åœ¨è¿™é‡Œé—®äº†å¾ˆå¤šå•ä¸€æç¤ºï¼šäº§å“å‘½åæ¶‰åŠå¾ˆå¤šå› ç´ ï¼Œè€Œè¿™é¡¹é‡è¦ä»»åŠ¡è¢«å¤©çœŸåœ°ä¸€æ¬¡æ€§å¤–åŒ…ç»™äººå·¥æ™ºèƒ½ï¼Œæ²¡æœ‰ä»»åŠ¡ä¸“é—¨åŒ–æˆ–äº†è§£å®ƒå¦‚ä½•å¤„ç†è¿™ä¸ªé—®é¢˜ç»™ä½ çš„ä»»åŠ¡ã€‚</p>
<p>Addressing these problems is the basis for the core principles we use throughout this book. There are manyÂ different ways to ask an AI model to do the same task, and even slight changes can make a big difference. LLMs work by continuously predicting the next token (approximately three-fourths of a word), starting from what was in your prompt. Each new token is selected based on its probability of appearing next, with anÂ element of randomness (controlled by theÂ <em>temperature</em>Â parameter). As demonstrated inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-1">FigureÂ 1-1</a>, the wordÂ <em>shoes</em>Â had a lower probability of coming after the start of the nameÂ <em>AnyFit</em>Â (0.88%), where a more predictable response would beÂ <em>Athletic</em>Â (72.35%).<br>
è§£å†³è¿™äº›é—®é¢˜æ˜¯æˆ‘ä»¬åœ¨æœ¬ä¹¦ä¸­ä½¿ç”¨çš„æ ¸å¿ƒåŸåˆ™çš„åŸºç¡€ã€‚è¦æ±‚äººå·¥æ™ºèƒ½æ¨¡å‹å®Œæˆç›¸åŒä»»åŠ¡çš„æ–¹æ³•æœ‰å¾ˆå¤šç§ï¼Œå³ä½¿æ˜¯å¾®å°çš„æ”¹å˜ä¹Ÿä¼šäº§ç”Ÿå¾ˆå¤§çš„å·®å¼‚ã€‚ LLMs ä»æç¤ºä¸­çš„å†…å®¹å¼€å§‹ï¼Œä¸æ–­é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ï¼ˆå¤§çº¦å››åˆ†ä¹‹ä¸‰çš„å•è¯ï¼‰ã€‚æ¯ä¸ªæ–°ä»¤ç‰Œéƒ½æ˜¯æ ¹æ®å…¶æ¥ä¸‹æ¥å‡ºç°çš„æ¦‚ç‡è¿›è¡Œé€‰æ‹©çš„ï¼Œå¹¶å…·æœ‰éšæœºæ€§ï¼ˆç”±æ¸©åº¦å‚æ•°æ§åˆ¶ï¼‰ã€‚å¦‚å›¾ 1-1 æ‰€ç¤ºï¼Œâ€œé‹â€ä¸€è¯å‡ºç°åœ¨ AnyFit åç§°å¼€å¤´ä¹‹åçš„æ¦‚ç‡è¾ƒä½ (0.88%)ï¼Œè€Œæ›´å¯é¢„æµ‹çš„å“åº”æ˜¯â€œè¿åŠ¨â€(72.35%)ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0101.png" alt="pega 0101"></p>
<h6 id="figure-1-1how-the-response-breaks-down-into-tokens">Figure 1-1.Â How the response breaks down into tokens</h6>
<p>å›¾ 1-1ã€‚å“åº”å¦‚ä½•åˆ†è§£ä¸ºä»¤ç‰Œ</p>
<p>LLMs are trained on essentially the entire text of the internet, and are then further fine-tuned to give helpful responses. Average prompts will return average responses, leading some to be underwhelmed when their results donâ€™t live up to the hype. What you put in your prompt changes the probability of every word generated, so it matters a great deal to the results youâ€™ll get. These models have seen the best and worst of what humans have produced and are capable of emulating almost anything if you know the right way to ask. OpenAI charges based on theÂ <a href="https://openai.com/pricing">number of tokens used</a>Â in the prompt and the response, so prompt engineers need to make these tokens count by optimizing prompts for cost, quality, and reliability.<br>
LLMs åŸºæœ¬ä¸Šæ¥å—äº†äº’è”ç½‘æ•´ä¸ªæ–‡æœ¬çš„è®­ç»ƒï¼Œç„¶åè¿›ä¸€æ­¥å¾®è°ƒä»¥æä¾›æœ‰ç”¨çš„å“åº”ã€‚ä¸€èˆ¬çš„æç¤ºå°†è¿”å›ä¸€èˆ¬çš„å“åº”ï¼Œå¯¼è‡´ä¸€äº›äººåœ¨ç»“æœä¸ç¬¦åˆå®£ä¼ æ—¶æ„Ÿåˆ°ä¸çŸ¥æ‰€æªã€‚ä½ åœ¨æç¤ºä¸­è¾“å…¥çš„å†…å®¹ä¼šæ”¹å˜ç”Ÿæˆæ¯ä¸ªå•è¯çš„æ¦‚ç‡ï¼Œå› æ­¤å®ƒå¯¹ä½ å¾—åˆ°çš„ç»“æœéå¸¸é‡è¦ã€‚è¿™äº›æ¨¡å‹å·²ç»çœ‹åˆ°äº†äººç±»åˆ›é€ çš„æœ€å¥½å’Œæœ€å·®çš„ä¸œè¥¿ï¼Œå¹¶ä¸”å¦‚æœä½ çŸ¥é“æ­£ç¡®çš„æé—®æ–¹å¼ï¼Œå®ƒä»¬å‡ ä¹èƒ½å¤Ÿæ¨¡æ‹Ÿä»»ä½•ä¸œè¥¿ã€‚ OpenAI æ ¹æ®æç¤ºå’Œå“åº”ä¸­ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡è¿›è¡Œæ”¶è´¹ï¼Œå› æ­¤æç¤ºå·¥ç¨‹å¸ˆéœ€è¦é€šè¿‡ä¼˜åŒ–æç¤ºçš„æˆæœ¬ã€è´¨é‡å’Œå¯é æ€§æ¥ä½¿è¿™äº›ä»¤ç‰Œè®¡æ•°ã€‚</p>
<p>Hereâ€™s the same example with the application of several prompt engineering techniques. We ask for names in the style of Steve Jobs, state that we want a comma-separated list, and supply examples of the task done well.<br>
è¿™æ˜¯åº”ç”¨äº†å‡ ç§å¿«é€Ÿå·¥ç¨‹æŠ€æœ¯çš„åŒä¸€ç¤ºä¾‹ã€‚æˆ‘ä»¬ä»¥å²è’‚å¤«Â·ä¹”å¸ƒæ–¯çš„é£æ ¼è¯¢é—®å§“åï¼Œå£°æ˜æˆ‘ä»¬æƒ³è¦ä¸€ä¸ªä»¥é€—å·åˆ†éš”çš„åˆ—è¡¨ï¼Œå¹¶æä¾›å‡ºè‰²å®Œæˆä»»åŠ¡çš„ç¤ºä¾‹ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Brainstorm a list of product names for a shoe that fits any
foot size, in the style of Steve Jobs.</p>
<p>Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of 3 product names]</p>
<h2 id="examples">Examples</h2>
<p>Product description: A refrigerator that dispenses beer
Product names: iBarFridge, iFridgeBeer, iDrinkBeerFridge</p>
<p>Product description: A watch that can tell accurate time in
space
Product names: iNaut, iSpace, iTime</p>
<p>Product description: A home milkshake maker
Product names: iShake, iSmoothie, iShake Mini</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Product description: A shoe that fits any foot size
Product names: iFitFoot, iPerfectFit, iShoeSize</p>
<p>While no prompt is ever perfect, this prompt is optimized to reliably deliver solid product names in the right format. The user of your product name generator can choose somebody other than Steve Jobs to get the types of names they like, they can change the response format if needed, and the output of this prompt can become the input of another. Finally, you could periodically update the examples you use in the prompt based on user feedback, making your system smarter over time.<br>
è™½ç„¶æ²¡æœ‰ä»»ä½•æç¤ºæ˜¯å®Œç¾çš„ï¼Œä½†æ­¤æç¤ºç»è¿‡ä¼˜åŒ–ï¼Œå¯ä»¥ä»¥æ­£ç¡®çš„æ ¼å¼å¯é åœ°æä¾›å¯é çš„äº§å“åç§°ã€‚äº§å“åç§°ç”Ÿæˆå™¨çš„ç”¨æˆ·å¯ä»¥é€‰æ‹©å²è’‚å¤«Â·ä¹”å¸ƒæ–¯ä»¥å¤–çš„å…¶ä»–äººæ¥è·å–ä»–ä»¬å–œæ¬¢çš„åç§°ç±»å‹ï¼Œå¦‚æœéœ€è¦ï¼Œä»–ä»¬å¯ä»¥æ›´æ”¹å“åº”æ ¼å¼ï¼Œå¹¶ä¸”æ­¤æç¤ºçš„è¾“å‡ºå¯ä»¥æˆä¸ºå¦ä¸€ä¸ªæç¤ºçš„è¾“å…¥ã€‚æœ€åï¼Œæ‚¨å¯ä»¥æ ¹æ®ç”¨æˆ·åé¦ˆå®šæœŸæ›´æ–°æç¤ºä¸­ä½¿ç”¨çš„ç¤ºä¾‹ï¼Œä»è€Œä½¿æ‚¨çš„ç³»ç»Ÿéšç€æ—¶é—´çš„æ¨ç§»å˜å¾—æ›´åŠ æ™ºèƒ½ã€‚</p>
<h1 id="overview-of-the-five-principles-of-prompting">Overview of the Five Principles of Prompting</h1>
<p>æç¤ºäº”é¡¹åŸåˆ™æ¦‚è¿°</p>
<p>The process for optimizing this prompt follows theÂ <em>Five Principles of Prompting</em>, which we will dissect using this example in the remainder of this chapter, and recall throughout the book. They map exactly to the five issues we raised when discussing the naive text prompt. Youâ€™ll find references back to these principles throughout the rest of the book to help you connect the dots to how theyâ€™re used in practice. The FiveÂ Principles of Prompting are as follows:<br>
ä¼˜åŒ–è¿™ä¸ªæç¤ºçš„è¿‡ç¨‹éµå¾ªæç¤ºçš„äº”é¡¹åŸåˆ™ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬ç« çš„å…¶ä½™éƒ¨åˆ†ä½¿ç”¨è¿™ä¸ªä¾‹å­è¿›è¡Œå‰–æï¼Œå¹¶åœ¨æ•´æœ¬ä¹¦ä¸­å›é¡¾ã€‚å®ƒä»¬å‡†ç¡®åœ°åæ˜ äº†æˆ‘ä»¬åœ¨è®¨è®ºå¹¼ç¨šæ–‡æœ¬æç¤ºæ—¶æå‡ºçš„äº”ä¸ªé—®é¢˜ã€‚åœ¨æœ¬ä¹¦çš„å…¶ä½™éƒ¨åˆ†ä¸­ï¼Œæ‚¨å°†æ‰¾åˆ°å¯¹è¿™äº›åŸåˆ™çš„å¼•ç”¨ï¼Œä»¥å¸®åŠ©æ‚¨å°†è¿™äº›ç‚¹ä¸å®ƒä»¬åœ¨å®è·µä¸­çš„ä½¿ç”¨æ–¹å¼è”ç³»èµ·æ¥ã€‚æç¤ºçš„äº”é¡¹åŸåˆ™å¦‚ä¸‹ï¼š</p>
<p>Give DirectionÂ ç»™äºˆæŒ‡å¯¼</p>
<p>Describe the desiredÂ style in detail, or reference a relevant persona<br>
è¯¦ç»†æè¿°æ‰€éœ€çš„é£æ ¼ï¼Œæˆ–å‚è€ƒç›¸å…³äººç‰©</p>
<p>Specify FormatÂ æŒ‡å®šæ ¼å¼</p>
<p>Define what rules to follow,Â and the required structure of the response<br>
å®šä¹‰è¦éµå¾ªçš„è§„åˆ™ä»¥åŠæ‰€éœ€çš„å“åº”ç»“æ„</p>
<p>Provide ExamplesÂ æä¾›ä¾‹å­</p>
<p>Insert a diverse set ofÂ test cases where the task was done correctly<br>
æ’å…¥æ­£ç¡®å®Œæˆä»»åŠ¡çš„ä¸€ç»„ä¸åŒçš„æµ‹è¯•ç”¨ä¾‹</p>
<p>Evaluate QualityÂ è¯„ä¼°è´¨é‡</p>
<p>Identify errorsÂ and rate responses, testing what drives performance.<br>
è¯†åˆ«é”™è¯¯å¹¶è¯„ä¼°å“åº”é€Ÿåº¦ï¼Œæµ‹è¯•é©±åŠ¨æ€§èƒ½çš„å› ç´ ã€‚</p>
<p>Divide LaborÂ åˆ†å·¥</p>
<p>Split tasks intoÂ multiple steps, chained together for complex goals<br>
å°†ä»»åŠ¡åˆ†æˆå¤šä¸ªæ­¥éª¤ï¼Œé“¾æ¥åœ¨ä¸€èµ·ä»¥å®ç°å¤æ‚çš„ç›®æ ‡</p>
<p>These principles are not short-livedÂ <em>tips</em>Â orÂ <em>hacks</em>Â but are generally accepted conventions that are useful for working with any level of intelligence, biological or artificial. These principles are model-agnostic and should work to improve your prompt no matter which generative text or image model youâ€™re using. We first published these principles in July 2022 in the blog postÂ <a href="https://oreil.ly/RYYiV">â€œPrompt Engineering: From Words to Art and Copyâ€</a>, and they have stood the test of time, including mapping quite closely to OpenAIâ€™s ownÂ <a href="https://oreil.ly/dF8q-">Prompt Engineering Guide</a>, which came a year later. Anyone who works closely with generative AI models is likely to converge on a similar set of strategies for solving common issues, and throughout this book youâ€™ll see hundreds of demonstrative examples of how they can be useful for improving your prompts.<br>
è¿™äº›åŸåˆ™ä¸æ˜¯çŸ­æš‚çš„æŠ€å·§æˆ–çªé—¨ï¼Œè€Œæ˜¯æ™®éæ¥å—çš„çº¦å®šï¼Œå¯¹äºä»»ä½•çº§åˆ«çš„æ™ºèƒ½ï¼ˆæ— è®ºæ˜¯ç”Ÿç‰©æ™ºèƒ½è¿˜æ˜¯äººå·¥æ™ºèƒ½ï¼‰éƒ½éå¸¸æœ‰ç”¨ã€‚è¿™äº›åŸåˆ™ä¸æ¨¡å‹æ— å…³ï¼Œæ— è®ºæ‚¨ä½¿ç”¨å“ªç§ç”Ÿæˆæ–‡æœ¬æˆ–å›¾åƒæ¨¡å‹ï¼Œéƒ½åº”è¯¥èƒ½å¤Ÿæ”¹å–„æ‚¨çš„æç¤ºã€‚æˆ‘ä»¬äº 2022 å¹´ 7 æœˆåœ¨åšå®¢æ–‡ç« â€œå³æ—¶å·¥ç¨‹ï¼šä»æ–‡å­—åˆ°è‰ºæœ¯å’Œå¤åˆ¶â€ä¸­é¦–æ¬¡å‘å¸ƒäº†è¿™äº›åŸåˆ™ï¼Œå®ƒä»¬ç»å—ä½äº†æ—¶é—´çš„è€ƒéªŒï¼ŒåŒ…æ‹¬ä¸ä¸€å¹´åå‘å¸ƒçš„ OpenAI è‡ªå·±çš„å³æ—¶å·¥ç¨‹æŒ‡å—éå¸¸æ¥è¿‘ã€‚ä»»ä½•ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹å¯†åˆ‡åˆä½œçš„äººéƒ½å¯èƒ½ä¼šé‡‡ç”¨ä¸€å¥—ç±»ä¼¼çš„ç­–ç•¥æ¥è§£å†³å¸¸è§é—®é¢˜ï¼Œåœ¨æœ¬ä¹¦ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°æ•°ç™¾ä¸ªè¯´æ˜æ€§ç¤ºä¾‹ï¼Œè¯´æ˜å®ƒä»¬å¦‚ä½•æœ‰åŠ©äºæ”¹è¿›æ‚¨çš„æç¤ºã€‚</p>
<p>We have provided downloadable one-pagers forÂ text and image generation you can use as a checklist when applying these principles. These were created for our popular Udemy courseÂ <a href="https://oreil.ly/V40zg">The Complete Prompt Engineering for AI Bootcamp</a>Â (70,000+ students), which was based on the same principles but with differentÂ material to this book.<br>
æˆ‘ä»¬æä¾›äº†å¯ä¸‹è½½çš„ç”¨äºæ–‡æœ¬å’Œå›¾åƒç”Ÿæˆçš„å•é¡µç¨‹åºï¼Œæ‚¨å¯ä»¥åœ¨åº”ç”¨è¿™äº›åŸåˆ™æ—¶å°†å…¶ç”¨ä½œæ¸…å•ã€‚è¿™äº›æ˜¯ä¸ºæˆ‘ä»¬æµè¡Œçš„ Udemy è¯¾ç¨‹â€œAI è®­ç»ƒè¥çš„å®Œæ•´æç¤ºå·¥ç¨‹â€ï¼ˆè¶…è¿‡ 70,000 åå­¦ç”Ÿï¼‰åˆ›å»ºçš„ï¼Œè¯¥è¯¾ç¨‹åŸºäºç›¸åŒçš„åŸç†ï¼Œä½†ä¸æœ¬ä¹¦çš„ææ–™ä¸åŒã€‚</p>
<ul>
<li>
<p><a href="https://oreil.ly/VCcgy">Text Generation One-Pager<br>
æ–‡æœ¬ç”Ÿæˆå•é¡µæœº</a></p>
</li>
<li>
<p><a href="https://oreil.ly/q7wQF">Image Generation One-Pager<br>
å›¾åƒç”Ÿæˆå•é¡µæœº</a></p>
</li>
</ul>
<p>To show these principles apply equally well to prompting image models, letâ€™s use the following example, and explain how to apply each of the Five Principles of Prompting to this specific scenario. Copy and paste the entire input prompt into theÂ Midjourney Bot in Discord, including the link to the image at the beginning, after typingÂ <code>**/imagine**</code>Â to trigger the prompt box to appear (requires a freeÂ <a href="https://discord.com/">Discord</a>Â account, and a paidÂ <a href="https://www.midjourney.com/">Midjourney</a>Â account).<br>
ä¸ºäº†è¡¨æ˜è¿™äº›åŸåˆ™åŒæ ·é€‚ç”¨äºæç¤ºå›¾åƒæ¨¡å‹ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ç¤ºä¾‹ï¼Œå¹¶è§£é‡Šå¦‚ä½•å°†æç¤ºçš„äº”é¡¹åŸåˆ™åº”ç”¨äºæ­¤ç‰¹å®šåœºæ™¯ã€‚å°†æ•´ä¸ªè¾“å…¥æç¤ºå¤åˆ¶å¹¶ç²˜è´´åˆ° Discord ä¸­çš„ Midjourney Bot ä¸­ï¼ŒåŒ…æ‹¬å¼€å¤´çš„å›¾åƒé“¾æ¥ï¼Œè¾“å…¥Â <code>**/imagine**</code>Â åè§¦å‘æç¤ºæ¡†å‡ºç°ï¼ˆéœ€è¦å…è´¹çš„ Discord å¸æˆ·å’Œä»˜è´¹å¸æˆ·ï¼‰ä¸­é€”å¸æˆ·ï¼‰ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p><a href="https://s.mj.run/TKAsyhNiKmc">https://s.mj.run/TKAsyhNiKmc</a> stock photo of business meeting
of 4 people watching on white MacBook on top of glass-top
table, Panasonic, DC-GH5</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-2">FigureÂ 1-2</a>Â shows the output.<br>
å›¾ 1-2 æ˜¾ç¤ºäº†è¾“å‡ºã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0102.png" alt="pega 0102"></p>
<h6 id="figure-1-2stock-photo-of-business-meeting">Figure 1-2.Â Stock photo of business meeting</h6>
<p>å›¾ 1-2ã€‚å•†åŠ¡ä¼šè®®çš„è‚¡ç¥¨ç…§ç‰‡</p>
<p>This prompt takes advantage of Midjourneyâ€™s ability to takeÂ a base image as an example by uploading the image to Discord and then copy and pasting the URL into the prompt (<em><a href="https://s.mj.run/TKAsyhNiKmc"><em>https://s.mj.run/TKAsyhNiKmc</em></a></em>), for which the royalty-free image from Unsplash is used (<a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-3">FigureÂ 1-3</a>). If you run into an error with the prompt, try uploading the image yourself and reviewingÂ <a href="https://oreil.ly/UTxpX">Midjourneyâ€™s documentation</a>Â for any formatting changes.<br>
æ­¤æç¤ºåˆ©ç”¨ Midjourney çš„åŠŸèƒ½ï¼Œä»¥åŸºæœ¬å›¾åƒä¸ºä¾‹ï¼Œå°†å›¾åƒä¸Šä¼ åˆ° Discordï¼Œç„¶åå°† URL å¤åˆ¶å¹¶ç²˜è´´åˆ°æç¤ºä¸­ (<a href="https://s.mj.run/TKAsyhNiKmc">https://s.mj.run/TKAsyhNiKmc</a>)ï¼Œä¸ºæ­¤ï¼Œç‰ˆç¨ -ä½¿ç”¨ Unsplash çš„å…è´¹å›¾åƒï¼ˆå›¾ 1-3ï¼‰ã€‚å¦‚æœæ‚¨é‡åˆ°æç¤ºé”™è¯¯ï¼Œè¯·å°è¯•è‡ªè¡Œä¸Šä¼ å›¾åƒå¹¶æŸ¥çœ‹ Midjourney çš„æ–‡æ¡£ä»¥äº†è§£ä»»ä½•æ ¼å¼æ›´æ”¹ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0103.png" alt="pega 0103"></p>
<h6 id="figure-1-3photo-by-mimi-thian-onunsplash">Figure 1-3.Â Photo by Mimi Thian onÂ <a href="https://oreil.ly/J4Hkr">Unsplash</a></h6>
<p>å›¾ 1-3ã€‚ç…§ç‰‡ç”± Unsplash ä¸Šçš„ Mimi Thian æ‹æ‘„</p>
<p>Letâ€™s compare this well-engineered promptÂ to what you get back from Midjourney if you naively ask for a stock photo in the simplest way possible.Â <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-4">FigureÂ 1-4</a>Â shows an example of what you get without prompt engineering, an image with a darker, more stylistic take on a stock photo than youâ€™d typically expect.<br>
è®©æˆ‘ä»¬å°†è¿™ä¸ªç²¾å¿ƒè®¾è®¡çš„æç¤ºä¸æ‚¨ä»ä¸­é€”å¤©çœŸåœ°ä»¥æœ€ç®€å•çš„æ–¹å¼ç´¢è¦åº“å­˜ç…§ç‰‡æ—¶å¾—åˆ°çš„æç¤ºè¿›è¡Œæ¯”è¾ƒã€‚å›¾ 1-4 å±•ç¤ºäº†æ‚¨æ— éœ€ç«‹å³è¿›è¡Œå·¥ç¨‹å¤„ç†å³å¯è·å¾—çš„ç¤ºä¾‹ï¼Œå³ä¸åº“å­˜ç…§ç‰‡ç›¸æ¯”ï¼Œå›¾åƒçš„é¢œè‰²æ¯”æ‚¨é€šå¸¸é¢„æœŸçš„æ›´æš—ã€æ›´å…·é£æ ¼ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>people in a business meeting</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-4">FigureÂ 1-4</a>Â shows the output.<br>
å›¾ 1-4 æ˜¾ç¤ºäº†è¾“å‡ºã€‚</p>
<p>Although less prominent an issue in v5 of Midjourney onwards, community feedback mechanisms (when users select an image to resize to a higher resolution, that choice may be used to train the model) have reportedly biased the model toward aÂ <em>fantasy</em>Â aesthetic, which is less suitable for the stock photo use case. The early adopters of Midjourney came from the digital art world and naturally gravitated toward fantasy and sci-fi styles, which can be reflected in the results from the model even when this aesthetic is not suitable.<br>
å°½ç®¡åœ¨ Midjourney çš„ v5 ç‰ˆæœ¬ä¸­è¿™ä¸ªé—®é¢˜ä¸å¤ªçªå‡ºï¼Œä½†æ®æŠ¥é“ï¼Œç¤¾åŒºåé¦ˆæœºåˆ¶ï¼ˆå½“ç”¨æˆ·é€‰æ‹©å°†å›¾åƒå¤§å°è°ƒæ•´ä¸ºæ›´é«˜åˆ†è¾¨ç‡æ—¶ï¼Œè¯¥é€‰æ‹©å¯èƒ½ä¼šç”¨äºè®­ç»ƒæ¨¡å‹ï¼‰ä½¿æ¨¡å‹åå‘äºå¹»æƒ³ç¾å­¦ï¼Œè¿™æ˜¯è¾ƒå°‘çš„é€‚åˆåº“å­˜ç…§ç‰‡ç”¨ä¾‹ã€‚ Midjourney çš„æ—©æœŸé‡‡ç”¨è€…æ¥è‡ªæ•°å­—è‰ºæœ¯ä¸–ç•Œï¼Œè‡ªç„¶åå‘å¥‡å¹»å’Œç§‘å¹»é£æ ¼ï¼Œå³ä½¿è¿™ç§å®¡ç¾å¹¶ä¸é€‚åˆï¼Œè¿™ä¹Ÿå¯ä»¥åæ˜ åœ¨æ¨¡å‹çš„ç»“æœä¸­ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0104.png" alt="pega 0104"></p>
<h6 id="figure-1-4people-in-a-business-meeting">Figure 1-4.Â People in a business meeting</h6>
<p>å›¾ 1-4ã€‚å•†åŠ¡ä¼šè®®ä¸­çš„äººä»¬</p>
<p>Throughout this book the examples used willÂ be compatiable with ChatGPT Plus (GPT-4) as the text model and Midjourney v6 or Stable Diffusion XL as the image model, though we will specify if itâ€™s important. These foundational models are the current state of the art and are good at a diverse range of tasks. The principles are intended to be future-proof as much as is possible, so if youâ€™re reading this book when GPT-5, Midjourney v7, or Stable Diffusion XXL is out, or if youâ€™re using another vendor like Google, everything you learn here should still prove useful.<br>
æœ¬ä¹¦ä¸­ä½¿ç”¨çš„ç¤ºä¾‹å°†ä¸ä½œä¸ºæ–‡æœ¬æ¨¡å‹çš„ ChatGPT Plus (GPT-4) å’Œä½œä¸ºå›¾åƒæ¨¡å‹çš„ Midjourney v6 æˆ– Stable Diffusion XL å…¼å®¹ï¼Œå°½ç®¡æˆ‘ä»¬å°†æŒ‡å®šå®ƒæ˜¯å¦é‡è¦ã€‚è¿™äº›åŸºç¡€æ¨¡å‹æ˜¯å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œæ“…é•¿æ‰§è¡Œå„ç§ä»»åŠ¡ã€‚è¿™äº›åŸåˆ™æ—¨åœ¨å°½å¯èƒ½é¢å‘æœªæ¥ï¼Œå› æ­¤ï¼Œå¦‚æœæ‚¨åœ¨ GPT-5ã€Midjourney v7 æˆ– Stable Diffusion XXL å‘å¸ƒæ—¶é˜…è¯»æœ¬ä¹¦ï¼Œæˆ–è€…å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨ Google ç­‰å…¶ä»–ä¾›åº”å•†ï¼Œé‚£ä¹ˆä¸€åˆ‡ä½ åœ¨è¿™é‡Œå­¦åˆ°çš„åº”è¯¥è¿˜æ˜¯æœ‰ç”¨çš„ã€‚</p>
<h1 id="1-give-direction1-ç»™äºˆæŒ‡å¯¼">1. Give DirectionÂ 1. ç»™äºˆæŒ‡å¯¼</h1>
<p>One of the issues with the naive textÂ prompt discussed earlier was that it wasnâ€™t briefing the AI on whatÂ <em>types</em>Â of product names you wanted. To some extent, naming a product is a subjective endeavor, and without giving the AI an idea of what names you like, it has a low probability of guessing right.<br>
å‰é¢è®¨è®ºçš„å¤©çœŸçš„æ–‡æœ¬æç¤ºçš„é—®é¢˜ä¹‹ä¸€æ˜¯å®ƒæ²¡æœ‰å‘äººå·¥æ™ºèƒ½ç®€è¦ä»‹ç»æ‚¨æƒ³è¦ä»€ä¹ˆç±»å‹çš„äº§å“åç§°ã€‚åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œä¸ºäº§å“å‘½åæ˜¯ä¸€ç§ä¸»è§‚åŠªåŠ›ï¼Œå¦‚æœä¸è®©äººå·¥æ™ºèƒ½çŸ¥é“ä½ å–œæ¬¢ä»€ä¹ˆåå­—ï¼Œå®ƒçŒœå¯¹çš„å¯èƒ½æ€§å¾ˆä½ã€‚</p>
<p>By the way, a human would also struggle to complete this task without a goodÂ <em>brief</em>, which is why creative and branding agencies require a detailed briefing on any task from their clients.<br>
é¡ºä¾¿è¯´ä¸€å¥ï¼Œå¦‚æœæ²¡æœ‰è‰¯å¥½çš„ç®€æŠ¥ï¼Œäººç±»ä¹Ÿå¾ˆéš¾å®Œæˆè¿™é¡¹ä»»åŠ¡ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆåˆ›æ„å’Œå“ç‰Œæœºæ„éœ€è¦å®¢æˆ·æä¾›æœ‰å…³ä»»ä½•ä»»åŠ¡çš„è¯¦ç»†ç®€æŠ¥çš„åŸå› ã€‚</p>
<h6 id="tip-1">TIP</h6>
<p>Although itâ€™s not a perfect mapping, it can be helpful to imagine what context a human might need for this task and try including it in the prompt.<br>
å°½ç®¡è¿™ä¸æ˜¯ä¸€ä¸ªå®Œç¾çš„æ˜ å°„ï¼Œä½†æƒ³è±¡ä¸€ä¸‹äººç±»å¯èƒ½éœ€è¦ä»€ä¹ˆä¸Šä¸‹æ–‡æ¥å®Œæˆæ­¤ä»»åŠ¡å¹¶å°è¯•å°†å…¶åŒ…å«åœ¨æç¤ºä¸­å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚</p>
<p>In the example prompt we gave directionÂ through the use ofÂ <em>role-playing</em>, in that case emulating the style of Steve Jobs, who was famous for iconically naming products. If you change this aspect of the prompt to someone else who is famous in the training data (as well as matching the examples to the right style), youâ€™ll get dramatically different results.<br>
åœ¨ç¤ºä¾‹æç¤ºä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨è§’è‰²æ‰®æ¼”æ¥ç»™å‡ºæŒ‡å¯¼ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ¨¡ä»¿å²è’‚å¤«Â·ä¹”å¸ƒæ–¯çš„é£æ ¼ï¼Œä»–ä»¥æ ‡å¿—æ€§çš„äº§å“å‘½åè€Œé—»åã€‚å¦‚æœæ‚¨å°†æç¤ºçš„è¿™æ–¹é¢æ›´æ”¹ä¸ºè®­ç»ƒæ•°æ®ä¸­è‘—åçš„å…¶ä»–äººï¼ˆä»¥åŠå°†ç¤ºä¾‹ä¸æ­£ç¡®çš„é£æ ¼ç›¸åŒ¹é…ï¼‰ï¼Œæ‚¨å°†å¾—åˆ°æˆªç„¶ä¸åŒçš„ç»“æœã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Brainstorm a list of product names for a shoe that fits any
foot size, in the style of Elon Musk.</p>
<p>Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of 3 product names]</p>
<h2 id="examples-1">Examples</h2>
<p>Product description: A refrigerator that dispenses beer
Product names: Teslacool, FridgeX, Neuradrink</p>
<p>Product description: A watch that can tell accurate time in
space
Product names: WatchX, iSpace, The Watch Company</p>
<p>Product description: A home milkshake maker
Product names: ShakeX, The Milkshake Company, HyperShake</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>ShoeX, MarsFit, The Shoe Company</p>
<p>There are also some rules or best practices youÂ would do well to follow, which could be included in the prompt as context to guide the AI toward a name that works. This technique is sometimes referred to asÂ <em>prewarming</em>Â orÂ <em>internal retrieval</em>, and it is simple but effective (<a href="https://oreil.ly/1lqzK">Liu et al., 2021</a>). Starting the conversation asking for best practice advice, then asking it to follow its own advice, can help a lot. In effect, you are using it to generate its own direction.<br>
è¿˜æœ‰ä¸€äº›æ‚¨æœ€å¥½éµå¾ªçš„è§„åˆ™æˆ–æœ€ä½³å®è·µï¼Œå®ƒä»¬å¯ä»¥ä½œä¸ºä¸Šä¸‹æ–‡åŒ…å«åœ¨æç¤ºä¸­ï¼Œä»¥æŒ‡å¯¼äººå·¥æ™ºèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„åç§°ã€‚è¿™ç§æŠ€æœ¯æœ‰æ—¶è¢«ç§°ä¸ºé¢„çƒ­æˆ–å†…éƒ¨æ£€ç´¢ï¼Œå®ƒç®€å•ä½†æœ‰æ•ˆï¼ˆLiu et al., 2021ï¼‰ã€‚å¼€å§‹å¯¹è¯æ—¶å¯»æ±‚æœ€ä½³å®è·µå»ºè®®ï¼Œç„¶åè¦æ±‚å…¶éµå¾ªè‡ªå·±çš„å»ºè®®ï¼Œè¿™ä¼šå¾ˆæœ‰å¸®åŠ©ã€‚å®é™…ä¸Šï¼Œæ‚¨æ­£åœ¨ä½¿ç”¨å®ƒæ¥ç”Ÿæˆè‡ªå·±çš„æ–¹å‘ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Please give me 5 tips for naming products based on expert
industry advice.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Certainly! Naming a product is crucial as it often gives the
first impression to potential customers. Here are five expert
tips to guide you in naming products:</p>
<ol>
<li><strong>Simplicity &amp; Memorability</strong>: Opt for names that are
short, easy to spell, and memorable. Long, complicated names
can be difficult for customers to recall. Remember, the
easier the name is to remember, the more likely it is that
consumers will think of your product when making a purchase
decision.</li>
</ol>
<p>&hellip;</p>
<ol start="5">
<li><strong>Positive Connotations</strong>: Names that evoke positive
emotions or imagery can create a favorable perception of
your product. Avoid names that have negative or
controversial associations in any of the primary languages
or cultures where you intend to do business. It&rsquo;s often
worthwhile to conduct linguistic checks and market testing
to ensure there aren&rsquo;t unintended meanings.</li>
</ol>
<p>Lastly, once you&rsquo;ve shortlisted a few names, it&rsquo;s a good
idea to run them by diverse focus groups or conduct surveys
to gauge reactions. Also, ensure that the name you choose is
available for trademarking and doesn&rsquo;t infringe on existing
trademarks.</p>
<p>Then within the same chat window, where the model has the context of the past advice it gave, you ask your initial prompt for the task you wanted to complete.<br>
ç„¶ååœ¨åŒä¸€ä¸ªèŠå¤©çª—å£ä¸­ï¼Œæ¨¡å‹å…·æœ‰å®ƒæ‰€æä¾›çš„è¿‡å»å»ºè®®çš„ä¸Šä¸‹æ–‡ï¼Œæ‚¨å¯ä»¥è¯¢é—®æ‚¨æƒ³è¦å®Œæˆçš„ä»»åŠ¡çš„åˆå§‹æç¤ºã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Using this advice, can I have a list of product names for a
pair of shoes that can fit any foot size?</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Certainly! Based on the criteria given:</p>
<ol>
<li><strong>Simplicity &amp; Memorability</strong></li>
<li><strong>Relevance &amp; Descriptiveness</strong></li>
<li><strong>Differentiation &amp; Uniqueness</strong></li>
<li><strong>Adaptable &amp; Scalable</strong></li>
<li><strong>Positive Connotations</strong></li>
</ol>
<p>Here&rsquo;s a list of potential product names for shoes that can
fit any foot size:</p>
<ol>
<li><strong>FitAll Walkers</strong>: A simple and memorable name that
instantly suggests a universal fit.</li>
</ol>
<p>&hellip;</p>
<ol start="10">
<li><strong>TrueFit Universals</strong>: Emphasizing genuine comfort and
fit for all.</li>
</ol>
<p>Before finalizing any name, remember to check for trademarks
to ensure there&rsquo;s no conflict with existing products or
brands.</p>
<p>Another fruitful strategy in our experience is to take the best advice out there for the task you want to accomplish and insert that context into the prompt. For example, you could takeÂ <a href="https://oreil.ly/3bWjz">Brandwatchâ€™s 5 Golden Rules for naming a product</a>Â or another trusted external resource you find, and insert that as context into the prompt. This will increase the length of the prompt significantly, which costs more money (when using the API as a developer), but may be worth the trade-off if the quality of the response improves.<br>
æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼Œå¦ä¸€ä¸ªå¯Œæœ‰æˆæ•ˆçš„ç­–ç•¥æ˜¯é’ˆå¯¹æ‚¨æƒ³è¦å®Œæˆçš„ä»»åŠ¡è·å–æœ€ä½³å»ºè®®ï¼Œå¹¶å°†è¯¥ä¸Šä¸‹æ–‡æ’å…¥æç¤ºä¸­ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥é‡‡ç”¨ Brandwatch çš„ 5 æ¡é»„é‡‘æ³•åˆ™æ¥å‘½åæ‚¨æ‰¾åˆ°çš„äº§å“æˆ–å…¶ä»–å—ä¿¡ä»»çš„å¤–éƒ¨èµ„æºï¼Œå¹¶å°†å…¶ä½œä¸ºä¸Šä¸‹æ–‡æ’å…¥åˆ°æç¤ºä¸­ã€‚è¿™å°†æ˜¾ç€å¢åŠ æç¤ºçš„é•¿åº¦ï¼Œè¿™ä¼šèŠ±è´¹æ›´å¤šçš„é’±ï¼ˆå½“ä½œä¸ºå¼€å‘äººå‘˜ä½¿ç”¨ API æ—¶ï¼‰ï¼Œä½†å¦‚æœå“åº”çš„è´¨é‡æé«˜ï¼Œå¯èƒ½å€¼å¾—æƒè¡¡ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Can I have a list of product names for a pair of shoes that
can fit any foot size? Please follow the five rules below:</p>
<ol>
<li>It should be readable and writable.
If your product name is hard to pronounce, people wonâ€™t talk about it,
and if they canâ€™t write it down (and spell it correctly!)
when they hear it, how do you expect them to Google it?</li>
</ol>
<p>Keep it simple and donâ€™t go with any wacky spellings just for the sake of it.</p>
<p>&hellip;</p>
<ol start="5">
<li>It should evoke an emotion, feeling, or idea.
Your product name should tie back into what your product is, what the
feeling you want people to have when experiencing your product is, and/or
what idea are you trying to get across.</li>
</ol>
<p>It should be emotive and inspiring.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<ol>
<li>FlexFit Footwear</li>
<li>OmniSize Sneakers</li>
<li>AdaptStride Shoes
&hellip;</li>
<li>OmniComfort Kicks</li>
<li>FlexSize Footwear</li>
<li>Boundless Soles</li>
</ol>
<p>There are other myriad ways ofÂ providing direction. In the image generation example, direction was given by specifying that the business meeting is taking place around a glass-top table. If you change only that detail, you can get a completely different image, as detailed inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-5">FigureÂ 1-5</a>.<br>
è¿˜æœ‰å…¶ä»–æ— æ•°ç§æä¾›æŒ‡å¯¼çš„æ–¹æ³•ã€‚åœ¨å›¾åƒç”Ÿæˆç¤ºä¾‹ä¸­ï¼Œé€šè¿‡æŒ‡å®šå•†åŠ¡ä¼šè®®åœ¨ç»ç’ƒé¡¶æ¡Œå­å‘¨å›´ä¸¾è¡Œæ¥ç»™å‡ºæ–¹å‘ã€‚å¦‚æœä»…æ›´æ”¹è¯¥ç»†èŠ‚ï¼Œæ‚¨å¯ä»¥è·å¾—å®Œå…¨ä¸åŒçš„å›¾åƒï¼Œå¦‚å›¾ 1-5 æ‰€ç¤ºã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p><a href="https://s.mj.run/TKAsyhNiKmc">https://s.mj.run/TKAsyhNiKmc</a> stock photo of business meeting
of four people gathered around a campfire outdoors in the
woods, Panasonic, DC-GH5</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-5">FigureÂ 1-5</a>Â shows the output.<br>
å›¾ 1-5 æ˜¾ç¤ºäº†è¾“å‡ºã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0105.png" alt="pega 0105"></p>
<h6 id="figure-1-5stock-photo-of-business-meeting-in-the-woods">Figure 1-5.Â Stock photo of business meeting in the woods</h6>
<p>å›¾ 1-5ã€‚åœ¨æ ‘æ—é‡Œä¸¾è¡Œå•†åŠ¡ä¼šè®®çš„è‚¡ç¥¨ç…§ç‰‡</p>
<p>Role-playing is also important forÂ image generation, and one of the quite powerful ways you can give Midjourney direction is to supply the name of an artist or art style to emulate. One artist that features heavily in the AI art world is Van Gogh, known for his bold, dramatic brush strokes and vivid use of colors. Watch what happens when you include his name in the prompt, as shown inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-6">FigureÂ 1-6</a>.<br>
è§’è‰²æ‰®æ¼”å¯¹äºå›¾åƒç”Ÿæˆä¹Ÿå¾ˆé‡è¦ï¼Œä¸ºä¸­é€”æä¾›æŒ‡å¯¼çš„ä¸€ç§éå¸¸æœ‰æ•ˆçš„æ–¹æ³•æ˜¯æä¾›è¦æ¨¡ä»¿çš„è‰ºæœ¯å®¶æˆ–è‰ºæœ¯é£æ ¼çš„åå­—ã€‚æ¢µé«˜æ˜¯äººå·¥æ™ºèƒ½è‰ºæœ¯ç•Œä¸­ä¸€ä½ä¸¾è¶³è½»é‡çš„è‰ºæœ¯å®¶ï¼Œä»–ä»¥å…¶å¤§èƒ†ã€æˆå‰§æ€§çš„ç¬”è§¦å’Œç”ŸåŠ¨çš„è‰²å½©è¿ç”¨è€Œé—»åã€‚è§‚å¯Ÿå½“æ‚¨åœ¨æç¤ºä¸­åŒ…å«ä»–çš„åå­—æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œå¦‚å›¾ 1-6 æ‰€ç¤ºã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>people in a business meeting, by Van Gogh</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-6">FigureÂ 1-6</a>Â shows the output.<br>
å›¾ 1-6 æ˜¾ç¤ºäº†è¾“å‡ºã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0106.png" alt="pega 0106"></p>
<h6 id="figure-1-6people-in-a-business-meeting-by-van-gogh">Figure 1-6.Â People in a business meeting, by Van Gogh</h6>
<p>å›¾ 1-6ã€‚å‚åŠ å•†åŠ¡ä¼šè®®çš„äººä»¬ï¼Œæ¢µé«˜</p>
<p>To get that last prompt to work, you need toÂ strip back a lot of the other direction. For example, losing the base image and the wordsÂ <em>stock photo</em>Â as well as the cameraÂ <em>Panasonic, DC-GH5</em>Â helps bring in Van Goghâ€™s style. The problem you may run into is that often with too much direction, the model can quickly get to a conflicting combination that it canâ€™t resolve. If your prompt is overly specific, there might not be enough samples in the training data to generate an image thatâ€™s consistent with all of your criteria. In cases like these, you should choose which element is more important (in this case, Van Gogh) and defer to that.<br>
ä¸ºäº†è®©æœ€åä¸€ä¸ªæç¤ºèµ·ä½œç”¨ï¼Œä½ éœ€è¦å»æ‰å¾ˆå¤šå…¶ä»–æ–¹å‘çš„å†…å®¹ã€‚ä¾‹å¦‚ï¼Œå»æ‰åº•å›¾å’Œstock photoå­—æ ·ä»¥åŠæ¾ä¸‹ç›¸æœºï¼ŒDC-GH5æœ‰åŠ©äºå¼•å…¥æ¢µé«˜çš„é£æ ¼ã€‚æ‚¨å¯èƒ½é‡åˆ°çš„é—®é¢˜æ˜¯ï¼Œé€šå¸¸æ–¹å‘å¤ªå¤šï¼Œæ¨¡å‹å¾ˆå¿«å°±ä¼šå‡ºç°æ— æ³•è§£å†³çš„å†²çªç»„åˆã€‚å¦‚æœæ‚¨çš„æç¤ºè¿‡äºå…·ä½“ï¼Œè®­ç»ƒæ•°æ®ä¸­å¯èƒ½æ²¡æœ‰è¶³å¤Ÿçš„æ ·æœ¬æ¥ç”Ÿæˆç¬¦åˆæ‚¨æ‰€æœ‰æ ‡å‡†çš„å›¾åƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨åº”è¯¥é€‰æ‹©å“ªä¸ªå…ƒç´ æ›´é‡è¦ï¼ˆåœ¨æœ¬ä¾‹ä¸­æ˜¯æ¢µé«˜ï¼‰å¹¶éµå¾ªå®ƒã€‚</p>
<p>Direction is one of the most commonly used and broadest principles. It can take the form of simply using the right descriptive words to clarify your intent, or channeling the personas of relevant business celebrities. While too much direction can narrow the creativity of theÂ model, too little direction is the more common problem.<br>
æ–¹å‘æ˜¯æœ€å¸¸ç”¨å’Œæœ€å¹¿æ³›çš„åŸåˆ™ä¹‹ä¸€ã€‚å®ƒå¯ä»¥é‡‡å–ç®€å•åœ°ä½¿ç”¨æ­£ç¡®çš„æè¿°æ€§è¯è¯­æ¥é˜æ˜æ‚¨çš„æ„å›¾çš„å½¢å¼ï¼Œæˆ–è€…å¼•å¯¼ç›¸å…³å•†ä¸šåäººçš„è§’è‰²ã€‚è™½ç„¶å¤ªå¤šçš„æ–¹å‘ä¼šç¼©å°æ¨¡å‹çš„åˆ›é€ åŠ›ï¼Œä½†æ–¹å‘å¤ªå°‘æ˜¯æ›´å¸¸è§çš„é—®é¢˜ã€‚</p>
<h1 id="2-specify-format2-æŒ‡å®šæ ¼å¼">2. Specify FormatÂ 2. æŒ‡å®šæ ¼å¼</h1>
<p>AI models are universal translators. Not onlyÂ does that mean translating from French to English, or Urdu to Klingon, but also between data structures like JSON to YAML, or naturalÂ language to Python code. These models are capable of returning a response in almost any format, so an important part of prompt engineering is finding ways to specify what format you want the response to be in.<br>
äººå·¥æ™ºèƒ½æ¨¡å‹æ˜¯é€šç”¨ç¿»è¯‘å™¨ã€‚è¿™ä¸ä»…æ„å‘³ç€ä»æ³•è¯­åˆ°è‹±è¯­ã€æˆ–ä»ä¹Œå°”éƒ½è¯­åˆ°å…‹æ—è´¡è¯­çš„ç¿»è¯‘ï¼Œè¿˜æ„å‘³ç€åœ¨ JSON åˆ° YAML ç­‰æ•°æ®ç»“æ„ä¹‹é—´çš„ç¿»è¯‘ï¼Œæˆ–è€…ä»è‡ªç„¶è¯­è¨€åˆ° Python ä»£ç çš„ç¿»è¯‘ã€‚è¿™äº›æ¨¡å‹èƒ½å¤Ÿä»¥å‡ ä¹ä»»ä½•æ ¼å¼è¿”å›å“åº”ï¼Œå› æ­¤æç¤ºå·¥ç¨‹çš„ä¸€ä¸ªé‡è¦éƒ¨åˆ†æ˜¯æ‰¾åˆ°æ–¹æ³•æ¥æŒ‡å®šæ‚¨å¸Œæœ›å“åº”é‡‡ç”¨çš„æ ¼å¼ã€‚</p>
<p>Every now and again youâ€™ll find that the same prompt will return a different format, for example, a numbered list instead of comma separated. This isnâ€™t a big deal most of the time, because most prompts are one-offs and typed into ChatGPT or Midjourney. However, when youâ€™re incorporating AI tools into production software, occasional flips in format can cause all kinds of errors.<br>
æ‚¨æ—¶ä¸æ—¶ä¼šå‘ç°ç›¸åŒçš„æç¤ºä¼šè¿”å›ä¸åŒçš„æ ¼å¼ï¼Œä¾‹å¦‚ï¼Œç¼–å·åˆ—è¡¨è€Œä¸æ˜¯é€—å·åˆ†éš”ã€‚å¤§å¤šæ•°æ—¶å€™è¿™å¹¶ä¸æ˜¯ä»€ä¹ˆå¤§é—®é¢˜ï¼Œå› ä¸ºå¤§å¤šæ•°æç¤ºéƒ½æ˜¯ä¸€æ¬¡æ€§çš„ï¼Œå¹¶è¾“å…¥ ChatGPT æˆ– Midjourney ä¸­ã€‚ç„¶è€Œï¼Œå½“æ‚¨å°†äººå·¥æ™ºèƒ½å·¥å…·æ•´åˆåˆ°ç”Ÿäº§è½¯ä»¶ä¸­æ—¶ï¼Œå¶å°”çš„æ ¼å¼ç¿»è½¬å¯èƒ½ä¼šå¯¼è‡´å„ç§é”™è¯¯ã€‚</p>
<p>Just like when working with a human, you can avoid wasted effort by specifying up front the format you expect the response to be in. For text generation models, it can often be helpful toÂ output JSON instead of a simple ordered list because thatâ€™s the universal format for API responses, which can make it simpler to parse and spot errors, as well as to use to render the front-end HTML of an application. YAML is also another popular choice because it enforces a parseable structure while still being simple and human-readable.<br>
å°±åƒä¸äººåˆä½œæ—¶ä¸€æ ·ï¼Œæ‚¨å¯ä»¥é€šè¿‡é¢„å…ˆæŒ‡å®šæ‚¨æœŸæœ›å“åº”çš„æ ¼å¼æ¥é¿å…æµªè´¹ç²¾åŠ›ã€‚å¯¹äºæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œè¾“å‡º JSON è€Œä¸æ˜¯ç®€å•çš„æœ‰åºåˆ—è¡¨é€šå¸¸ä¼šå¾ˆæœ‰å¸®åŠ©ï¼Œå› ä¸ºè¿™æ˜¯é€šç”¨çš„API å“åº”çš„æ ¼å¼ï¼Œå¯ä»¥æ›´è½»æ¾åœ°è§£æå’Œå‘ç°é”™è¯¯ï¼Œä»¥åŠç”¨äºå‘ˆç°åº”ç”¨ç¨‹åºçš„å‰ç«¯ HTMLã€‚ YAML ä¹Ÿæ˜¯å¦ä¸€ä¸ªæµè¡Œçš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒå¼ºåˆ¶æ‰§è¡Œå¯è§£æçš„ç»“æ„ï¼ŒåŒæ—¶ä»ç„¶ç®€å•ä¸”æ˜“äºé˜…è¯»ã€‚</p>
<p>In the original prompt you gave direction through both the examples provided, and the colon at the end of the prompt indicated it should complete the list inline. To swap the format to JSON, you need to update both and leave the JSON uncompleted, so GPT-4 knows to complete it.<br>
åœ¨åŸå§‹æç¤ºä¸­ï¼Œæ‚¨é€šè¿‡æä¾›çš„ä¸¤ä¸ªç¤ºä¾‹ç»™å‡ºäº†æŒ‡ç¤ºï¼Œæç¤ºæœ«å°¾çš„å†’å·è¡¨ç¤ºå®ƒåº”è¯¥å†…è”å®Œæˆåˆ—è¡¨ã€‚è¦å°†æ ¼å¼äº¤æ¢ä¸º JSONï¼Œæ‚¨éœ€è¦æ›´æ–°ä¸¤è€…å¹¶ä¿ç•™ JSON ä¸å®Œæ•´ï¼Œä»¥ä¾¿ GPT-4 çŸ¥é“è¦å®Œæˆå®ƒã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Return a comma-separated list of product names in JSON for
&ldquo;A pair of shoes that can fit any foot size.&rdquo;.
Return only JSON.</p>
<p>Examples:
[{
&ldquo;Product description&rdquo;: &ldquo;A home milkshake maker.&rdquo;,
&ldquo;Product names&rdquo;: [&ldquo;HomeShaker&rdquo;, &ldquo;Fit Shaker&rdquo;,
&ldquo;QuickShake&rdquo;, &ldquo;Shake Maker&rdquo;]
},
{
&ldquo;Product description&rdquo;: &ldquo;A watch that can tell
accurate time in space.&rdquo;,
&ldquo;Product names&rdquo;: [&ldquo;AstroTime&rdquo;, &ldquo;SpaceGuard&rdquo;,
&ldquo;Orbit-Accurate&rdquo;, &ldquo;EliptoTime&rdquo;]}
]</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>[
{
&ldquo;Product description&rdquo;: &ldquo;A pair of shoes that can <br>
fit any foot size.&rdquo;,
&ldquo;Product names&rdquo;: [&ldquo;FlexFit Footwear&rdquo;, &ldquo;OneSize Step&rdquo;,
&ldquo;Adapt-a-Shoe&rdquo;, &ldquo;Universal Walker&rdquo;]
}
]</p>
<p>The output we get back is the completed JSON containing the product names. This can then be parsed and used programmatically, in an application or local script. Itâ€™s also easy from this point to check if thereâ€™s an error in the formatting using a JSON parser like Pythonâ€™sÂ standardÂ <em>json</em>Â library, because broken JSON will result in a parsing error, which can act as a trigger to retry the prompt or investigate before continuing. If youâ€™re still not getting the right format back, it can help to specify at the beginning or end of the prompt, or in the system message if using a chat model:Â <code>You are a helpful assistant that only responds in JSON</code>, or specifyÂ <a href="https://oreil.ly/E7wua">JSON output</a>Â in the model parameters where available (this is calledÂ <em>grammars</em>Â withÂ <a href="https://oreil.ly/yU27T">Llama models</a>.<br>
æˆ‘ä»¬å¾—åˆ°çš„è¾“å‡ºæ˜¯åŒ…å«äº§å“åç§°çš„å®Œæ•´ JSONã€‚ç„¶åå¯ä»¥åœ¨åº”ç”¨ç¨‹åºæˆ–æœ¬åœ°è„šæœ¬ä¸­ä»¥ç¼–ç¨‹æ–¹å¼è§£æå’Œä½¿ç”¨å®ƒã€‚ä»ç°åœ¨èµ·ï¼Œä½¿ç”¨ JSON è§£æå™¨ï¼ˆä¾‹å¦‚ Python çš„æ ‡å‡† json åº“ï¼‰æ£€æŸ¥æ ¼å¼æ˜¯å¦å­˜åœ¨é”™è¯¯ä¹Ÿå¾ˆå®¹æ˜“ï¼Œå› ä¸ºæŸåçš„ JSON ä¼šå¯¼è‡´è§£æé”™è¯¯ï¼Œè¿™å¯ä»¥ä½œä¸ºè§¦å‘å™¨ï¼Œåœ¨ç»§ç»­ä¹‹å‰é‡è¯•æç¤ºæˆ–è¿›è¡Œè°ƒæŸ¥ã€‚å¦‚æœæ‚¨ä»ç„¶æ²¡æœ‰å¾—åˆ°æ­£ç¡®çš„æ ¼å¼ï¼Œå®ƒå¯ä»¥å¸®åŠ©æ‚¨åœ¨æç¤ºçš„å¼€å¤´æˆ–ç»“å°¾æŒ‡å®šï¼Œæˆ–è€…åœ¨ç³»ç»Ÿæ¶ˆæ¯ä¸­æŒ‡å®šï¼ˆå¦‚æœä½¿ç”¨èŠå¤©æ¨¡å‹ï¼‰ï¼šÂ <code>You are a helpful assistant that only responds in JSON</code>Â ï¼Œæˆ–è€…åœ¨ä¸­æŒ‡å®š JSON è¾“å‡ºå¯ç”¨çš„æ¨¡å‹å‚æ•°ï¼ˆè¿™ç§°ä¸º Llama æ¨¡å‹çš„è¯­æ³•ã€‚</p>
<h6 id="tip-2">TIP</h6>
<p>To get up to speed on JSON if youâ€™re unfamiliar, W3SchoolsÂ <a href="https://oreil.ly/Xakgc">has a good introduction</a>.<br>
å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ JSONï¼Œä¸ºäº†å¿«é€Ÿäº†è§£ JSONï¼ŒW3Schools æœ‰ä¸€ä¸ªå¾ˆå¥½çš„ä»‹ç»ã€‚</p>
<p>For image generationÂ models, format is very important, because the opportunities for modifying an image are near endless. They range from obvious formats likeÂ <code>stock photo</code>,Â <code>illustration</code>, andÂ <code>oil painting</code>, to more unusual formats likeÂ <code>dashcam footage</code>,Â <code>ice sculpture</code>, orÂ <code>in Minecraft</code>Â (seeÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-7">FigureÂ 1-7</a>).<br>
å¯¹äºå›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæ ¼å¼éå¸¸é‡è¦ï¼Œå› ä¸ºä¿®æ”¹å›¾åƒçš„æœºä¼šå‡ ä¹æ˜¯æ— ç©·æ— å°½çš„ã€‚å®ƒä»¬çš„èŒƒå›´ä»æ˜æ˜¾çš„æ ¼å¼ï¼ˆå¦‚Â <code>stock photo</code>Â ã€Â <code>illustration</code>Â å’ŒÂ <code>oil painting</code>Â ï¼‰åˆ°æ›´ä¸å¯»å¸¸çš„æ ¼å¼ï¼ˆå¦‚Â <code>dashcam footage</code>Â ã€Â <code>ice sculpture</code>Â ï¼ˆå‚è§å›¾ 1-7ï¼‰ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>business meeting of four people watching on MacBook on top of
table, in Minecraft</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-7">FigureÂ 1-7</a>Â shows the output.<br>
å›¾ 1-7 æ˜¾ç¤ºäº†è¾“å‡ºã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0107.png" alt="pega 0107"></p>
<h6 id="figure-1-7business-meeting-in-minecraft">Figure 1-7.Â Business meeting in Minecraft</h6>
<p>å›¾ 1-7ã€‚ Minecraft ä¸­çš„å•†åŠ¡ä¼šè®®</p>
<p>When setting a format, it is often necessary to remove other aspects of the prompt that might clash with the specified format. For example, if you supply a base image of a stock photo, the result is some combination of stock photo and the format you wanted. To some degree, image generation models can generalize to new scenarios and combinations they havenâ€™t seen before in their training set, but in our experience, the more layers of unrelated elements, the more likely you are to get an unsuitable image.<br>
è®¾ç½®æ ¼å¼æ—¶ï¼Œé€šå¸¸éœ€è¦åˆ é™¤å¯èƒ½ä¸æŒ‡å®šæ ¼å¼å†²çªçš„æç¤ºçš„å…¶ä»–æ–¹é¢ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æä¾›åº“å­˜ç…§ç‰‡çš„åŸºæœ¬å›¾åƒï¼Œåˆ™ç»“æœæ˜¯åº“å­˜ç…§ç‰‡å’Œæ‚¨æƒ³è¦çš„æ ¼å¼çš„æŸç§ç»„åˆã€‚åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œå›¾åƒç”Ÿæˆæ¨¡å‹å¯ä»¥æ³›åŒ–åˆ°ä»–ä»¬ä»¥å‰åœ¨è®­ç»ƒé›†ä¸­ä»æœªè§è¿‡çš„æ–°åœºæ™¯å’Œç»„åˆï¼Œä½†æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼Œä¸ç›¸å…³å…ƒç´ çš„å±‚æ•°è¶Šå¤šï¼Œè·å¾—ä¸åˆé€‚å›¾åƒçš„å¯èƒ½æ€§å°±è¶Šå¤§ã€‚</p>
<p>There is often some overlap between theÂ first and second principles, Give Direction and Specify Format. The latter is about defining what type of output you want, for example JSON format, or the format of a stock photo. The former is about the style of response you want, independent from the format, for example product names in the style of Steve Jobs, or an image of a business meeting in the style of Van Gogh. When there are clashes between style and format, itâ€™s often best to resolve themÂ by dropping whichever element is less important to your final result.<br>
ç¬¬ä¸€åŸåˆ™å’Œç¬¬äºŒåŸåˆ™ï¼ˆç»™å‡ºæ–¹å‘å’ŒæŒ‡å®šæ ¼å¼ï¼‰ä¹‹é—´ç»å¸¸æœ‰ä¸€äº›é‡å ã€‚åè€…æ˜¯å…³äºå®šä¹‰æ‚¨æƒ³è¦çš„è¾“å‡ºç±»å‹ï¼Œä¾‹å¦‚ JSON æ ¼å¼æˆ–åº“å­˜ç…§ç‰‡çš„æ ¼å¼ã€‚å‰è€…æ˜¯å…³äºæ‚¨æƒ³è¦çš„å“åº”é£æ ¼ï¼Œä¸æ ¼å¼æ— å…³ï¼Œä¾‹å¦‚å²è’‚å¤«Â·ä¹”å¸ƒæ–¯é£æ ¼çš„äº§å“åç§°ï¼Œæˆ–æ¢µé«˜é£æ ¼çš„å•†åŠ¡ä¼šè®®å›¾åƒã€‚å½“é£æ ¼å’Œæ ¼å¼ä¹‹é—´å­˜åœ¨å†²çªæ—¶ï¼Œé€šå¸¸æœ€å¥½é€šè¿‡åˆ é™¤å¯¹æœ€ç»ˆç»“æœä¸å¤ªé‡è¦çš„å…ƒç´ æ¥è§£å†³å®ƒä»¬ã€‚</p>
<h1 id="3-provide-examples3-æä¾›ä¾‹å­">3. Provide ExamplesÂ 3. æä¾›ä¾‹å­</h1>
<p>The original prompt didnâ€™t give the AI any examplesÂ of what you thinkÂ <em>good</em>Â names look like. Therefore, the response is approximate to an average of the internet, and you can do better than that. Researchers would call a prompt with no examplesÂ <em>zero-shot</em>, and itâ€™sÂ always a pleasant surprise when AI can even do a task zero shot: itâ€™s a sign of a powerful model. If youâ€™re providing zero examples, youâ€™re asking for a lot without giving much in return. Even providing one example (<em>one-shot</em>) helps considerably, and itâ€™s the norm among researchers to test how models perform with multiple examples (<em>few-shot</em>). One such piece of research is the famous GPT-3 paperÂ <a href="https://oreil.ly/KW5PS">â€œLanguage Models are Few-Shot Learnersâ€</a>, the results of which are illustrated inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-8">FigureÂ 1-8</a>, showing adding one example along with a prompt can improve accuracy in some tasks from 10% to near 50%!<br>
æœ€åˆçš„æç¤ºå¹¶æ²¡æœ‰ç»™äººå·¥æ™ºèƒ½ä»»ä½•ä½ è®¤ä¸ºå¥½åå­—æ˜¯ä»€ä¹ˆæ ·å­çš„ä¾‹å­ã€‚å› æ­¤ï¼Œå“åº”è¿‘ä¼¼äºäº’è”ç½‘çš„å¹³å‡æ°´å¹³ï¼Œæ‚¨å¯ä»¥åšå¾—æ›´å¥½ã€‚ç ”ç©¶äººå‘˜å°†æ²¡æœ‰ç¤ºä¾‹çš„æç¤ºç§°ä¸ºé›¶æ ·æœ¬ï¼Œå½“äººå·¥æ™ºèƒ½ç”šè‡³å¯ä»¥å®Œæˆé›¶æ ·æœ¬ä»»åŠ¡æ—¶ï¼Œæ€»æ˜¯ä»¤äººæƒŠå–œï¼šè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§æ¨¡å‹çš„æ ‡å¿—ã€‚å¦‚æœä½ æä¾›çš„ä¾‹å­ä¸ºé›¶ï¼Œé‚£ä¹ˆä½ å°±è¦æ±‚å¾ˆå¤šå´æ²¡æœ‰ç»™äºˆå¤ªå¤šå›æŠ¥ã€‚å³ä½¿æä¾›ä¸€ä¸ªç¤ºä¾‹ï¼ˆä¸€æ¬¡æ€§ï¼‰ä¹Ÿä¼šæœ‰å¾ˆå¤§å¸®åŠ©ï¼Œå¹¶ä¸”ç ”ç©¶äººå‘˜ä½¿ç”¨å¤šä¸ªç¤ºä¾‹ï¼ˆå‡ æ¬¡ï¼‰æ¥æµ‹è¯•æ¨¡å‹çš„è¡¨ç°æ˜¯ä¸€ç§å¸¸æ€ã€‚å…¶ä¸­ä¸€é¡¹ç ”ç©¶æ˜¯è‘—åçš„ GPT-3 è®ºæ–‡â€œLanguage Models are Few-Shot Learnersâ€ï¼Œå…¶ç»“æœå¦‚å›¾ 1-8 æ‰€ç¤ºï¼Œæ˜¾ç¤ºæ·»åŠ ä¸€ä¸ªç¤ºä¾‹å’Œæç¤ºå¯ä»¥å°†æŸäº›ä»»åŠ¡çš„å‡†ç¡®æ€§ä» 10 æé«˜åˆ° 10ã€‚ % æ¥è¿‘ 50%ï¼</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0108.png" alt="pega 0108"></p>
<h6 id="figure-1-8number-of-examples-in-context">Figure 1-8.Â Number of examples in context</h6>
<p>å›¾ 1-8ã€‚ä¸Šä¸‹æ–‡ä¸­çš„ç¤ºä¾‹æ•°é‡</p>
<p>When briefing a colleague or training a junior employee on a new task, itâ€™s only natural that youâ€™d include examples of times that task had previously been done well. Working with AI is the same, and the strength of a prompt often comes down to the examples used. Providing examples can sometimes be easier than trying to explain exactly what it is about those examples you like, so this technique is most effective when you are not a domain expert in the subject area of the task you are attempting to complete. The amount of text you can fit in a prompt is limited (at the time of writing around 6,000 characters on Midjourney and approximately 32,000 characters for the free version of ChatGPT), so a lot of the work of prompt engineering involves selecting and inserting diverse and instructive examples.<br>
å½“å‘åŒäº‹ä»‹ç»æ–°ä»»åŠ¡æˆ–å¯¹åˆçº§å‘˜å·¥è¿›è¡Œæ–°ä»»åŠ¡åŸ¹è®­æ—¶ï¼Œæ‚¨å¾ˆè‡ªç„¶åœ°ä¼šåˆ—ä¸¾ä¹‹å‰å®Œæˆè¯¥ä»»åŠ¡çš„ä¾‹å­ã€‚ä½¿ç”¨äººå·¥æ™ºèƒ½ä¹Ÿæ˜¯ä¸€æ ·ï¼Œæç¤ºçš„å¼ºåº¦é€šå¸¸å–å†³äºæ‰€ä½¿ç”¨çš„ç¤ºä¾‹ã€‚æä¾›ç¤ºä¾‹æœ‰æ—¶æ¯”å°è¯•å‡†ç¡®è§£é‡Šæ‚¨å–œæ¬¢çš„ç¤ºä¾‹æ›´å®¹æ˜“ï¼Œå› æ­¤å½“æ‚¨ä¸æ˜¯è¦å®Œæˆçš„ä»»åŠ¡çš„ä¸»é¢˜é¢†åŸŸçš„é¢†åŸŸä¸“å®¶æ—¶ï¼Œæ­¤æŠ€æœ¯æœ€æœ‰æ•ˆã€‚æç¤ºä¸­å¯ä»¥å®¹çº³çš„æ–‡æœ¬é‡æ˜¯æœ‰é™çš„ï¼ˆåœ¨ Midjourney ä¸Šç¼–å†™æ—¶çº¦ä¸º 6,000 ä¸ªå­—ç¬¦ï¼Œåœ¨ ChatGPT å…è´¹ç‰ˆæœ¬ä¸­çº¦ä¸º 32,000 ä¸ªå­—ç¬¦ï¼‰ï¼Œå› æ­¤æç¤ºå·¥ç¨‹çš„å¤§é‡å·¥ä½œæ¶‰åŠé€‰æ‹©å’Œæ’å…¥å„ç§ä¸åŒçš„æ–‡æœ¬ã€‚å…·æœ‰æŒ‡å¯¼æ„ä¹‰çš„ä¾‹å­ã€‚</p>
<p>Thereâ€™s a trade-off between reliability and creativity: go past three to five examples and your results will become more reliable, while sacrificing creativity. The more examples you provide, and the lesser the diversity between them, the more constrained the response will be to match your examples. If you change all of the examples to animal names in the previous prompt, youâ€™ll have a strong effect on the response, which will reliably return only names including animals.<br>
å¯é æ€§å’Œåˆ›é€ åŠ›ä¹‹é—´éœ€è¦æƒè¡¡ï¼šç»è¿‡ä¸‰åˆ°äº”ä¸ªä¾‹å­ï¼Œä½ çš„ç»“æœä¼šå˜å¾—æ›´åŠ å¯é ï¼Œä½†ä¼šç‰ºç‰²åˆ›é€ åŠ›ã€‚æ‚¨æä¾›çš„ç¤ºä¾‹è¶Šå¤šï¼Œå®ƒä»¬ä¹‹é—´çš„å¤šæ ·æ€§è¶Šå°ï¼Œå“åº”ä¸æ‚¨çš„ç¤ºä¾‹ç›¸åŒ¹é…çš„é™åˆ¶å°±è¶Šå¤§ã€‚å¦‚æœæ‚¨å°†ä¸Šä¸€ä¸ªæç¤ºä¸­çš„æ‰€æœ‰ç¤ºä¾‹æ›´æ”¹ä¸ºåŠ¨ç‰©åç§°ï¼Œå°†å¯¹å“åº”äº§ç”Ÿå¾ˆå¤§å½±å“ï¼Œè¯¥å“åº”å°†å¯é åœ°ä»…è¿”å›åŒ…æ‹¬åŠ¨ç‰©çš„åç§°ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Brainstorm a list of product names for a shoe that fits any
foot size.</p>
<p>Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of 3 product names]</p>
<h2 id="examples-2">Examples:</h2>
<p>Product description: A home milkshake maker.
Product names: Fast Panda, Healthy Bear, Compact Koala</p>
<p>Product description: A watch that can tell accurate time in
space.
Product names: AstroLamb, Space Bear, Eagle Orbit</p>
<p>Product description: A refrigerator that dispenses beer
Product names: BearFridge, Cool Cat, PenguinBox</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Product description: A shoe that fits any foot size
Product names: FlexiFox, ChameleonStep, PandaPaws</p>
<p>Of course this runs the risk of missing out on returning a much better name that doesnâ€™t fit the limited space left for the AI to play in. Lack of diversity and variation in examples is also a problem in handling edge cases, or uncommon scenarios. Including one to three examples is easy and almost always has a positive effect, but above that number it becomes essential to experiment with the number of examples you include, as well as the similarity between them. There is some evidence (<a href="https://oreil.ly/6Ixcw">Hsieh et al., 2023</a>) that direction works better than providing examples, and it typically isnâ€™t straightforward to collect good examples, so itâ€™s usually prudent to attempt the principle of Give Direction first.<br>
å½“ç„¶ï¼Œè¿™å­˜åœ¨ç€é”™è¿‡è¿”å›ä¸€ä¸ªæ›´å¥½çš„åç§°çš„é£é™©ï¼Œè¯¥åç§°ä¸é€‚åˆäººå·¥æ™ºèƒ½å‘æŒ¥ä½œç”¨çš„æœ‰é™ç©ºé—´ã€‚ç¤ºä¾‹ä¸­ç¼ºä¹å¤šæ ·æ€§å’Œå˜åŒ–ä¹Ÿæ˜¯å¤„ç†è¾¹ç¼˜æƒ…å†µæˆ–ä¸å¸¸è§åœºæ™¯çš„é—®é¢˜ã€‚åŒ…å«ä¸€åˆ°ä¸‰ä¸ªç¤ºä¾‹å¾ˆå®¹æ˜“ï¼Œå¹¶ä¸”å‡ ä¹æ€»æ˜¯ä¼šäº§ç”Ÿç§¯æçš„æ•ˆæœï¼Œä½†è¶…è¿‡è¿™ä¸ªæ•°å­—ï¼Œå°±å¿…é¡»å°è¯•åŒ…å«çš„ç¤ºä¾‹æ•°é‡ä»¥åŠå®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚æœ‰ä¸€äº›è¯æ®ï¼ˆHsieh ç­‰äººï¼Œ2023ï¼‰è¡¨æ˜æŒ‡å¯¼æ¯”æä¾›ç¤ºä¾‹æ›´æœ‰æ•ˆï¼Œè€Œä¸”æ”¶é›†å¥½çš„ç¤ºä¾‹é€šå¸¸å¹¶ä¸å®¹æ˜“ï¼Œå› æ­¤é¦–å…ˆå°è¯•â€œç»™äºˆæŒ‡å¯¼â€åŸåˆ™é€šå¸¸æ˜¯è°¨æ…çš„ã€‚</p>
<p>In the image generation space, providing examples usuallyÂ comes in the form of providing a base image in the prompt, calledÂ <em>img2img</em>Â in the open sourceÂ <a href="https://oreil.ly/huVRu">Stable Diffusion</a>Â community. Depending on the image generation model being used, these images can be used as a starting point for the model to generate from, which greatly affects the results. You can keep everything about the prompt the same but swap out the provided base image for a radically different effect, as inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-9">FigureÂ 1-9</a>.<br>
åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸï¼Œæä¾›ç¤ºä¾‹é€šå¸¸ä»¥åœ¨æç¤ºä¸­æä¾›åŸºç¡€å›¾åƒçš„å½¢å¼å‡ºç°ï¼Œåœ¨å¼€æº Stable Diffusion ç¤¾åŒºä¸­ç§°ä¸º img2imgã€‚æ ¹æ®æ‰€ä½¿ç”¨çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œè¿™äº›å›¾åƒå¯ä»¥ç”¨ä½œæ¨¡å‹ç”Ÿæˆçš„èµ·ç‚¹ï¼Œè¿™æå¤§åœ°å½±å“ç»“æœã€‚æ‚¨å¯ä»¥ä¿æŒæç¤ºçš„æ‰€æœ‰å†…å®¹ç›¸åŒï¼Œä½†å°†æä¾›çš„åŸºæœ¬å›¾åƒæ›¿æ¢ä¸ºå®Œå…¨ä¸åŒçš„æ•ˆæœï¼Œå¦‚å›¾ 1-9 æ‰€ç¤ºã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>stock photo of business meeting of 4 people watching on
white MacBook on top of glass-top table, Panasonic, DC-GH5</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-9">FigureÂ 1-9</a>Â shows the output.<br>
å›¾ 1-9 æ˜¾ç¤ºäº†è¾“å‡ºã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0109.png" alt="pega 0109"></p>
<h6 id="figure-1-9stock-photo-of-business-meeting-of-four-people">Figure 1-9.Â Stock photo of business meeting of four people</h6>
<p>å›¾ 1-9ã€‚å››äººå•†åŠ¡ä¼šè®®å›¾åº“ç…§ç‰‡</p>
<p>In this case, by substituting for the image shown inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-10">FigureÂ 1-10</a>, also from Unsplash, you can see how the model was pulled in a different direction and incorporates whiteboards and sticky notes now.<br>
åœ¨æœ¬ä¾‹ä¸­ï¼Œé€šè¿‡æ›¿æ¢åŒæ ·æ¥è‡ª Unsplash çš„å›¾ 1-10 ä¸­æ‰€ç¤ºçš„å›¾åƒï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ¨¡å‹å¦‚ä½•è¢«æ‹‰å‘ä¸åŒçš„æ–¹å‘ï¼Œå¹¶ä¸”ç°åœ¨å¦‚ä½•åˆå¹¶ç™½æ¿å’Œä¾¿ç­¾ã€‚</p>
<h6 id="cautionè­¦å‘Š">CAUTIONÂ è­¦å‘Š</h6>
<p>These examples demonstrate the capabilities of image generation models, but we would exercise caution when uploading base images for use in prompts. Check the licensing of the image you plan to upload and use in your prompt as the base image, and avoid using clearly copyrighted images. Doing so can land you in legal trouble and is against the terms ofÂ service for all the major image generation model providers.<br>
è¿™äº›ç¤ºä¾‹æ¼”ç¤ºäº†å›¾åƒç”Ÿæˆæ¨¡å‹çš„åŠŸèƒ½ï¼Œä½†æˆ‘ä»¬åœ¨ä¸Šä¼ ç”¨äºæç¤ºçš„åŸºç¡€å›¾åƒæ—¶è¦å°å¿ƒã€‚æ£€æŸ¥æ‚¨è®¡åˆ’ä¸Šä¼ å¹¶åœ¨æç¤ºä¸­ç”¨ä½œåŸºç¡€å›¾åƒçš„å›¾åƒçš„è®¸å¯ï¼Œå¹¶é¿å…ä½¿ç”¨æ˜æ˜¾å—ç‰ˆæƒä¿æŠ¤çš„å›¾åƒã€‚è¿™æ ·åšå¯èƒ½ä¼šç»™æ‚¨å¸¦æ¥æ³•å¾‹éº»çƒ¦ï¼Œå¹¶ä¸”è¿åæ‰€æœ‰ä¸»è¦å›¾åƒç”Ÿæˆæ¨¡å‹æä¾›å•†çš„æœåŠ¡æ¡æ¬¾ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0110.png" alt="pega 0110"></p>
<h6 id="figure-1-10photo-by-jason-goodman-onunsplash">Figure 1-10.Â Photo by Jason Goodman onÂ <a href="https://oreil.ly/ZbzZy">Unsplash</a></h6>
<p>å›¾ 1-10ã€‚æ°æ£®Â·å¤å¾·æ›¼ (Jason Goodman) åœ¨ Unsplash ä¸Šæ‹æ‘„çš„ç…§ç‰‡</p>
<h1 id="4-evaluate-quality4-è¯„ä¼°è´¨é‡">4. Evaluate QualityÂ 4. è¯„ä¼°è´¨é‡</h1>
<p>As of yet, there has been no feedback loop toÂ judge the quality of your responses, other than the basic trial and error of running the prompt and seeing the results, referred to asÂ <a href="https://oreil.ly/42rSz"><em>blind prompting</em></a>. This is fine when your prompts are used temporarily for a single task and rarely revisited. However, when youâ€™re reusing the same prompt multiple times or building a production application that relies on a prompt, you need to be more rigorous with measuring results.<br>
åˆ°ç›®å‰ä¸ºæ­¢ï¼Œé™¤äº†è¿è¡Œæç¤ºå¹¶æŸ¥çœ‹ç»“æœçš„åŸºæœ¬å°è¯•å’Œé”™è¯¯ï¼ˆç§°ä¸ºç›²ç›®æç¤ºï¼‰ä¹‹å¤–ï¼Œè¿˜æ²¡æœ‰åé¦ˆå¾ªç¯æ¥åˆ¤æ–­æ‚¨çš„å›ç­”è´¨é‡ã€‚å½“æ‚¨çš„æç¤ºæš‚æ—¶ç”¨äºå•ä¸ªä»»åŠ¡å¹¶ä¸”å¾ˆå°‘é‡æ–°è®¿é—®æ—¶ï¼Œè¿™å¾ˆå¥½ã€‚ä½†æ˜¯ï¼Œå½“æ‚¨å¤šæ¬¡é‡å¤ä½¿ç”¨ç›¸åŒçš„æç¤ºæˆ–æ„å»ºä¾èµ–äºæç¤ºçš„ç”Ÿäº§åº”ç”¨ç¨‹åºæ—¶ï¼Œæ‚¨éœ€è¦æ›´åŠ ä¸¥æ ¼åœ°æµ‹é‡ç»“æœã€‚</p>
<p>There are a number of ways performance can be evaluated, and it depends largely on what tasks youâ€™re hoping to accomplish. When a new AI model is released, the focus tends to be onÂ how well the model did onÂ <em>evals</em>Â (evaluations), a standardized set of questions with predefined answers or grading criteria that are used to test performance across models. Different models perform differently across different types of tasks, and there is no guarantee a prompt that worked previously will translate well to a new model. OpenAI hasÂ <a href="https://oreil.ly/wolEL">made its evals framework</a>Â for benchmarking performance of LLMs open source and encourages others to contribute additional eval templates.<br>
è¯„ä¼°ç»©æ•ˆçš„æ–¹æ³•æœ‰å¾ˆå¤šç§ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ‚¨å¸Œæœ›å®Œæˆçš„ä»»åŠ¡ã€‚å½“æ–°çš„äººå·¥æ™ºèƒ½æ¨¡å‹å‘å¸ƒæ—¶ï¼Œäººä»¬å…³æ³¨çš„ç„¦ç‚¹å¾€å¾€æ˜¯è¯¥æ¨¡å‹åœ¨è¯„ä¼°ï¼ˆevalï¼‰æ–¹é¢çš„è¡¨ç°å¦‚ä½•ï¼Œè¯„ä¼°æ˜¯ä¸€ç»„å¸¦æœ‰é¢„å®šä¹‰ç­”æ¡ˆæˆ–è¯„åˆ†æ ‡å‡†çš„æ ‡å‡†åŒ–é—®é¢˜ï¼Œç”¨äºæµ‹è¯•è·¨æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸åŒçš„æ¨¡å‹åœ¨ä¸åŒç±»å‹çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸åŒï¼Œå¹¶ä¸”ä¸èƒ½ä¿è¯ä»¥å‰æœ‰æ•ˆçš„æç¤ºèƒ½å¤Ÿå¾ˆå¥½åœ°è½¬æ¢ä¸ºæ–°æ¨¡å‹ã€‚ OpenAI å·²å°†å…¶ç”¨äº LLMs æ€§èƒ½åŸºå‡†æµ‹è¯•çš„è¯„ä¼°æ¡†æ¶å¼€æºï¼Œå¹¶é¼“åŠ±å…¶ä»–äººè´¡çŒ®æ›´å¤šè¯„ä¼°æ¨¡æ¿ã€‚</p>
<p>In addition to the standard academic evals, there are also more headline-worthy tests likeÂ <a href="https://oreil.ly/txhSZ">GPT-4 passing the bar exam</a>. Evaluation is difficult for more subjective tasks, and can be time-consuming or prohibitively costly for smaller teams. In some instances researchers have turned to using more advanced models like GPT-4 to evaluate responses from less sophisticated models, as was done withÂ <a href="https://oreil.ly/NW3WX">the release of Vicuna-13B</a>, a fine-tuned model based on Metaâ€™s Llama open source model (seeÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-11">FigureÂ 1-11</a>).<br>
é™¤äº†æ ‡å‡†çš„å­¦æœ¯è¯„ä¼°ä¹‹å¤–ï¼Œè¿˜æœ‰æ›´å¤šå€¼å¾—å…³æ³¨çš„æµ‹è¯•ï¼Œä¾‹å¦‚é€šè¿‡å¾‹å¸ˆèµ„æ ¼è€ƒè¯•çš„ GPT-4ã€‚å¯¹äºæ›´ä¸»è§‚çš„ä»»åŠ¡æ¥è¯´ï¼Œè¯„ä¼°å¾ˆå›°éš¾ï¼Œå¯¹äºè¾ƒå°çš„å›¢é˜Ÿæ¥è¯´ï¼Œè¯„ä¼°å¯èƒ½éå¸¸è€—æ—¶æˆ–æˆæœ¬é«˜æ˜‚ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç ”ç©¶äººå‘˜è½¬å‘ä½¿ç”¨ GPT-4 ç­‰æ›´å…ˆè¿›çš„æ¨¡å‹æ¥è¯„ä¼°ä¸å¤ªå¤æ‚çš„æ¨¡å‹çš„å“åº”ï¼Œå°±åƒå‘å¸ƒ Vicuna-13B æ‰€åšçš„é‚£æ ·ï¼ŒVicuna-13B æ˜¯ä¸€ä¸ªåŸºäº Meta çš„ Llama å¼€æºæ¨¡å‹çš„å¾®è°ƒæ¨¡å‹ï¼ˆè§å›¾ 1ï¼‰ -11ï¼‰ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0111.png" alt="pega 0111"></p>
<h6 id="figure-1-11vicuna-gpt-4-evals">Figure 1-11.Â Vicuna GPT-4 Evals</h6>
<p>å›¾ 1-11ã€‚éª†é©¼æ¯› GPT-4 è¯„ä¼°</p>
<p>More rigorous evaluation techniques are necessary when writing scientific papers or grading a new foundation model release, but often you will only need to go just one step above basic trial and error. You may find that a simple thumbs-up/thumbs-down rating system implemented in a Jupyter Notebook can be enough to add some rigor to prompt optimization, without adding too much overhead. One common test is to see whether providing examples is worth the additional cost in terms of prompt length, or whether you can get away with providing no examples in the prompt. The first step is getting responses for multiple runs of each prompt and storing them in a spreadsheet, which we will do after setting up our environment.<br>
åœ¨æ’°å†™ç§‘å­¦è®ºæ–‡æˆ–å¯¹æ–°çš„åŸºç¡€æ¨¡å‹ç‰ˆæœ¬è¿›è¡Œè¯„åˆ†æ—¶ï¼Œéœ€è¦æ›´ä¸¥æ ¼çš„è¯„ä¼°æŠ€æœ¯ï¼Œä½†é€šå¸¸æ‚¨åªéœ€è¦åœ¨åŸºæœ¬çš„è¯•é”™ä¹‹ä¸Šå†è¿ˆå‡ºä¸€æ­¥ã€‚æ‚¨å¯èƒ½ä¼šå‘ç°ï¼Œåœ¨ Jupyter Notebook ä¸­å®ç°çš„ç®€å•çš„èµæˆ/åå¯¹è¯„çº§ç³»ç»Ÿè¶³ä»¥ä¸ºæç¤ºä¼˜åŒ–æ·»åŠ ä¸€äº›ä¸¥æ ¼æ€§ï¼Œè€Œä¸ä¼šå¢åŠ å¤ªå¤šå¼€é”€ã€‚ä¸€ç§å¸¸è§çš„æµ‹è¯•æ˜¯çœ‹çœ‹æä¾›ç¤ºä¾‹æ˜¯å¦å€¼å¾—åœ¨æç¤ºé•¿åº¦æ–¹é¢ä»˜å‡ºé¢å¤–çš„æˆæœ¬ï¼Œæˆ–è€…æ‚¨æ˜¯å¦å¯ä»¥åœ¨æç¤ºä¸­ä¸æä¾›ç¤ºä¾‹ã€‚ç¬¬ä¸€æ­¥æ˜¯è·å–æ¯ä¸ªæç¤ºå¤šæ¬¡è¿è¡Œçš„å“åº”å¹¶å°†å…¶å­˜å‚¨åœ¨ç”µå­è¡¨æ ¼ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨è®¾ç½®ç¯å¢ƒåæ‰§è¡Œæ­¤æ“ä½œã€‚</p>
<p>You can install the OpenAI PythonÂ package withÂ <code>pip install openai</code>. If youâ€™re running into compatability issues with this package, create a virtual environment and install ourÂ <a href="https://oreil.ly/2KDV6"><em>requirements.txt</em></a>Â (instructions in the preface).<br>
æ‚¨å¯ä»¥ä½¿ç”¨Â <code>pip install openai</code>Â å®‰è£… OpenAI Python åŒ…ã€‚å¦‚æœæ‚¨é‡åˆ°æ­¤è½¯ä»¶åŒ…çš„å…¼å®¹æ€§é—®é¢˜ï¼Œè¯·åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…æˆ‘ä»¬çš„requirements.txtï¼ˆå‰è¨€ä¸­çš„è¯´æ˜ï¼‰ã€‚</p>
<p>To utilize the API, youâ€™llÂ need toÂ <a href="https://oreil.ly/oGv4j">create an OpenAI account</a>Â and thenÂ <a href="https://oreil.ly/oHID1">navigate here for your API key</a>.<br>
è¦ä½¿ç”¨è¯¥ APIï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ª OpenAI å¸æˆ·ï¼Œç„¶ååœ¨æ­¤å¤„å¯¼èˆªä»¥è·å–æ‚¨çš„ API å¯†é’¥ã€‚</p>
<h6 id="warningè­¦å‘Š-1">WARNINGÂ è­¦å‘Š</h6>
<p>Hardcoding API keys in scripts is not recommended due to security reasons. Instead, utilize environment variables or configuration files to manage your keys.<br>
å‡ºäºå®‰å…¨åŸå› ï¼Œä¸å»ºè®®åœ¨è„šæœ¬ä¸­å¯¹ API å¯†é’¥è¿›è¡Œç¡¬ç¼–ç ã€‚ç›¸åï¼Œåˆ©ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶æ¥ç®¡ç†æ‚¨çš„å¯†é’¥ã€‚</p>
<p>Once you have an API key, itâ€™s crucial to assign it as an environment variable by executing the following command, replacingÂ <code>api_key</code>Â with your actual API key value:<br>
è·å¾— API å¯†é’¥åï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å°†å…¶åˆ†é…ä¸ºç¯å¢ƒå˜é‡è‡³å…³é‡è¦ï¼Œå¹¶å°†Â <code>api_key</code>Â æ›¿æ¢ä¸ºæ‚¨çš„å®é™… API å¯†é’¥å€¼ï¼š</p>
<pre tabindex="0"><code>export
</code></pre><p>Or on Windows:Â æˆ–è€…åœ¨ Windows ä¸Šï¼š</p>
<pre tabindex="0"><code>set
</code></pre><p>Alternatively, if youâ€™d prefer not to preset an API key, then you can manually set the key while initializing the model, or load it from anÂ <em>.env</em>Â file usingÂ <em><a href="https://oreil.ly/IaQjS">python-dotenv</a></em>. First, install the library withÂ <code>pip install python-dotenv</code>, and then load the environment variables with the following code at the top of your script or notebook:<br>
æˆ–è€…ï¼Œå¦‚æœæ‚¨ä¸æƒ³é¢„è®¾ API å¯†é’¥ï¼Œåˆ™å¯ä»¥åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶æ‰‹åŠ¨è®¾ç½®å¯†é’¥ï¼Œæˆ–ä½¿ç”¨ python-dotenv ä» .env æ–‡ä»¶åŠ è½½å®ƒã€‚é¦–å…ˆï¼Œä½¿ç”¨Â <code>pip install python-dotenv</code>Â å®‰è£…åº“ï¼Œç„¶ååœ¨è„šæœ¬æˆ–ç¬”è®°æœ¬é¡¶éƒ¨ä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½ç¯å¢ƒå˜é‡ï¼š</p>
<pre tabindex="0"><code>from
</code></pre><p>The first step is getting responses for multiple runs of each prompt and storing them in a spreadsheet.<br>
ç¬¬ä¸€æ­¥æ˜¯è·å–æ¯ä¸ªæç¤ºå¤šæ¬¡è¿è¡Œçš„å“åº”å¹¶å°†å…¶å­˜å‚¨åœ¨ç”µå­è¡¨æ ¼ä¸­ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<pre tabindex="0"><code># Define two variants of the prompt to test zero-shot
</code></pre><p>Output:Â è¾“å‡ºï¼š</p>
<p>variant                                             prompt
<br>
0       A  Product description: A pair of shoes that can &hellip;
1       A  Product description: A pair of shoes that can &hellip;
2       A  Product description: A pair of shoes that can &hellip;
3       A  Product description: A pair of shoes that can &hellip;
4       A  Product description: A pair of shoes that can &hellip;
5       B  Product description: A home milkshake maker.\n&hellip;
6       B  Product description: A home milkshake maker.\n&hellip;
7       B  Product description: A home milkshake maker.\n&hellip;
8       B  Product description: A home milkshake maker.\n&hellip;
9       B  Product description: A home milkshake maker.\n&hellip;</p>
<pre><code>                                        response
</code></pre>
<p>0  1. Adapt-a-Fit Shoes \n2. Omni-Fit Footwear \n&hellip;
1  1. OmniFit Shoes\n2. Adapt-a-Sneaks \n3. OneFi&hellip;
2  1. Adapt-a-fit\n2. Flexi-fit shoes\n3. Omni-fe&hellip;
3  1. Adapt-A-Sole\n2. FitFlex\n3. Omni-FitX\n4. &hellip;
4  1. Omni-Fit Shoes\n2. Adapt-a-Fit Shoes\n3. An&hellip;
5  Adapt-a-Fit, Perfect Fit Shoes, OmniShoe, OneS&hellip;
6       FitAll, OmniFit Shoes, SizeLess, AdaptaShoes
7       AdaptaFit, OmniShoe, PerfectFit, AllSizeFit.
8  FitMaster, AdaptoShoe, OmniFit, AnySize Footwe&hellip;
9        Adapt-a-Shoe, PerfectFit, OmniSize, FitForm</p>
<p>Here weâ€™re using the OpenAI API to generate modelÂ responses to a set of prompts and storing the results in a dataframe, which is saved to a CSV file. Hereâ€™s how it works:<br>
åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ OpenAI API ç”Ÿæˆå¯¹ä¸€ç»„æç¤ºçš„æ¨¡å‹å“åº”ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨æ•°æ®æ¡†ä¸­ï¼Œè¯¥æ•°æ®æ¡†ä¿å­˜åˆ° CSV æ–‡ä»¶ä¸­ã€‚å®ƒçš„å·¥ä½œåŸç†å¦‚ä¸‹ï¼š</p>
<ol>
<li>
<p>Two prompt variants are defined, and each variant consists of a product description, seed words, and potential product names, butÂ <code>prompt_B</code>Â provides twoÂ examples.<br>
å®šä¹‰äº†ä¸¤ä¸ªæç¤ºå˜ä½“ï¼Œæ¯ä¸ªå˜ä½“ç”±äº§å“æè¿°ã€ç§å­è¯å’Œæ½œåœ¨äº§å“åç§°ç»„æˆï¼Œä½†Â <code>prompt_B</code>Â æä¾›äº†ä¸¤ä¸ªç¤ºä¾‹ã€‚</p>
</li>
<li>
<p>Import statements are called for the Pandas library, OpenAI library, and os library.<br>
Pandas åº“ã€OpenAI åº“å’Œ os åº“è°ƒç”¨å¯¼å…¥è¯­å¥ã€‚</p>
</li>
<li>
<p>TheÂ <code>get_response</code>Â function takes a prompt as input and returns a response from theÂ <code>gpt-3.5-turbo</code>Â model. The prompt is passed as a user message to the model, along with a system message to set the modelâ€™s behavior.<br>
<code>get_response</code>Â å‡½æ•°å°†æç¤ºä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»Â <code>gpt-3.5-turbo</code>Â æ¨¡å‹è¿”å›å“åº”ã€‚æç¤ºä½œä¸ºç”¨æˆ·æ¶ˆæ¯ä¼ é€’åˆ°æ¨¡å‹ï¼Œå¹¶è¿åŒç”¨äºè®¾ç½®æ¨¡å‹è¡Œä¸ºçš„ç³»ç»Ÿæ¶ˆæ¯ã€‚</p>
</li>
<li>
<p>Two prompt variants are storedÂ in theÂ <code>test_prompts</code>Â list.<br>
<code>test_prompts</code>Â åˆ—è¡¨ä¸­å­˜å‚¨äº†ä¸¤ä¸ªæç¤ºå˜ä½“ã€‚</p>
</li>
<li>
<p>An empty listÂ <code>responses</code>Â is created to store theÂ generated responses, and the variableÂ <code>num_tests</code>Â is set to 5.<br>
åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨Â <code>responses</code>Â æ¥å­˜å‚¨ç”Ÿæˆçš„å“åº”ï¼Œå¹¶å°†å˜é‡Â <code>num_tests</code>Â è®¾ç½®ä¸º 5ã€‚</p>
</li>
<li>
<p>A nested loop isÂ used to generate responses. The outer loop iterates over each prompt, and the inner loop generatesÂ <code>num_tests</code>Â (five in this case) number of responses per prompt.<br>
åµŒå¥—å¾ªç¯ç”¨äºç”Ÿæˆå“åº”ã€‚å¤–éƒ¨å¾ªç¯è¿­ä»£æ¯ä¸ªæç¤ºï¼Œå†…éƒ¨å¾ªç¯ä¸ºæ¯ä¸ªæç¤ºç”ŸæˆÂ <code>num_tests</code>Â ï¼ˆæœ¬ä¾‹ä¸­ä¸º 5ï¼‰ä¸ªå“åº”ã€‚</p>
<ol>
<li>
<p>TheÂ <code>enumerate</code>Â function is used to get the index and value of each prompt inÂ <code>test_prompts</code>. This index is then converted to a corresponding uppercase letter (e.g., 0 becomesÂ <em>A</em>, 1 becomesÂ <em>B</em>) to be used as a variant name.<br>
<code>enumerate</code>Â å‡½æ•°ç”¨äºè·å–Â <code>test_prompts</code>Â ä¸­æ¯ä¸ªæç¤ºçš„ç´¢å¼•å’Œå€¼ã€‚ç„¶åå°†è¯¥ç´¢å¼•è½¬æ¢ä¸ºç›¸åº”çš„å¤§å†™å­—æ¯ï¼ˆä¾‹å¦‚ï¼Œ0 å˜ä¸º Aï¼Œ1 å˜ä¸º Bï¼‰ä»¥ç”¨ä½œå˜ä½“åç§°ã€‚</p>
</li>
<li>
<p>For each iteration, theÂ <code>get_response</code>Â function is called with the current prompt to generate a response from the model.<br>
å¯¹äºæ¯æ¬¡è¿­ä»£ï¼Œéƒ½ä¼šä½¿ç”¨å½“å‰æç¤ºè°ƒç”¨Â <code>get_response</code>Â å‡½æ•°ï¼Œä»¥ä»æ¨¡å‹ç”Ÿæˆå“åº”ã€‚</p>
</li>
<li>
<p>A dictionary is created with the variant name, the prompt, and the modelâ€™s response, and this dictionary is appended to theÂ <code>responses</code>Â list.<br>
ä½¿ç”¨å˜ä½“åç§°ã€æç¤ºå’Œæ¨¡å‹å“åº”åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œå¹¶å°†è¯¥å­—å…¸é™„åŠ åˆ°Â <code>responses</code>Â åˆ—è¡¨ä¸­ã€‚</p>
</li>
</ol>
</li>
<li>
<p>Once all responses have been generated, theÂ <code>responses</code>Â list (which is now a list of dictionaries) is converted into a Pandas DataFrame.<br>
ç”Ÿæˆæ‰€æœ‰å“åº”åï¼ŒÂ <code>responses</code>Â åˆ—è¡¨ï¼ˆç°åœ¨æ˜¯å­—å…¸åˆ—è¡¨ï¼‰å°†è½¬æ¢ä¸º Pandas DataFrameã€‚</p>
</li>
<li>
<p>This dataframe is then saved to a CSV file with the Pandas built-inÂ <code>to_csv</code>Â function, making the fileÂ <em>responses.csv</em>Â withÂ <code>index=False</code>Â so as to not write row indices.<br>
ç„¶åä½¿ç”¨ Pandas å†…ç½®Â <code>to_csv</code>Â å‡½æ•°å°†è¯¥æ•°æ®å¸§ä¿å­˜åˆ° CSV æ–‡ä»¶ä¸­ï¼Œä½¿æ–‡ä»¶response.csv å¸¦æœ‰Â <code>index=False</code>Â ä»¥ä¾¿ä¸å†™å…¥è¡Œç´¢å¼•ã€‚</p>
</li>
<li>
<p>Finally, the dataframe is printed to the console.<br>
æœ€åï¼Œæ•°æ®å¸§è¢«æ‰“å°åˆ°æ§åˆ¶å°ã€‚</p>
</li>
</ol>
<p>Having these responses in a spreadsheet is already useful, because you can see right away even in the printed response thatÂ <code>prompt_A</code>Â (zero-shot) in the first five rows is giving us a numbered list, whereasÂ <code>prompt_B</code>Â (few-shot) in the last five rows tends to output the desired format of a comma-separated inline list. The next step is to give a rating on each of the responses, which is best done blind and randomized to avoid favoring one prompt over another.<br>
åœ¨ç”µå­è¡¨æ ¼ä¸­åŒ…å«è¿™äº›å“åº”å·²ç»å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå³ä½¿åœ¨æ‰“å°çš„å“åº”ä¸­ï¼Œæ‚¨ä¹Ÿå¯ä»¥ç«‹å³çœ‹åˆ°å‰äº”è¡Œä¸­çš„Â <code>prompt_A</code>Â ï¼ˆé›¶æ ·æœ¬ï¼‰ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç¼–å·åˆ—è¡¨ï¼Œè€ŒÂ <code>prompt_B</code>Â (few-shot) å€¾å‘äºè¾“å‡ºä»¥é€—å·åˆ†éš”çš„å†…è”åˆ—è¡¨çš„æ‰€éœ€æ ¼å¼ã€‚ä¸‹ä¸€æ­¥æ˜¯å¯¹æ¯ä¸ªç­”æ¡ˆè¿›è¡Œè¯„åˆ†ï¼Œæœ€å¥½æ˜¯ç›²ç›®å’Œéšæœºè¿›è¡Œè¯„åˆ†ï¼Œä»¥é¿å…åå‘æŸä¸€æç¤ºè€Œä¸æ˜¯å¦ä¸€æç¤ºã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<pre tabindex="0"><code>import
</code></pre><p>The output is shown inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-12">FigureÂ 1-12</a>:<br>
è¾“å‡ºå¦‚å›¾ 1-12 æ‰€ç¤ºï¼š</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0112.png" alt="pega 0112"></p>
<h6 id="figure-1-12thumbs-upthumbs-down-rating-system">Figure 1-12.Â Thumbs-up/thumbs-down rating system</h6>
<p>å›¾ 1-12ã€‚èµæˆ/åå¯¹è¯„çº§ç³»ç»Ÿ</p>
<p>If you run this inÂ a Jupyter Notebook, a widget displays each AI response, with a thumbs-up or thumbs-down button (seeÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-12">FigureÂ 1-12</a>) This provides a simple interface for quickly labeling responses, with minimal overhead. If you wish to do this outside of a Jupyter Notebook, you could change the thumbs-up and thumbs-down emojis forÂ <em>Y</em>Â andÂ <em>N</em>, and implement a loop using the built-inÂ <code>input()</code>Â function, as a text-only replacement for iPyWidgets.<br>
å¦‚æœæ‚¨åœ¨ Jupyter Notebook ä¸­è¿è¡Œæ­¤ç¨‹åºï¼Œå°éƒ¨ä»¶ä¼šæ˜¾ç¤ºæ¯ä¸ª AI å“åº”ï¼Œå¹¶å¸¦æœ‰â€œèµæˆâ€æˆ–â€œåå¯¹â€æŒ‰é’®ï¼ˆè§å›¾ 1-12ï¼‰ã€‚è¿™æä¾›äº†ä¸€ä¸ªç®€å•çš„ç•Œé¢ï¼Œå¯ä»¥ä»¥æœ€å°çš„å¼€é”€å¿«é€Ÿæ ‡è®°å“åº”ã€‚å¦‚æœæ‚¨å¸Œæœ›åœ¨ Jupyter Notebook ä¹‹å¤–æ‰§è¡Œæ­¤æ“ä½œï¼Œæ‚¨å¯ä»¥æ›´æ”¹ Y å’Œ N çš„æ‹‡æŒ‡å‘ä¸Šå’Œæ‹‡æŒ‡å‘ä¸‹è¡¨æƒ…ç¬¦å·ï¼Œå¹¶ä½¿ç”¨å†…ç½®Â <code>input()</code>Â å‡½æ•°ä»¥æ–‡æœ¬å½¢å¼å®ç°å¾ªç¯- ä»…æ›¿ä»£ iPyWidgetsã€‚</p>
<p>Once youâ€™ve finished labeling the responses, you get the output, which shows you how each prompt performs.<br>
å®Œæˆå¯¹å“åº”çš„æ ‡è®°åï¼Œæ‚¨å°†è·å¾—è¾“å‡ºï¼Œå…¶ä¸­æ˜¾ç¤ºæ¯ä¸ªæç¤ºçš„æ‰§è¡Œæƒ…å†µã€‚</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>A/B testing completed. Here&rsquo;s the results:
variant  count  score
0       A      5    0.2
1       B      5    0.6</p>
<p>The dataframe was shuffled at random, and each response was labeled blind (without seeing the prompt), so you get an accurate picture of how often each prompt performed. Here is the step-by-step explanation:<br>
æ•°æ®æ¡†è¢«éšæœºæ‰“ä¹±ï¼Œæ¯ä¸ªå“åº”éƒ½è¢«æ ‡è®°ä¸ºç›²ï¼ˆçœ‹ä¸åˆ°æç¤ºï¼‰ï¼Œå› æ­¤æ‚¨å¯ä»¥å‡†ç¡®äº†è§£æ¯ä¸ªæç¤ºæ‰§è¡Œçš„é¢‘ç‡ã€‚ä»¥ä¸‹æ˜¯åˆ†æ­¥è¯´æ˜ï¼š</p>
<ol>
<li>
<p>Three modules are imported:Â <code>ipywidgets</code>,Â <code>IPython.display</code>, andÂ <code>pandas</code>.Â <code>ipywidgets</code>Â contains interactive HTML widgets for Jupyter Notebooks and the IPython kernel.Â <code>IPython.display</code>Â provides classes for displaying various types of output like images, sound, displaying HTML, etc. Pandas is a powerful data manipulation library.<br>
å¯¼å…¥ä¸‰ä¸ªæ¨¡å—ï¼šÂ <code>ipywidgets</code>Â ã€Â <code>IPython.display</code>Â å’ŒÂ <code>pandas</code>Â ã€‚Â <code>ipywidgets</code>Â åŒ…å« Jupyter Notebooks å’Œ IPython å†…æ ¸çš„äº¤äº’å¼ HTML å°éƒ¨ä»¶ã€‚Â <code>IPython.display</code>Â æä¾›äº†ç”¨äºæ˜¾ç¤ºå„ç§ç±»å‹è¾“å‡ºçš„ç±»ï¼Œå¦‚å›¾åƒã€å£°éŸ³ã€æ˜¾ç¤º HTML ç­‰ã€‚Pandas æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ•°æ®æ“ä½œåº“ã€‚</p>
</li>
<li>
<p>The pandas library is used to read in the CSV fileÂ <em>responses.csv</em>, which contains the responses you want to test. This creates a Pandas DataFrame calledÂ <code>df</code>.<br>
pandas åº“ç”¨äºè¯»å– CSV æ–‡ä»¶response.csvï¼Œå…¶ä¸­åŒ…å«æ‚¨è¦æµ‹è¯•çš„å“åº”ã€‚è¿™å°†åˆ›å»ºä¸€ä¸ªåä¸ºÂ <code>df</code>Â çš„ Pandas DataFrameã€‚</p>
</li>
<li>
<p><code>df</code>Â is shuffled using theÂ <code>sample()</code>Â function withÂ <code>frac=1</code>, which means it uses all the rows. TheÂ <code>reset_index(drop=True)</code>Â is used to reset the indices to the standard 0, 1, 2, â€¦â€‹, n index.<br>
<code>df</code>Â ä½¿ç”¨Â <code>sample()</code>Â å‡½æ•°ä¸Â <code>frac=1</code>Â è¿›è¡Œæ··æ´—ï¼Œè¿™æ„å‘³ç€å®ƒä½¿ç”¨æ‰€æœ‰è¡Œã€‚Â <code>reset_index(drop=True)</code>Â ç”¨äºå°†ç´¢å¼•é‡ç½®ä¸ºæ ‡å‡† 0, 1, 2, â€¦â€‹, n ç´¢å¼•ã€‚</p>
</li>
<li>
<p>The script definesÂ <code>response_index</code>Â as 0. This is used to track which response from the dataframe the user is currently viewing.<br>
è¯¥è„šæœ¬å°†Â <code>response_index</code>Â å®šä¹‰ä¸º 0ã€‚è¿™ç”¨äºè·Ÿè¸ªç”¨æˆ·å½“å‰æ­£åœ¨æŸ¥çœ‹çš„æ•°æ®å¸§çš„å“åº”ã€‚</p>
</li>
<li>
<p>A new columnÂ <code>feedback</code>Â is added to the dataframeÂ <code>df</code>Â with the data type asÂ <code>str</code>Â or string.<br>
æ–°åˆ—Â <code>feedback</code>Â å°†æ·»åŠ åˆ°æ•°æ®æ¡†Â <code>df</code>Â ä¸­ï¼Œæ•°æ®ç±»å‹ä¸ºÂ <code>str</code>Â æˆ–å­—ç¬¦ä¸²ã€‚</p>
</li>
<li>
<p>Next, the script defines a functionÂ <code>on_button_clicked(b)</code>, which will execute whenever one of the two buttons in the interface is clicked.<br>
æ¥ä¸‹æ¥ï¼Œè¯¥è„šæœ¬å®šä¹‰ä¸€ä¸ªå‡½æ•°Â <code>on_button_clicked(b)</code>Â ï¼Œåªè¦å•å‡»ç•Œé¢ä¸­çš„ä¸¤ä¸ªæŒ‰é’®ä¹‹ä¸€ï¼Œè¯¥å‡½æ•°å°±ä¼šæ‰§è¡Œã€‚</p>
<ol>
<li>
<p>The function first checks theÂ <code>description</code>Â of the button clicked was the thumbs-up button (<code>\U0001F44D</code>;Â <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/thumbs-up_1f44d.png" alt="thumbs up 1f44d">), and setsÂ <code>user_feedback</code>Â as 1, or if it was the thumbs-down button (<code>\U0001F44E</code>Â <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/thumbs-down_1f44e.png" alt="thumbs down 1f44e">), it setsÂ <code>user_feedback</code>Â as 0.<br>
è¯¥å‡½æ•°é¦–å…ˆæ£€æŸ¥å•å‡»çš„æŒ‰é’®çš„Â <code>description</code>Â æ˜¯ç«–èµ·å¤§æ‹‡æŒ‡æŒ‰é’®ï¼ˆÂ <code>\U0001F44D</code>Â ;Â <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/thumbs-up_1f44d.png" alt="thumbs up 1f44d">Â ï¼‰ï¼Œå¹¶å°†Â <code>user_feedback</code>Â è®¾ç½®ä¸º1ï¼Œæˆ–è€…å¦‚æœæ˜¯æ‹‡æŒ‡å‘ä¸‹æŒ‰é’® (Â <code>\U0001F44E</code>Â <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/thumbs-down_1f44e.png" alt="thumbs down 1f44e">Â )ï¼Œåˆ™å°†Â <code>user_feedback</code>Â è®¾ç½®ä¸º 0ã€‚</p>
</li>
<li>
<p>Then it updates theÂ <code>feedback</code>Â column of the dataframe at the currentÂ <code>response_index</code>Â withÂ <code>user_feedback</code>.<br>
ç„¶åå®ƒç”¨Â <code>user_feedback</code>Â æ›´æ–°å½“å‰Â <code>response_index</code>Â å¤„æ•°æ®å¸§çš„Â <code>feedback</code>Â åˆ—ã€‚</p>
</li>
<li>
<p>After that, it incrementsÂ <code>response_index</code>Â to move to the next response.<br>
ä¹‹åï¼Œå®ƒä¼šé€’å¢Â <code>response_index</code>Â ä»¥ç§»è‡³ä¸‹ä¸€ä¸ªå“åº”ã€‚</p>
</li>
<li>
<p>IfÂ <code>response_index</code>Â is still less than the total number of responses (i.e., the length of the dataframe), it calls the functionÂ <code>update_response()</code>.<br>
å¦‚æœÂ <code>response_index</code>Â ä»ç„¶å°äºå“åº”æ€»æ•°ï¼ˆå³æ•°æ®å¸§çš„é•¿åº¦ï¼‰ï¼Œåˆ™è°ƒç”¨å‡½æ•°Â <code>update_response()</code>Â ã€‚</p>
</li>
<li>
<p>If there are no more responses, it saves the dataframe to a new CSV fileÂ <em>results.csv</em>, then prints a message, and also prints a summary of the results by variant, showing the count of feedback received and the average score (mean) for each variant.<br>
å¦‚æœæ²¡æœ‰æ›´å¤šå“åº”ï¼Œå®ƒå°†æ•°æ®å¸§ä¿å­˜åˆ°æ–°çš„ CSV æ–‡ä»¶ results.csvï¼Œç„¶åæ‰“å°ä¸€æ¡æ¶ˆæ¯ï¼Œå¹¶æŒ‰å˜ä½“æ‰“å°ç»“æœæ‘˜è¦ï¼Œæ˜¾ç¤ºæ”¶åˆ°çš„åé¦ˆè®¡æ•°å’Œå¹³å‡åˆ†æ•°ï¼ˆå¹³å‡å€¼ï¼‰æ¯ä¸ªå˜ä½“ã€‚</p>
</li>
</ol>
</li>
<li>
<p>The functionÂ <code>update_response()</code>Â fetches the next response from the dataframe, wraps it in paragraph HTML tags (if itâ€™s not null), updates theÂ <code>response</code>Â widget to display the new response, and updates theÂ <code>count_label</code>Â widget to reflect the current response number and total number of responses.<br>
å‡½æ•°Â <code>update_response()</code>Â ä»æ•°æ®å¸§ä¸­è·å–ä¸‹ä¸€ä¸ªå“åº”ï¼Œå°†å…¶åŒ…è£…åœ¨æ®µè½ HTML æ ‡è®°ä¸­ï¼ˆå¦‚æœå®ƒä¸ä¸ºç©ºï¼‰ï¼Œæ›´æ–°Â <code>response</code>Â å°éƒ¨ä»¶ä»¥æ˜¾ç¤ºæ–°å“åº”ï¼Œå¹¶æ›´æ–° &lt; b2&gt; å°éƒ¨ä»¶åæ˜ å½“å‰å“åº”æ•°å’Œå“åº”æ€»æ•°ã€‚</p>
</li>
<li>
<p>Two widgets,Â <code>response</code>Â (an HTML widget) andÂ <code>count_label</code>Â (a Label widget), are instantiated. TheÂ <code>update_response()</code>Â function is then called to initialize these widgets with the first response and the appropriate label.<br>
ä¸¤ä¸ªå°éƒ¨ä»¶Â <code>response</code>Â ï¼ˆHTML å°éƒ¨ä»¶ï¼‰å’ŒÂ <code>count_label</code>Â ï¼ˆLabel å°éƒ¨ä»¶ï¼‰è¢«å®ä¾‹åŒ–ã€‚ç„¶åè°ƒç”¨Â <code>update_response()</code>Â å‡½æ•°ä»¥ä½¿ç”¨ç¬¬ä¸€ä¸ªå“åº”å’Œé€‚å½“çš„æ ‡ç­¾æ¥åˆå§‹åŒ–è¿™äº›å°éƒ¨ä»¶ã€‚</p>
</li>
<li>
<p>Two more widgets,Â <code>thumbs_up_button</code>Â andÂ <code>thumbs_down_button</code>Â (both Button widgets), are created with thumbs-up and thumbs-down emoji as their descriptions, respectively. Both buttons are configured to call theÂ <code>on_button_clicked()</code>Â function when clicked.<br>
å¦å¤–ä¸¤ä¸ªå°éƒ¨ä»¶Â <code>thumbs_up_button</code>Â å’ŒÂ <code>thumbs_down_button</code>Â ï¼ˆéƒ½æ˜¯æŒ‰é’®å°éƒ¨ä»¶ï¼‰æ˜¯åˆ†åˆ«ä½¿ç”¨æ‹‡æŒ‡å‘ä¸Šå’Œæ‹‡æŒ‡å‘ä¸‹è¡¨æƒ…ç¬¦å·ä½œä¸ºå…¶æè¿°æ¥åˆ›å»ºçš„ã€‚è¿™ä¸¤ä¸ªæŒ‰é’®éƒ½é…ç½®ä¸ºåœ¨å•å‡»æ—¶è°ƒç”¨Â <code>on_button_clicked()</code>Â å‡½æ•°ã€‚</p>
</li>
<li>
<p>The two buttons are grouped into a horizontal box (<code>button_box</code>) using theÂ <code>HBox</code>Â function.<br>
ä½¿ç”¨Â <code>HBox</code>Â å‡½æ•°å°†ä¸¤ä¸ªæŒ‰é’®åˆ†ç»„åˆ°ä¸€ä¸ªæ°´å¹³æ¡† (Â <code>button_box</code>Â ) ä¸­ã€‚</p>
</li>
<li>
<p>Finally, theÂ <code>response</code>,Â <code>button_box</code>, andÂ <code>count_label</code>Â widgets are displayed to the user using theÂ <code>display()</code>Â function from theÂ <code>IPython.display</code>Â module.<br>
æœ€åï¼Œä½¿ç”¨Â <code>IPython.display</code>Â ã€Â <code>button_box</code>Â å’ŒÂ <code>count_label</code>Â å°éƒ¨ä»¶ã€‚ b4&gt; æ¨¡å—ã€‚</p>
</li>
</ol>
<p>A simple rating system such as this one can be useful in judging prompt quality and encountering edge cases. Usually in less than 10 test runs of a prompt you uncover a deviation, which you otherwise wouldnâ€™t have caught until you started using it in production. The downside is that it can get tedious rating lots of responses manually, and your ratings might not represent the preferences of your intended audience. However, even small numbers of tests can reveal large differences between two prompting strategies and reveal nonobvious issues before reaching production.<br>
åƒè¿™æ ·çš„ç®€å•è¯„çº§ç³»ç»Ÿå¯ç”¨äºåˆ¤æ–­å³æ—¶è´¨é‡å’Œé‡åˆ°è¾¹ç¼˜æƒ…å†µã€‚é€šå¸¸ï¼Œåœ¨æç¤ºçš„ä¸åˆ° 10 æ¬¡æµ‹è¯•è¿è¡Œä¸­ï¼Œæ‚¨å°±ä¼šå‘ç°ä¸€ä¸ªåå·®ï¼Œå¦åˆ™æ‚¨å°†æ— æ³•å‘ç°è¯¥åå·®ï¼Œç›´åˆ°æ‚¨å¼€å§‹åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨å®ƒä¸ºæ­¢ã€‚ç¼ºç‚¹æ˜¯ï¼Œå®ƒå¯èƒ½ä¼šæ‰‹åŠ¨å¯¹å¤§é‡å›å¤è¿›è¡Œç¹ççš„è¯„çº§ï¼Œå¹¶ä¸”æ‚¨çš„è¯„çº§å¯èƒ½ä¸ä»£è¡¨ç›®æ ‡å—ä¼—çš„åå¥½ã€‚ç„¶è€Œï¼Œå³ä½¿å°‘é‡çš„æµ‹è¯•ä¹Ÿå¯ä»¥æ­ç¤ºä¸¤ç§æç¤ºç­–ç•¥ä¹‹é—´çš„å·¨å¤§å·®å¼‚ï¼Œå¹¶åœ¨æŠ•å…¥ç”Ÿäº§ä¹‹å‰æ­ç¤ºä¸æ˜æ˜¾çš„é—®é¢˜ã€‚</p>
<p>Iterating on and testing prompts can lead to radical decreases in the length of the prompt and therefore the cost and latency of your system. If you can find another prompt that performs equally as well (or better) but uses a shorter prompt, you can afford to scale up your operation considerably. Often youâ€™ll find in this process that many elements of a complex prompt are completely superfluous, or evenÂ counterproductive.<br>
è¿­ä»£å’Œæµ‹è¯•æç¤ºå¯ä»¥å¤§å¤§ç¼©çŸ­æç¤ºçš„é•¿åº¦ï¼Œä»è€Œé™ä½ç³»ç»Ÿçš„æˆæœ¬å’Œå»¶è¿Ÿã€‚å¦‚æœæ‚¨èƒ½æ‰¾åˆ°å¦ä¸€ä¸ªæ€§èƒ½åŒæ ·å¥½ï¼ˆæˆ–æ›´å¥½ï¼‰ä½†ä½¿ç”¨æ›´çŸ­æç¤ºçš„æç¤ºï¼Œæ‚¨å°±å¯ä»¥å¤§å¹…æ‰©å±•æ‚¨çš„æ“ä½œã€‚é€šå¸¸ï¼Œæ‚¨ä¼šå‘ç°åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œå¤æ‚æç¤ºçš„è®¸å¤šå…ƒç´ å®Œå…¨æ˜¯å¤šä½™çš„ï¼Œç”šè‡³é€‚å¾—å…¶åã€‚</p>
<p>TheÂ <em>thumbs-up</em>Â or other manually labeled indicators of quality donâ€™t have to be the only judging criteria. Human evaluation is generally considered to be the most accurate form of feedback. However, it can be tedious and costly to rate many samples manually. In many cases, as in math or classification use cases, it may be possible toÂ establishÂ <em>ground truth</em>Â (reference answers to test cases) to programmatically rate the results, allowing you to scale up considerably your testing and monitoring efforts. The following is not an exhaustive list because there are many motivations for evaluating yourÂ prompt programmatically:<br>
ç«–èµ·å¤§æ‹‡æŒ‡æˆ–å…¶ä»–æ‰‹åŠ¨æ ‡è®°çš„è´¨é‡æŒ‡æ ‡ä¸ä¸€å®šæ˜¯å”¯ä¸€çš„è¯„åˆ¤æ ‡å‡†ã€‚äººç±»è¯„ä¼°é€šå¸¸è¢«è®¤ä¸ºæ˜¯æœ€å‡†ç¡®çš„åé¦ˆå½¢å¼ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨å¯¹è®¸å¤šæ ·æœ¬è¿›è¡Œè¯„çº§å¯èƒ½æ˜¯ä¹å‘³ä¸”æ˜‚è´µçš„ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå¦‚åœ¨æ•°å­¦æˆ–åˆ†ç±»ç”¨ä¾‹ä¸­ï¼Œå¯ä»¥å»ºç«‹åŸºæœ¬äº‹å®ï¼ˆæµ‹è¯•ç”¨ä¾‹çš„å‚è€ƒç­”æ¡ˆï¼‰ä»¥ç¼–ç¨‹æ–¹å¼å¯¹ç»“æœè¿›è¡Œè¯„çº§ï¼Œä»è€Œå…è®¸æ‚¨å¤§å¹…æ‰©å±•æµ‹è¯•å’Œç›‘æ§å·¥ä½œã€‚ä»¥ä¸‹å¹¶ä¸æ˜¯è¯¦å°½çš„åˆ—è¡¨ï¼Œå› ä¸ºä»¥ç¼–ç¨‹æ–¹å¼è¯„ä¼°æç¤ºçš„åŠ¨æœºæœ‰å¾ˆå¤šï¼š</p>
<p>CostÂ æˆæœ¬</p>
<p>Prompts that use a lot of tokens, or work only with more expensive models, might be impractical for production use.<br>
ä½¿ç”¨å¤§é‡ä»¤ç‰Œæˆ–ä»…é€‚ç”¨äºæ›´æ˜‚è´µçš„æ¨¡å‹çš„æç¤ºå¯¹äºç”Ÿäº§ç”¨é€”å¯èƒ½ä¸åˆ‡å®é™…ã€‚</p>
<p>LatencyÂ æ½œä¼</p>
<p>Equally the more tokens there are, or the larger the model required, the longer it takes to complete a task, which can harm user experience.<br>
åŒæ ·ï¼Œä»£å¸è¶Šå¤šï¼Œæˆ–è€…æ‰€éœ€çš„æ¨¡å‹è¶Šå¤§ï¼Œå®Œæˆä»»åŠ¡æ‰€éœ€çš„æ—¶é—´å°±è¶Šé•¿ï¼Œè¿™å¯èƒ½ä¼šæŸå®³ç”¨æˆ·ä½“éªŒã€‚</p>
<p>CallsÂ é€šè¯</p>
<p>Many AI systems require multiple calls in a loop to complete a task, which can seriously slow down the process.<br>
è®¸å¤šäººå·¥æ™ºèƒ½ç³»ç»Ÿéœ€è¦å¾ªç¯å¤šæ¬¡è°ƒç”¨æ‰èƒ½å®Œæˆä»»åŠ¡ï¼Œè¿™ä¼šä¸¥é‡å‡æ…¢è¿›ç¨‹ã€‚</p>
<p>PerformanceÂ è¡¨ç°</p>
<p>Implement some form of external feedback system, for example a physics engine or other model for predicting real-world results.<br>
å®æ–½æŸç§å½¢å¼çš„å¤–éƒ¨åé¦ˆç³»ç»Ÿï¼Œä¾‹å¦‚ç‰©ç†å¼•æ“æˆ–å…¶ä»–ç”¨äºé¢„æµ‹ç°å®ä¸–ç•Œç»“æœçš„æ¨¡å‹ã€‚</p>
<p>ClassificationÂ åˆ†ç±»</p>
<p>Determine how often a prompt correctly labels given text, using another AI model or rules-based labeling.<br>
ä½¿ç”¨å…¶ä»– AI æ¨¡å‹æˆ–åŸºäºè§„åˆ™çš„æ ‡ç­¾ç¡®å®šæç¤ºæ­£ç¡®æ ‡è®°ç»™å®šæ–‡æœ¬çš„é¢‘ç‡ã€‚</p>
<p>ReasoningÂ æ¨ç†</p>
<p>Work out which instances the AI fails to apply logical reasoning or gets the math wrong versus reference cases.<br>
ä¸å‚è€ƒæ¡ˆä¾‹ç›¸æ¯”ï¼Œæ‰¾å‡ºäººå·¥æ™ºèƒ½æœªèƒ½åº”ç”¨é€»è¾‘æ¨ç†æˆ–æ•°å­¦é”™è¯¯çš„å®ä¾‹ã€‚</p>
<p>HallucinationsÂ å¹»è§‰</p>
<p>See how frequently you encouner hallucinations, as measured by invention of new terms not included in the promptâ€™s context.<br>
çœ‹çœ‹æ‚¨é‡åˆ°å¹»è§‰çš„é¢‘ç‡ï¼Œé€šè¿‡å‘æ˜æœªåŒ…å«åœ¨æç¤ºä¸Šä¸‹æ–‡ä¸­çš„æ–°æœ¯è¯­æ¥è¡¡é‡ã€‚</p>
<p>SafetyÂ å®‰å…¨</p>
<p>Flag any scenarios where the system might return unsafe or undesirable results using a safety filter or detection system.<br>
ä½¿ç”¨å®‰å…¨è¿‡æ»¤å™¨æˆ–æ£€æµ‹ç³»ç»Ÿæ ‡è®°ç³»ç»Ÿå¯èƒ½è¿”å›ä¸å®‰å…¨æˆ–ä¸è‰¯ç»“æœçš„ä»»ä½•åœºæ™¯ã€‚</p>
<p>RefusalsÂ æ‹’ç»</p>
<p>Find out how often the system incorrectly refuses to fulfill a reasonable user request by flagging known refusal language.<br>
é€šè¿‡æ ‡è®°å·²çŸ¥çš„æ‹’ç»è¯­è¨€ï¼Œäº†è§£ç³»ç»Ÿé”™è¯¯åœ°æ‹’ç»æ»¡è¶³åˆç†ç”¨æˆ·è¯·æ±‚çš„é¢‘ç‡ã€‚</p>
<p>AdversarialÂ å¯¹æŠ—æ€§çš„</p>
<p>Make the prompt robust against knownÂ <a href="https://oreil.ly/KGAqe">prompt injection</a>Â attacks that can get the model to run undesirable prompts instead of what you programmed.<br>
ä½¿æç¤ºèƒ½å¤ŸæŠµå¾¡å·²çŸ¥çš„æç¤ºæ³¨å…¥æ”»å‡»ï¼Œè¿™äº›æ”»å‡»å¯ä»¥ä½¿æ¨¡å‹è¿è¡Œä¸éœ€è¦çš„æç¤ºè€Œä¸æ˜¯æ‚¨ç¼–ç¨‹çš„æç¤ºã€‚</p>
<p>SimilarityÂ ç›¸ä¼¼</p>
<p>Use shared words and phrases (<a href="https://oreil.ly/iEGZ9">BLEU or ROGUE</a>) or vector distance (explained inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch05.html#vector_databases_05">ChapterÂ 5</a>) to measure similarity between generated and reference text.<br>
ä½¿ç”¨å…±äº«å•è¯å’ŒçŸ­è¯­ï¼ˆBLEU æˆ– ROGUEï¼‰æˆ–çŸ¢é‡è·ç¦»ï¼ˆç¬¬ 5 ç« ä¸­è¯´æ˜ï¼‰æ¥è¡¡é‡ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚</p>
<p>Once you start rating which examples were good, you can more easily update the examples used in your prompt as a way to continuously make your system smarter over time. The data from this feedback can also feed into examples for fine-tuning, which starts to beat prompt engineering once you canÂ <a href="https://oreil.ly/DZ-br">supply a few thousand examples</a>, as shown inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-13">FigureÂ 1-13</a>.<br>
ä¸€æ—¦æ‚¨å¼€å§‹è¯„ä¼°å“ªäº›ç¤ºä¾‹ä¸é”™ï¼Œæ‚¨å°±å¯ä»¥æ›´è½»æ¾åœ°æ›´æ–°æç¤ºä¸­ä½¿ç”¨çš„ç¤ºä¾‹ï¼Œä»è€Œéšç€æ—¶é—´çš„æ¨ç§»ä¸æ–­ä½¿æ‚¨çš„ç³»ç»Ÿå˜å¾—æ›´åŠ æ™ºèƒ½ã€‚æ¥è‡ªæ­¤åé¦ˆçš„æ•°æ®è¿˜å¯ä»¥è¾“å…¥åˆ°ç¤ºä¾‹ä¸­è¿›è¡Œå¾®è°ƒï¼Œä¸€æ—¦æ‚¨å¯ä»¥æä¾›å‡ åƒä¸ªç¤ºä¾‹ï¼Œå¾®è°ƒå°±å¼€å§‹èƒœè¿‡å³æ—¶å·¥ç¨‹ï¼Œå¦‚å›¾ 1-13 æ‰€ç¤ºã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0113.png" alt="pega 0113"></p>
<h6 id="figure-1-13how-many-data-points-is-a-prompt-worth">Figure 1-13.Â How many data points is a prompt worth?</h6>
<p>å›¾ 1-13ã€‚ä¸€ä¸ªæç¤ºå€¼å¤šå°‘ä¸ªæ•°æ®ç‚¹ï¼Ÿ</p>
<p>Graduating from thumbs-up or thumbs-down, you can implement a 3-, 5-, or 10-point rating system to get more fine-grained feedback on the quality of your prompts. Itâ€™s also possible to determine aggregate relative performance through comparing responses side by side, rather than looking at responses one at a time. From this you can construct a fair across-model comparison using anÂ <em><a href="https://oreil.ly/TlldE">Elo rating</a></em>, as is popular in chess and used in theÂ <a href="https://oreil.ly/P2IcU">Chatbot Arena</a>Â byÂ <em>lmsys.org</em>.<br>
ä»èµæˆæˆ–åå¯¹æ¯•ä¸šï¼Œæ‚¨å¯ä»¥å®æ–½ 3 åˆ†ã€5 åˆ†æˆ– 10 åˆ†è¯„çº§ç³»ç»Ÿï¼Œä»¥è·å¾—æœ‰å…³æç¤ºè´¨é‡çš„æ›´ç»†ç²’åº¦çš„åé¦ˆã€‚è¿˜å¯ä»¥é€šè¿‡å¹¶æ’æ¯”è¾ƒå“åº”æ¥ç¡®å®šæ€»ä½“ç›¸å¯¹æ€§èƒ½ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æŸ¥çœ‹ä¸€ä¸ªå“åº”ã€‚ç”±æ­¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Elo è¯„çº§æ„å»ºå…¬å¹³çš„è·¨æ¨¡å‹æ¯”è¾ƒï¼Œè¿™åœ¨å›½é™…è±¡æ£‹ä¸­å¾ˆæµè¡Œï¼Œå¹¶ç”± lmsys.org åœ¨ Chatbot Arena ä¸­ä½¿ç”¨ã€‚</p>
<p>For image generation, evaluation usuallyÂ takes the form ofÂ <em>permutation</em>Â prompting, where you input multiple directions or formats and generate an image for each combination. Images can than be scanned or later arranged in a grid to show the effect that different elements of the prompt can have on the final image.<br>
å¯¹äºå›¾åƒç”Ÿæˆï¼Œè¯„ä¼°é€šå¸¸é‡‡ç”¨æ’åˆ—æç¤ºçš„å½¢å¼ï¼Œæ‚¨è¾“å…¥å¤šä¸ªæ–¹å‘æˆ–æ ¼å¼ï¼Œå¹¶ä¸ºæ¯ä¸ªç»„åˆç”Ÿæˆå›¾åƒã€‚ç„¶åå¯ä»¥æ‰«æå›¾åƒæˆ–ç¨åå°†å›¾åƒæ’åˆ—åœ¨ç½‘æ ¼ä¸­ï¼Œä»¥æ˜¾ç¤ºæç¤ºçš„ä¸åŒå…ƒç´ å¯¹æœ€ç»ˆå›¾åƒçš„å½±å“ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>{stock photo, oil painting, illustration} of business
meeting of {four, eight} people watching on white MacBook on
top of glass-top table</p>
<p>In Midjourney this would be compiled into six different prompts, one for every combination of the three formats (stock photo, oil painting, illustration) and two numbers of people (four, eight).<br>
åœ¨ã€Šä¸­é€”æ—…ç¨‹ã€‹ä¸­ï¼Œè¿™å°†è¢«ç¼–è¯‘æˆå…­ç§ä¸åŒçš„æç¤ºï¼Œä¸€ç§å¯¹åº”ä¸‰ç§æ ¼å¼ï¼ˆåº“å­˜ç…§ç‰‡ã€æ²¹ç”»ã€æ’å›¾ï¼‰å’Œä¸¤ç§äººæ•°ï¼ˆå››äººã€å…«äººï¼‰çš„æ¯ä¸€ç§ç»„åˆã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<ol>
<li>
<p>stock photo of business meeting of four people watching
on white MacBook on top of glass-top table</p>
</li>
<li>
<p>stock photo of business meeting of eight people watching
on white MacBook on top of glass-top table</p>
</li>
<li>
<p>oil painting of business meeting of four people watching
on white MacBook on top of glass-top table</p>
</li>
<li>
<p>oil painting of business meeting of eight people watching
on white MacBook on top of glass-top table</p>
</li>
<li>
<p>illustration of business meeting of four people watching
on white MacBook on top of glass-top table</p>
</li>
<li>
<p>illustration of business meeting of eight people watching
on white MacBook on top of glass-top table</p>
</li>
</ol>
<p>Each prompt generates its own four images as usual, which makes the output a little harder to see. We have selected one from each prompt to upscale and then put them together in a grid, shown asÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-14">FigureÂ 1-14</a>. Youâ€™ll notice that the model doesnâ€™t always get the correct number of people (generative AI models are surprisingly bad at math), but it has correctly inferred the general intention by adding more people to the photos on the right than the left.<br>
æ¯ä¸ªæç¤ºéƒ½ä¼šåƒå¾€å¸¸ä¸€æ ·ç”Ÿæˆè‡ªå·±çš„å››ä¸ªå›¾åƒï¼Œè¿™ä½¿å¾—è¾“å‡ºæœ‰ç‚¹éš¾ä»¥æŸ¥çœ‹ã€‚æˆ‘ä»¬ä»æ¯ä¸ªæç¤ºä¸­é€‰æ‹©ä¸€ä¸ªè¿›è¡Œå‡çº§ï¼Œç„¶åå°†å®ƒä»¬æ”¾åœ¨ä¸€ä¸ªç½‘æ ¼ä¸­ï¼Œå¦‚å›¾ 1-14 æ‰€ç¤ºã€‚ä½ ä¼šæ³¨æ„åˆ°ï¼Œè¯¥æ¨¡å‹å¹¶ä¸æ€»æ˜¯èƒ½å¾—åˆ°æ­£ç¡®çš„äººæ•°ï¼ˆç”Ÿæˆå¼ AI æ¨¡å‹çš„æ•°å­¦å‡ºå¥‡åœ°ç³Ÿç³•ï¼‰ï¼Œä½†å®ƒé€šè¿‡åœ¨å³ä¾§ç…§ç‰‡ä¸­æ·»åŠ æ¯”å·¦ä¾§æ›´å¤šçš„äººæ¥æ­£ç¡®æ¨æ–­å‡ºæ€»ä½“æ„å›¾ã€‚</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-14">FigureÂ 1-14</a>Â shows the output.<br>
å›¾ 1-14 æ˜¾ç¤ºäº†è¾“å‡ºã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0114.png" alt="pega 0114"></p>
<h6 id="figure-1-14prompt-permutations-grid">Figure 1-14.Â Prompt permutations grid</h6>
<p>å›¾ 1-14ã€‚æç¤ºæ’åˆ—ç½‘æ ¼</p>
<p>With models that have APIs like Stable Diffusion, youÂ can more easily manipulate the photos and display them in a grid format for easy scanning. You can also manipulate the random seed of the image to fix a style in place for maximum reproducibility. With image classifiers it may also be possible to programmatically rate images based on their safe content, or if they contain certain elements associatedÂ with success or failure.<br>
å€ŸåŠ©å…·æœ‰ç¨³å®šæ‰©æ•£ç­‰ API çš„æ¨¡å‹ï¼Œæ‚¨å¯ä»¥æ›´è½»æ¾åœ°æ“ä½œç…§ç‰‡å¹¶ä»¥ç½‘æ ¼æ ¼å¼æ˜¾ç¤ºå®ƒä»¬ï¼Œä»¥ä¾¿äºæ‰«æã€‚æ‚¨è¿˜å¯ä»¥æ“çºµå›¾åƒçš„éšæœºç§å­æ¥å›ºå®šæ ·å¼ï¼Œä»¥è·å¾—æœ€å¤§çš„å¯é‡å¤æ€§ã€‚ä½¿ç”¨å›¾åƒåˆ†ç±»å™¨ï¼Œè¿˜å¯ä»¥æ ¹æ®å›¾åƒçš„å®‰å…¨å†…å®¹ï¼Œæˆ–è€…å›¾åƒæ˜¯å¦åŒ…å«ä¸æˆåŠŸæˆ–å¤±è´¥ç›¸å…³çš„æŸäº›å…ƒç´ ï¼Œä»¥ç¼–ç¨‹æ–¹å¼å¯¹å›¾åƒè¿›è¡Œè¯„çº§ã€‚</p>
<h1 id="5-divide-labor5-åˆ†å·¥">5. Divide LaborÂ 5. åˆ†å·¥</h1>
<p>As you build out your prompt, you startÂ to get to the point where youâ€™re asking a lot in a single call to the AI. When prompts get longer and more convoluted, you may find the responses get less deterministic, and hallucinations or anomalies increase. Even if you manage to arrive at a reliable prompt for your task, that task is likely just one of a number of interrelated tasks you need to do your job. Itâ€™s natural to start exploring how many other of these tasks could be done by AI and how you might string them together.<br>
å½“ä½ æ„å»ºæç¤ºæ—¶ï¼Œä½ å¼€å§‹åœ¨ä¸€æ¬¡å¯¹äººå·¥æ™ºèƒ½çš„è°ƒç”¨ä¸­æå‡ºå¾ˆå¤šé—®é¢˜ã€‚å½“æç¤ºå˜å¾—æ›´é•¿ã€æ›´å¤æ‚æ—¶ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°å“åº”çš„ç¡®å®šæ€§é™ä½ï¼Œå¹¶ä¸”å¹»è§‰æˆ–å¼‚å¸¸ç°è±¡ä¼šå¢åŠ ã€‚å³ä½¿æ‚¨è®¾æ³•ä¸ºæ‚¨çš„ä»»åŠ¡æ‰¾åˆ°å¯é çš„æç¤ºï¼Œè¯¥ä»»åŠ¡ä¹Ÿå¯èƒ½åªæ˜¯æ‚¨å®Œæˆå·¥ä½œæ‰€éœ€çš„ä¼—å¤šç›¸äº’å…³è”çš„ä»»åŠ¡ä¹‹ä¸€ã€‚æˆ‘ä»¬å¾ˆè‡ªç„¶åœ°ä¼šå¼€å§‹æ¢ç´¢äººå·¥æ™ºèƒ½å¯ä»¥å®Œæˆå¤šå°‘å…¶ä»–ä»»åŠ¡ä»¥åŠå¦‚ä½•å°†å®ƒä»¬ä¸²è”èµ·æ¥ã€‚</p>
<p>One of the core principles of engineering is toÂ use task decomposition to break problems down into their component parts, so you can more easily solve each individual problem and then reaggregate the results. Breaking your AI work into multiple calls that are chained together can help you accomplish more complex tasks, as well as provide more visibility into what part of the chain is failing.<br>
å·¥ç¨‹çš„æ ¸å¿ƒåŸåˆ™ä¹‹ä¸€æ˜¯ä½¿ç”¨ä»»åŠ¡åˆ†è§£å°†é—®é¢˜åˆ†è§£ä¸ºå„ä¸ªç»„æˆéƒ¨åˆ†ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥æ›´è½»æ¾åœ°è§£å†³æ¯ä¸ªå•ç‹¬çš„é—®é¢˜ï¼Œç„¶åé‡æ–°èšåˆç»“æœã€‚å°†æ‚¨çš„ AI å·¥ä½œåˆ†è§£ä¸ºå¤šä¸ªé“¾æ¥åœ¨ä¸€èµ·çš„è°ƒç”¨å¯ä»¥å¸®åŠ©æ‚¨å®Œæˆæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶æ›´æ¸…æ¥šåœ°äº†è§£è¯¥é“¾çš„å“ªä¸ªéƒ¨åˆ†å‘ç”Ÿäº†æ•…éšœã€‚</p>
<p>There are lots of factors that go into product naming, and an important task is naively outsourced to the AI with no visibility into how itâ€™s weighing the importance of these factors (if at all). The way our current system works, weâ€™re getting a list of names, but all names are displayed with equal importance with no further context for helping us decide. Fortunately AI tools are capable of self-evaluation; if we add a second step to our task, we can automatically check for nondesirable outputs.<br>
äº§å“å‘½åæ¶‰åŠå¾ˆå¤šå› ç´ ï¼Œè€Œä¸€é¡¹é‡è¦ä»»åŠ¡è¢«å¤©çœŸåœ°å¤–åŒ…ç»™äººå·¥æ™ºèƒ½ï¼Œè€Œæ— æ³•äº†è§£å®ƒå¦‚ä½•æƒè¡¡è¿™äº›å› ç´ çš„é‡è¦æ€§ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ã€‚æŒ‰ç…§æˆ‘ä»¬å½“å‰ç³»ç»Ÿçš„å·¥ä½œæ–¹å¼ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªåç§°åˆ—è¡¨ï¼Œä½†æ‰€æœ‰åç§°éƒ½ä»¥åŒç­‰é‡è¦æ€§æ˜¾ç¤ºï¼Œæ²¡æœ‰è¿›ä¸€æ­¥çš„ä¸Šä¸‹æ–‡æ¥å¸®åŠ©æˆ‘ä»¬åšå‡ºå†³å®šã€‚å¹¸è¿çš„æ˜¯ï¼Œäººå·¥æ™ºèƒ½å·¥å…·èƒ½å¤Ÿè‡ªæˆ‘è¯„ä¼°ï¼›å¦‚æœæˆ‘ä»¬åœ¨ä»»åŠ¡ä¸­æ·»åŠ ç¬¬äºŒæ­¥ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªåŠ¨æ£€æŸ¥ä¸éœ€è¦çš„è¾“å‡ºã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Rate this list of product names for a pair of shoes that can
fit any foot size. The rating should be out of 10, inline
next to the product name:</p>
<p>UniFit SoleStrides
FlexiSize All-Fit Shoes
OmniFit FootFlex
&hellip;
AnyStep AdaptiWalkers
FlexiRange UniversalSteps
PerfectGlide MultiFit Shoes</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>UniFit SoleStrides: 7/10
FlexiSize All-Fit Shoes: 8/10
OmniFit FootFlex: 7/10
&hellip;
AnyStep AdaptiWalkers: 7/10
FlexiRange UniversalSteps: 7.5/10
PerfectGlide MultiFit Shoes: 9/10</p>
<p>This ranking is based on clarity, memorability, and how well
the name communicates the unique selling point of the
product (that they can fit any foot size). Remember, these
ratings are subjective and the ultimate success of a product
name also depends on the target market, branding, and
marketing efforts.</p>
<p>In running this multiple times, it consistently rates the name â€œOneSize Glovewalkersâ€ as the worst, providing context (if you ask) that the concept might be confusing in a shoe context. You may be wondering why, if the modelÂ <em>knows</em>Â this is a bad name, does it suggest it in the first place? LLMs work by predicting the next token in a sequence and therefore struggle to know what the overall response will be when finished. However, when it has all the tokens from a previous response to review, it can more easily predict whether this would be labeled as a good or bad response.<br>
åœ¨å¤šæ¬¡è¿è¡Œæ­¤è¿‡ç¨‹ä¸­ï¼Œå®ƒå§‹ç»ˆå°†â€œOneSize Glovewalkersâ€è¿™ä¸ªåç§°è¯„ä¸ºæœ€å·®çš„ï¼Œæä¾›äº†ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæ‚¨é—®çš„è¯ï¼‰ï¼Œè¯¥æ¦‚å¿µåœ¨é‹å­ä¸Šä¸‹æ–‡ä¸­å¯èƒ½ä¼šä»¤äººå›°æƒ‘ã€‚æ‚¨å¯èƒ½æƒ³çŸ¥é“ï¼Œå¦‚æœæ¨¡å‹çŸ¥é“è¿™æ˜¯ä¸€ä¸ªååå­—ï¼Œä¸ºä»€ä¹ˆå®ƒé¦–å…ˆä¼šå»ºè®®å®ƒï¼Ÿ LLMs é€šè¿‡é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°æ¥å·¥ä½œï¼Œå› æ­¤å¾ˆéš¾çŸ¥é“å®Œæˆåçš„æ€»ä½“å“åº”æ˜¯ä»€ä¹ˆã€‚ç„¶è€Œï¼Œå½“å®ƒæ‹¥æœ‰ä¹‹å‰å“åº”çš„æ‰€æœ‰æ ‡è®°è¿›è¡Œå®¡æŸ¥æ—¶ï¼Œå®ƒå¯ä»¥æ›´è½»æ¾åœ°é¢„æµ‹è¿™æ˜¯å¦ä¼šè¢«æ ‡è®°ä¸ºå¥½å“åº”æˆ–åå“åº”ã€‚</p>
<p>We can continue to chain multiple calls together to improve the results of our task. For example, we could split this into three separate ratings: clarity, memorability, and how well the name communicates the unique selling point of the product. These ratings could then be given to a human as additional context on the final decision, or even calculated together to select the final name programmatically.<br>
æˆ‘ä»¬å¯ä»¥ç»§ç»­å°†å¤šä¸ªè°ƒç”¨é“¾æ¥åœ¨ä¸€èµ·ä»¥æ”¹è¿›æˆ‘ä»¬çš„ä»»åŠ¡ç»“æœã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶åˆ†ä¸ºä¸‰ä¸ªå•ç‹¬çš„è¯„çº§ï¼šæ¸…æ™°åº¦ã€æ˜“è®°æ€§ä»¥åŠåç§°ä¼ è¾¾äº§å“ç‹¬ç‰¹å–ç‚¹çš„ç¨‹åº¦ã€‚ç„¶åå¯ä»¥å°†è¿™äº›è¯„çº§ä½œä¸ºæœ€ç»ˆå†³ç­–çš„é™„åŠ èƒŒæ™¯æä¾›ç»™äººç±»ï¼Œç”šè‡³å¯ä»¥ä¸€èµ·è®¡ç®—ä»¥é€šè¿‡ç¼–ç¨‹æ–¹å¼é€‰æ‹©æœ€ç»ˆåç§°ã€‚</p>
<p>The real unlock in learning to work professionally with AI versus just playing around with prompting is realizing that every part of the system can be broken down into a series of iterative steps. Even with a single prompt this principles applies, as simply appendingÂ <code>Let's think step by step</code>Â to the prompt can lead to demonstrable gains in reasoning and proficiency, as well as provide an audit trail for quality assurance and debugging. When taking the time and tokens to reason, the ratings change and are more consistent with the scoring criteria.<br>
å­¦ä¹ ä¸“ä¸šåœ°ä½¿ç”¨äººå·¥æ™ºèƒ½å·¥ä½œè€Œä¸åªæ˜¯ç©å¼„æç¤ºï¼ŒçœŸæ­£çš„è§£é”æ˜¯è®¤è¯†åˆ°ç³»ç»Ÿçš„æ¯ä¸ªéƒ¨åˆ†éƒ½å¯ä»¥åˆ†è§£ä¸ºä¸€ç³»åˆ—è¿­ä»£æ­¥éª¤ã€‚å³ä½¿åªæœ‰ä¸€ä¸ªæç¤ºï¼Œè¯¥åŸåˆ™ä¹Ÿé€‚ç”¨ï¼Œå› ä¸ºåªéœ€å°†Â <code>Let's think step by step</code>Â é™„åŠ åˆ°æç¤ºå³å¯åœ¨æ¨ç†å’Œç†Ÿç»ƒç¨‹åº¦æ–¹é¢å¸¦æ¥æ˜æ˜¾çš„æ”¶ç›Šï¼Œå¹¶ä¸ºè´¨é‡ä¿è¯å’Œè°ƒè¯•æä¾›å®¡æ ¸è·Ÿè¸ªã€‚å½“èŠ±æ—¶é—´å’Œæ ‡è®°è¿›è¡Œæ¨ç†æ—¶ï¼Œè¯„çº§ä¼šå‘ç”Ÿå˜åŒ–å¹¶ä¸”ä¸è¯„åˆ†æ ‡å‡†æ›´åŠ ä¸€è‡´ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Let&rsquo;s think step by step. Rate this list of product names
for a pair of shoes that can fit any foot size. The rating
should be out of 10, inline next to the product name:</p>
<p>UniFit SoleStrides
FlexiSize All-Fit Shoes
OmniFit FootFlex
&hellip;
AnyStep AdaptiWalkers
FlexiRange UniversalSteps
PerfectGlide MultiFit Shoes</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Here&rsquo;s a rating out of 10 for each product name, considering
factors like descriptiveness, catchiness, and overall
appeal:</p>
<ol>
<li><strong>UniFit SoleStrides</strong>: 7/10 - Sounds unifying but may
not immediately convey the &ldquo;fit any size&rdquo; idea.</li>
<li><strong>FlexiSize All-Fit Shoes</strong>: 8.5/10 - Combines
flexibility and the notion that it fits all sizes.</li>
<li><strong>OmniFit FootFlex</strong>: 8/10 - Omnipresence (fitting all)
and flexibility nicely conveyed.
&hellip;</li>
<li><strong>AnyStep AdaptiWalkers</strong>: 7.5/10 - &ldquo;AnyStep&rdquo; suggests
versatility, but the overall name might be less direct.</li>
<li><strong>FlexiRange UniversalSteps</strong>: 8/10 - Combines
flexibility and universal fit.</li>
<li><strong>PerfectGlide MultiFit Shoes</strong>: 8.5/10 - &ldquo;MultiFit&rdquo; is
clear, and &ldquo;PerfectGlide&rdquo; suggests a comfortable experience.</li>
</ol>
<p>Remember, the best name can also be influenced by your
target audience, branding strategy, and the overall message
you want to convey.</p>
<p>OpenAIÂ <a href="https://oreil.ly/0MZ3-">calls this</a>Â â€œgiving the model time to think,â€ and it is a key tenet of prompt engineering. In effect,Â <em>chain of thought</em>Â techniques likeÂ this, where the model is encouraged to list out its steps, are like dividing a task within the same prompt. Once weâ€™ve automated product naming given a product idea, we can call ChatGPT again to describe each product, which in turn can be fed into Midjourney to generate an image of each product. Using an AI model to generate a prompt for an AI model isÂ <em>meta prompting</em>, and it works because LLMs are human-level prompt engineers (<a href="https://oreil.ly/Dwszu">Zhou, 2022</a>).<br>
OpenAI ç§°ä¹‹ä¸ºâ€œç»™æ¨¡å‹æ—¶é—´æ€è€ƒâ€ï¼Œè¿™æ˜¯å³æ—¶å·¥ç¨‹çš„å…³é”®åŸåˆ™ã€‚å®é™…ä¸Šï¼Œåƒè¿™æ ·çš„æ€ç»´é“¾æŠ€æœ¯é¼“åŠ±æ¨¡å‹åˆ—å‡ºå…¶æ­¥éª¤ï¼Œå°±åƒåœ¨åŒä¸€æç¤ºä¸­åˆ’åˆ†ä»»åŠ¡ä¸€æ ·ã€‚ä¸€æ—¦æˆ‘ä»¬æ ¹æ®äº§å“æƒ³æ³•è‡ªåŠ¨å‘½åäº§å“ï¼Œæˆ‘ä»¬å°±å¯ä»¥å†æ¬¡è°ƒç”¨ ChatGPT æ¥æè¿°æ¯ä¸ªäº§å“ï¼Œç„¶åå°†å…¶è¾“å…¥ Midjourney ä¸­ä»¥ç”Ÿæˆæ¯ä¸ªäº§å“çš„å›¾åƒã€‚ä½¿ç”¨ AI æ¨¡å‹ä¸º AI æ¨¡å‹ç”Ÿæˆæç¤ºæ˜¯å…ƒæç¤ºï¼Œå®ƒä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸º LLMs æ˜¯äººç±»çº§åˆ«çš„æç¤ºå·¥ç¨‹å¸ˆï¼ˆZhouï¼Œ2022ï¼‰ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Describe the new product &ldquo;OneFit UltraStride Shoes,&rdquo; a shoe
that can fit any foot size, in great detail:</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Introducing the revolutionary &ldquo;OneFit UltraStride Shoes,&rdquo;
the footwear innovation designed to fit any foot size with
unparalleled comfort and style. These shoes are a result of
cutting-edge technology and meticulous craftsmanship,
ensuring a truly personalized and enjoyable walking or
running experience for everyone.</p>
<p>Design and Materials:
The OneFit UltraStride Shoes boast an elegant and modern
design that appeals to both fashion-forward individuals and
sports enthusiasts. They feature a sleek and streamlined
silhouette, crafted using premium materials for durability
and aesthetic appeal. The shoe&rsquo;s upper is constructed from a
combination of breathable mesh and flexible synthetic
materials, allowing for excellent airflow and a lightweight
feel.</p>
<p>&hellip;</p>
<p>Color and Customization:
OneFit UltraStride Shoes are available in a variety of
vibrant colors and stylish patterns, catering to diverse
personal preferences. You can choose from classic
monochromatic options for a timeless look or opt for bold
and vibrant hues to make a fashion statement. The ability to
customize the shoe&rsquo;s appearance allows individuals to
express their unique style while enjoying the benefits of
the OneFit technology.</p>
<p>Conclusion:
The OneFit UltraStride Shoes represent a breakthrough in
footwear technology, eliminating the hassle of finding the
perfect fit for your feet. With their adaptive design,
superior comfort, and stylish aesthetics, these shoes offer
a truly personalized experience for individuals of all foot
sizes. Stay comfortable, confident, and on-trend with the
OneFit UltraStride Shoes, the ultimate choice for those
seeking unmatched versatility and a perfect fit.</p>
<p>DALL-E is well-known by GPT-4, and thereforeÂ you can invoke its name within ChatGPT and it does a reasonable job at crafting a prompt for an image generation tool. If you were planning on using this prompt in production, you may consider applying the prompting principle of providing examples, but it does a good enough job for our purposes without examples.<br>
DALL-E åœ¨ GPT-4 ä¸­å¹¿ä¸ºäººçŸ¥ï¼Œå› æ­¤æ‚¨å¯ä»¥åœ¨ ChatGPT ä¸­è°ƒç”¨å®ƒçš„åç§°ï¼Œå¹¶ä¸”å®ƒåœ¨ä¸ºå›¾åƒç”Ÿæˆå·¥å…·åˆ¶ä½œæç¤ºæ–¹é¢åšå¾—ç›¸å½“ä¸é”™ã€‚å¦‚æœæ‚¨è®¡åˆ’åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨æ­¤æç¤ºï¼Œæ‚¨å¯ä»¥è€ƒè™‘åº”ç”¨æä¾›ç¤ºä¾‹çš„æç¤ºåŸåˆ™ï¼Œä½†å¯¹äºæˆ‘ä»¬çš„ç›®çš„è€Œè¨€ï¼Œæ— éœ€ç¤ºä¾‹å®ƒå°±å·²ç»è¶³å¤Ÿå¥½äº†ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Convert this description into a short prompt for an AI image
generation model such as DALL-E:</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Create an eye-catching image that visually represents the
&ldquo;OneFit UltraStride Shoes,&rdquo; a revolutionary footwear product
that adapts to any foot size. Showcase a sleek and modern
shoe design incorporating a stretchable upper, an adjustable
lacing system, and a cushioned insole for optimal comfort.
Emphasize the shoe&rsquo;s versatility by displaying it in action
on various surfaces, highlighting its excellent traction.
Incorporate vibrant colors and patterns to convey the
customizable nature of the product. The image should capture
the essence of innovation, style, and personalized fit that
the OneFit UltraStride Shoes offer.</p>
<p>The output of this prompt can now be plugged into image generation tools like DALL-E or Midjourney as a prompt, which can give you a good starting point for visualizing what the product might look like. Although this might not be the final design you go with, seeing an image is more evocative and helps people form an opinion faster. Itâ€™s easier cognitively to criticize or compliment an existing image than it is to imagine a new image from a blank page or section of text.<br>
ç°åœ¨å¯ä»¥å°†æ­¤æç¤ºçš„è¾“å‡ºä½œä¸ºæç¤ºæ’å…¥åˆ° DALL-E æˆ– Midjourney ç­‰å›¾åƒç”Ÿæˆå·¥å…·ä¸­ï¼Œè¿™å¯ä»¥ä¸ºæ‚¨æä¾›ä¸€ä¸ªè‰¯å¥½çš„èµ·ç‚¹æ¥å¯è§†åŒ–äº§å“çš„å¤–è§‚ã€‚å°½ç®¡è¿™å¯èƒ½ä¸æ˜¯æ‚¨æœ€ç»ˆé‡‡ç”¨çš„è®¾è®¡ï¼Œä½†çœ‹åˆ°å›¾åƒæ›´èƒ½å”¤èµ·äººä»¬çš„å›å¿†ï¼Œå¹¶å¸®åŠ©äººä»¬æ›´å¿«åœ°å½¢æˆæ„è§ã€‚ä»è®¤çŸ¥ä¸Šæ¥è¯´ï¼Œæ‰¹è¯„æˆ–èµç¾ç°æœ‰å›¾åƒæ¯”ä»ç©ºç™½é¡µé¢æˆ–æ–‡æœ¬éƒ¨åˆ†æƒ³è±¡æ–°å›¾åƒæ›´å®¹æ˜“ã€‚</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#figure-1-15">FigureÂ 1-15</a>Â shows the output.<br>
å›¾ 1-15 æ˜¾ç¤ºäº†è¾“å‡ºã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0115.png" alt="pega 0115"></p>
<h6 id="figure-1-15onefit-ultrastride-shoes">Figure 1-15.Â OneFit UltraStride shoes</h6>
<p>å›¾ 1-15ã€‚ OneFit UltraStride é‹</p>
<p>Itâ€™s common practice when working withÂ AI professionally to chain multiple calls to AI together, and even multiple models, to accomplish more complex goals. Even single-prompt applications are often built dynamically, based on outside context queried from various databases or other calls to an AI model. The libraryÂ <a href="https://www.langchain.com/">LangChain</a>Â has developed tooling for chaining multiple prompt templates and queries together, making this process more observable and well structured. A foundational example is progressive summarization, where text that is too large to fit into a context window can be split into multiple chunks of text, with each being summarized, before finally summarizing the summaries. If you talk to builders of early AI products, youâ€™ll find theyâ€™re all under the hood chaining multiple prompts together, calledÂ <em>AI chaining</em>, to accomplish better results in the final output.<br>
åœ¨ä¸“ä¸šåœ°ä½¿ç”¨äººå·¥æ™ºèƒ½æ—¶ï¼Œé€šå¸¸çš„åšæ³•æ˜¯å°†å¯¹äººå·¥æ™ºèƒ½çš„å¤šä¸ªè°ƒç”¨é“¾æ¥åœ¨ä¸€èµ·ï¼Œç”šè‡³å¤šä¸ªæ¨¡å‹ï¼Œä»¥å®ç°æ›´å¤æ‚çš„ç›®æ ‡ã€‚å³ä½¿å•æç¤ºåº”ç”¨ç¨‹åºä¹Ÿé€šå¸¸æ˜¯åŸºäºä»å„ç§æ•°æ®åº“æŸ¥è¯¢çš„å¤–éƒ¨ä¸Šä¸‹æ–‡æˆ–å¯¹äººå·¥æ™ºèƒ½æ¨¡å‹çš„å…¶ä»–è°ƒç”¨åŠ¨æ€æ„å»ºçš„ã€‚ LangChain åº“å¼€å‘äº†ç”¨äºå°†å¤šä¸ªæç¤ºæ¨¡æ¿å’ŒæŸ¥è¯¢é“¾æ¥åœ¨ä¸€èµ·çš„å·¥å…·ï¼Œä½¿è¯¥è¿‡ç¨‹æ›´åŠ å¯è§‚å¯Ÿä¸”ç»“æ„è‰¯å¥½ã€‚ä¸€ä¸ªåŸºæœ¬çš„ä¾‹å­æ˜¯æ¸è¿›å¼æ‘˜è¦ï¼Œå…¶ä¸­å¤ªå¤§è€Œæ— æ³•æ”¾å…¥ä¸Šä¸‹æ–‡çª—å£çš„æ–‡æœ¬å¯ä»¥è¢«åˆ†æˆå¤šä¸ªæ–‡æœ¬å—ï¼Œæ¯ä¸ªæ–‡æœ¬å—éƒ½è¢«æ€»ç»“ï¼Œç„¶åæœ€åæ€»ç»“æ‘˜è¦ã€‚å¦‚æœä½ ä¸æ—©æœŸäººå·¥æ™ºèƒ½äº§å“çš„æ„å»ºè€…äº¤è°ˆï¼Œä½ ä¼šå‘ç°ä»–ä»¬éƒ½åœ¨å¹•åå°†å¤šä¸ªæç¤ºé“¾æ¥åœ¨ä¸€èµ·ï¼Œç§°ä¸ºäººå·¥æ™ºèƒ½é“¾æ¥ï¼Œä»¥åœ¨æœ€ç»ˆè¾“å‡ºä¸­å®ç°æ›´å¥½çš„ç»“æœã€‚</p>
<p>TheÂ <a href="https://oreil.ly/tPPW9">Reason and Act (ReAct)</a>Â framework was one of the first popular attempts at AI agents, including the open source projectsÂ <a href="https://oreil.ly/TEiQx">BabyAGI</a>,Â <a href="https://oreil.ly/48lq6">AgentGPT</a>Â andÂ <a href="https://oreil.ly/KG5Xl">Microsoft AutoGen</a>. In effect, these agents are the result of chaining multiple AI calls together in order to plan, observe, act, and then evaluate the results of the action. Autonomous agents will be covered inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch06.html#autonomous_agents_06">ChapterÂ 6</a>Â but are still not widely used in production at the time of writing. This practice of self-reasoning agents is still early and prone to errors, but there are promising signs this approach can be useful in achieving complex tasks, and is likely to be part of the next stage in evolution for AI systems.<br>
Reason and Act (ReAct) æ¡†æ¶æ˜¯äººå·¥æ™ºèƒ½ä»£ç†çš„æœ€æ—©æµè¡Œå°è¯•ä¹‹ä¸€ï¼ŒåŒ…æ‹¬å¼€æºé¡¹ç›® BabyAGIã€AgentGPT å’Œ Microsoft AutoGenã€‚å®é™…ä¸Šï¼Œè¿™äº›ä»£ç†æ˜¯å°†å¤šä¸ªäººå·¥æ™ºèƒ½è°ƒç”¨é“¾æ¥åœ¨ä¸€èµ·çš„ç»“æœï¼Œä»¥ä¾¿è®¡åˆ’ã€è§‚å¯Ÿã€è¡ŒåŠ¨ï¼Œç„¶åè¯„ä¼°è¡ŒåŠ¨çš„ç»“æœã€‚è‡ªä¸»ä»£ç†å°†åœ¨ç¬¬ 6 ç« ä¸­ä»‹ç»ï¼Œä½†åœ¨æ’°å†™æœ¬æ–‡æ—¶ä»æœªåœ¨ç”Ÿäº§ä¸­å¹¿æ³›ä½¿ç”¨ã€‚è¿™ç§è‡ªæˆ‘æ¨ç†ä»£ç†çš„å®è·µè¿˜å¤„äºæ—©æœŸé˜¶æ®µï¼Œå¹¶ä¸”å®¹æ˜“å‡ºé”™ï¼Œä½†æœ‰è¿¹è±¡è¡¨æ˜è¿™ç§æ–¹æ³•å¯ç”¨äºå®Œæˆå¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶ä¸”å¾ˆå¯èƒ½æˆä¸ºäººå·¥æ™ºèƒ½ç³»ç»Ÿä¸‹ä¸€é˜¶æ®µè¿›åŒ–çš„ä¸€éƒ¨åˆ†ã€‚</p>
<p>There is an AI battle occurring between large tech firms like Microsoft and Google, as well as a wide array of open source projects on Hugging Face, and venture-funded start-ups like OpenAI and Anthropic. As new models continue to proliferate, theyâ€™re diversifying in order to compete for different segments of the growing market. For example, Anthropicâ€™s Claude 2 had anÂ <a href="https://oreil.ly/NQcFW">100,000-token context window</a>, compared to GPT-4â€™s standardÂ <a href="https://oreil.ly/iZhMl">8,192 tokens</a>. OpenAI soon responded with aÂ <a href="https://oreil.ly/3TTZ9">128,000-token window version of GPT-4</a>, and Google touts a 1 million token context length withÂ <a href="https://oreil.ly/cyhR4">Gemini 1.5</a>. For comparison, one of the Harry Potter books would be around 185,000 tokens, so it may become common for an entire book to fit inside a single prompt, though processing millions of tokens with each API call may be cost prohibitive for most use cases.<br>
å¾®è½¯å’Œè°·æ­Œç­‰å¤§å‹ç§‘æŠ€å…¬å¸ã€Hugging Face ä¸Šçš„å„ç§å¼€æºé¡¹ç›®ä»¥åŠ OpenAI å’Œ Anthropic ç­‰é£é™©æŠ•èµ„åˆåˆ›å…¬å¸ä¹‹é—´æ­£åœ¨å±•å¼€ä¸€åœºäººå·¥æ™ºèƒ½ä¹‹æˆ˜ã€‚éšç€æ–°è½¦å‹ä¸æ–­æ¶Œç°ï¼Œå®ƒä»¬æ­£åœ¨èµ°å‘å¤šå…ƒåŒ–ï¼Œä»¥äº‰å¤ºä¸æ–­å¢é•¿çš„å¸‚åœºçš„ä¸åŒç»†åˆ†å¸‚åœºã€‚ä¾‹å¦‚ï¼ŒAnthropic çš„ Claude 2 å…·æœ‰ 100,000 ä¸ªä»¤ç‰Œä¸Šä¸‹æ–‡çª—å£ï¼Œè€Œ GPT-4 çš„æ ‡å‡†æœ‰ 8,192 ä¸ªä»¤ç‰Œã€‚ OpenAI å¾ˆå¿«å°±æ¨å‡ºäº† 128,000 ä¸ªä»¤ç‰Œçª—å£ç‰ˆæœ¬çš„ GPT-4ï¼Œè€Œ Google åˆ™å®£ç§° Gemini 1.5 å…·æœ‰ 100 ä¸‡ä¸ªä»¤ç‰Œä¸Šä¸‹æ–‡é•¿åº¦ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸€æœ¬ã€Šå“ˆåˆ©Â·æ³¢ç‰¹ã€‹ä¹¦ç±å¤§çº¦æœ‰ 185,000 ä¸ªä»¤ç‰Œï¼Œå› æ­¤å°†æ•´æœ¬ä¹¦æ”¾å…¥ä¸€ä¸ªæç¤ºä¸­å¯èƒ½ä¼šå˜å¾—å¾ˆå¸¸è§ï¼Œå°½ç®¡å¯¹äºå¤§å¤šæ•°ç”¨ä¾‹æ¥è¯´ï¼Œæ¯æ¬¡ API è°ƒç”¨å¤„ç†æ•°ç™¾ä¸‡ä¸ªä»¤ç‰Œå¯èƒ½æˆæœ¬è¿‡é«˜ã€‚</p>
<p>This book focuses on GPT-4 for text generation techniques, as well as Midjourney v6 and Stable Diffusion XL for image generation techniques, but within months these models may no longer be state of the art. This means it will become increasingly important to be able to select the right model for the job and chain multiple AI systems together. Prompt templates are rarely comparable when transferring to a new model, but the effect of the Five Prompting Principles will consistently improve anyÂ prompt you use, for any model, getting you more reliable results.<br>
æœ¬ä¹¦é‡ç‚¹ä»‹ç»ç”¨äºæ–‡æœ¬ç”ŸæˆæŠ€æœ¯çš„ GPT-4ï¼Œä»¥åŠç”¨äºå›¾åƒç”ŸæˆæŠ€æœ¯çš„ Midjourney v6 å’Œ Stable Diffusion XLï¼Œä½†å‡ ä¸ªæœˆå†…è¿™äº›æ¨¡å‹å¯èƒ½ä¸å†æ˜¯æœ€å…ˆè¿›çš„ã€‚è¿™æ„å‘³ç€èƒ½å¤Ÿä¸ºå·¥ä½œé€‰æ‹©æ­£ç¡®çš„æ¨¡å‹å¹¶å°†å¤šä¸ªäººå·¥æ™ºèƒ½ç³»ç»Ÿé“¾æ¥åœ¨ä¸€èµ·å°†å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚è½¬ç§»åˆ°æ–°æ¨¡å‹æ—¶ï¼Œæç¤ºæ¨¡æ¿å¾ˆå°‘å…·æœ‰å¯æ¯”æ€§ï¼Œä½†æ˜¯äº”é¡¹æç¤ºåŸåˆ™çš„æ•ˆæœå°†æŒç»­æ”¹è¿›æ‚¨ä½¿ç”¨çš„ä»»ä½•æ¨¡å‹çš„ä»»ä½•æç¤ºï¼Œä¸ºæ‚¨æä¾›æ›´å¯é çš„ç»“æœã€‚</p>
<h1 id="summaryæ¦‚æ‹¬">SummaryÂ æ¦‚æ‹¬</h1>
<p>In this chapter, you learned about the importance of prompt engineering in the context of generative AI. We defined prompt engineering as the process of developing effective prompts that yield desired results when interacting with AI models. You discovered that providing clear direction, formatting the output, incorporating examples, establishing an evaluation system, and dividing complex tasks into smaller prompts are key principles of prompt engineering. By applying these principles and using common prompting techniques, you can improve the quality and reliability of AI-generated outputs.<br>
åœ¨æœ¬ç« ä¸­ï¼Œæ‚¨äº†è§£äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½èƒŒæ™¯ä¸‹å³æ—¶å·¥ç¨‹çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å°†æç¤ºå·¥ç¨‹å®šä¹‰ä¸ºå¼€å‘æœ‰æ•ˆæç¤ºçš„è¿‡ç¨‹ï¼Œåœ¨ä¸äººå·¥æ™ºèƒ½æ¨¡å‹äº¤äº’æ—¶äº§ç”ŸæœŸæœ›çš„ç»“æœã€‚æ‚¨å‘ç°ï¼Œæä¾›æ˜ç¡®çš„æ–¹å‘ã€æ ¼å¼åŒ–è¾“å‡ºã€åˆå¹¶ç¤ºä¾‹ã€å»ºç«‹è¯„ä¼°ç³»ç»Ÿä»¥åŠå°†å¤æ‚çš„ä»»åŠ¡åˆ’åˆ†ä¸ºæ›´å°çš„æç¤ºæ˜¯æç¤ºå·¥ç¨‹çš„å…³é”®åŸåˆ™ã€‚é€šè¿‡åº”ç”¨è¿™äº›åŸåˆ™å¹¶ä½¿ç”¨å¸¸è§çš„æç¤ºæŠ€æœ¯ï¼Œæ‚¨å¯ä»¥æé«˜ AI ç”Ÿæˆçš„è¾“å‡ºçš„è´¨é‡å’Œå¯é æ€§ã€‚</p>
<p>You also explored the role of prompt engineering in generating product names and images. You saw how specifying the desired format and providing instructive examples can greatly influence the AIâ€™s output. Additionally, you learned about the concept of role-playing, where you can ask the AI to generate outputs as if it were a famous person like Steve Jobs. The chapter emphasized the need for clear direction and context to achieve desired outcomes when using generative AI models. Furthermore, you discovered the importance of evaluating the performance of AI models and the various methods used for measuring results, as well as the trade-offs between quality and token usage, cost, and latency.<br>
æ‚¨è¿˜æ¢è®¨äº†æç¤ºå·¥ç¨‹åœ¨ç”Ÿæˆäº§å“åç§°å’Œå›¾åƒä¸­çš„ä½œç”¨ã€‚æ‚¨çœ‹åˆ°äº†æŒ‡å®šæ‰€éœ€çš„æ ¼å¼å¹¶æä¾›æŒ‡å¯¼æ€§ç¤ºä¾‹å¦‚ä½•æå¤§åœ°å½±å“äººå·¥æ™ºèƒ½çš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œæ‚¨è¿˜äº†è§£äº†è§’è‰²æ‰®æ¼”çš„æ¦‚å¿µï¼Œæ‚¨å¯ä»¥è¦æ±‚äººå·¥æ™ºèƒ½åƒå²è’‚å¤«Â·ä¹”å¸ƒæ–¯è¿™æ ·çš„åäººä¸€æ ·ç”Ÿæˆè¾“å‡ºã€‚æœ¬ç« å¼ºè°ƒåœ¨ä½¿ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹æ—¶éœ€è¦æ˜ç¡®çš„æ–¹å‘å’ŒèƒŒæ™¯æ‰èƒ½å®ç°é¢„æœŸç»“æœã€‚æ­¤å¤–ï¼Œæ‚¨è¿˜å‘ç°äº†è¯„ä¼° AI æ¨¡å‹æ€§èƒ½å’Œç”¨äºæµ‹é‡ç»“æœçš„å„ç§æ–¹æ³•çš„é‡è¦æ€§ï¼Œä»¥åŠè´¨é‡å’Œä»¤ç‰Œä½¿ç”¨ã€æˆæœ¬å’Œå»¶è¿Ÿä¹‹é—´çš„æƒè¡¡ã€‚</p>
<p>In the next chapter, you will be introduced to text generation models. You will learn about the different types of foundation models and their capabilities, as well as their limitations. The chapter will also review the standard OpenAI offerings, as well as competitors and open source alternatives. By the end of the chapter, you will have a solid understanding of the history of text generation models and their relative strengths and weaknesses. This book will return to image generation prompting in ChaptersÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch07.html#intro_image_07">7</a>,Â <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch08.html#standard_image_08">8</a>, andÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch09.html#advanced_image_09">9</a>, so you should feel free to skip ahead if that is your immediate need. Get ready to dive deeper into the discipline of prompt engineering and expand your comfort working with AI.<br>
åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæ‚¨å°†äº†è§£æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚æ‚¨å°†äº†è§£ä¸åŒç±»å‹çš„åŸºç¡€æ¨¡å‹åŠå…¶åŠŸèƒ½ä»¥åŠå±€é™æ€§ã€‚æœ¬ç« è¿˜å°†å›é¡¾æ ‡å‡† OpenAI äº§å“ä»¥åŠç«äº‰å¯¹æ‰‹å’Œå¼€æºæ›¿ä»£å“ã€‚åœ¨æœ¬ç« ç»“æŸæ—¶ï¼Œæ‚¨å°†å¯¹æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„å†å²åŠå…¶ç›¸å¯¹ä¼˜åŠ¿å’ŒåŠ£åŠ¿æœ‰æ·±å…¥çš„äº†è§£ã€‚æœ¬ä¹¦å°†åœ¨ç¬¬ 7ã€8 å’Œ 9 ç« ä¸­è¿”å›åˆ°å›¾åƒç”Ÿæˆæç¤ºï¼Œå› æ­¤å¦‚æœæ‚¨è¿«åˆ‡éœ€è¦çš„è¯ï¼Œå¯ä»¥éšæ„è·³è¿‡ã€‚å‡†å¤‡å¥½æ·±å…¥ç ”ç©¶å³æ—¶å·¥ç¨‹å­¦ç§‘ï¼Œå¹¶æé«˜æ‚¨ä½¿ç”¨äººå·¥æ™ºèƒ½çš„èˆ’é€‚åº¦ã€‚</p>
<h1 id="2-introduction-to-large-language-models-for-text-generation"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch02.html">2. Introduction To Large Language Models For Text Generation</a></h1>
<h1 id="chapter-2introduction-to-large-language-modelsfor-text-generation">Chapter 2.Â Introduction to Large Language ModelsÂ for Text Generation</h1>
<p>ç¬¬ 2 ç« ã€‚ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ç®€ä»‹</p>
<p>In artificial intelligence, a recent focus has been the evolution of large language models. Unlike their less-flexible predecessors, LLMs are capable of handling and learning from a much larger volume of data, resulting in the emergent capability of producing text that closely resembles human language output. These models have generalized across diverse applications, from writing content to automating software development and enabling real-time interactive chatbot experiences.<br>
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œæœ€è¿‘çš„ä¸€ä¸ªç„¦ç‚¹æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¼”å˜ã€‚ä¸ä¸å¤ªçµæ´»çš„å‰è¾ˆä¸åŒï¼ŒLLMs èƒ½å¤Ÿå¤„ç†å’Œå­¦ä¹ å¤§é‡æ•°æ®ï¼Œä»è€Œäº§ç”Ÿä¸äººç±»è¯­è¨€è¾“å‡ºéå¸¸ç›¸ä¼¼çš„æ–‡æœ¬çš„ç´§æ€¥èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹å·²ç»æ¨å¹¿åˆ°å„ç§åº”ç”¨ç¨‹åºä¸­ï¼Œä»ç¼–å†™å†…å®¹åˆ°è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘ä»¥åŠå®ç°å®æ—¶äº¤äº’å¼èŠå¤©æœºå™¨äººä½“éªŒã€‚</p>
<h1 id="what-are-text-generation-models">What Are Text Generation Models?</h1>
<p>ä»€ä¹ˆæ˜¯æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Ÿ</p>
<p>Text generation models utilize advancedÂ algorithms to understand the meaning in text and produce outputs that are often indistinguishable from human work. If youâ€™ve ever interacted withÂ <a href="https://chat.openai.com/">ChatGPT</a>Â or marveled at its ability to craft coherent and contextually relevant sentences, youâ€™ve witnessed the power of an LLM in action.<br>
æ–‡æœ¬ç”Ÿæˆæ¨¡å‹åˆ©ç”¨å…ˆè¿›çš„ç®—æ³•æ¥ç†è§£æ–‡æœ¬ä¸­çš„å«ä¹‰ï¼Œå¹¶äº§ç”Ÿé€šå¸¸ä¸äººç±»å·¥ä½œæ— æ³•åŒºåˆ†çš„è¾“å‡ºã€‚å¦‚æœæ‚¨æ›¾ç»ä¸ ChatGPT äº’åŠ¨è¿‡ï¼Œæˆ–è€…æƒŠå¹äºå®ƒåˆ¶ä½œè¿è´¯ä¸”ä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„å¥å­çš„èƒ½åŠ›ï¼Œé‚£ä¹ˆæ‚¨å·²ç»ç›®ç¹äº† LLM åœ¨è¡ŒåŠ¨ä¸­çš„å¼ºå¤§åŠŸèƒ½ã€‚</p>
<p>In natural language processing (NLP) and LLMs, theÂ fundamental linguistic unit is aÂ <em>token</em>.Â <a href="https://oreil.ly/3fOsM">Tokens</a>Â can represent sentences, words, or even subwords such as a set of characters. A useful way to understand the size of text data is by looking at the number of tokens it comprises; for instance, a text of 100 tokens roughly equates to about 75 words. This comparison can be essential for managing the processing limits of LLMs as different models may have varying token capacities.<br>
åœ¨è‡ªç„¶è¯­è¨€å¤„ç† ï¼ˆNLPï¼‰ å’Œ LLMs ä¸­ï¼ŒåŸºæœ¬è¯­è¨€å•ä½æ˜¯æ ‡è®°ã€‚æ ‡è®°å¯ä»¥è¡¨ç¤ºå¥å­ã€å•è¯ï¼Œç”šè‡³æ˜¯å­è¯ï¼Œä¾‹å¦‚ä¸€ç»„å­—ç¬¦ã€‚äº†è§£æ–‡æœ¬æ•°æ®å¤§å°çš„ä¸€ä¸ªæœ‰ç”¨æ–¹æ³•æ˜¯æŸ¥çœ‹å®ƒåŒ…å«çš„æ ‡è®°æ•°é‡;ä¾‹å¦‚ï¼Œ100 ä¸ªæ ‡è®°çš„æ–‡æœ¬å¤§è‡´ç›¸å½“äºå¤§çº¦ 75 ä¸ªå•è¯ã€‚è¿™ç§æ¯”è¾ƒå¯¹äºç®¡ç† LLMs çš„å¤„ç†é™åˆ¶è‡³å…³é‡è¦ï¼Œå› ä¸ºä¸åŒçš„æ¨¡å‹å¯èƒ½å…·æœ‰ä¸åŒçš„ä»¤ç‰Œå®¹é‡ã€‚</p>
<p><em>Tokenization</em>, the process of breakingÂ down text into tokens, is a crucial step in preparing data for NLP tasks. SeveralÂ methods can be used for tokenization, includingÂ <a href="https://oreil.ly/iSOp7">Byte-Pair Encoding (BPE)</a>, WordPiece, and SentencePiece. Each of these methods has its unique advantages and is suited to particular use cases. BPE is commonly used due to its efficiency in handling a wide range of vocabulary while keeping the number of tokens manageable.<br>
æ ‡è®°åŒ–æ˜¯å°†æ–‡æœ¬åˆ†è§£ä¸ºæ ‡è®°çš„è¿‡ç¨‹ï¼Œæ˜¯ä¸º NLP ä»»åŠ¡å‡†å¤‡æ•°æ®çš„å…³é”®æ­¥éª¤ã€‚æœ‰å‡ ç§æ–¹æ³•å¯ç”¨äºæ ‡è®°åŒ–ï¼ŒåŒ…æ‹¬å­—èŠ‚å¯¹ç¼–ç  ï¼ˆBPEï¼‰ã€WordPiece å’Œ SentencePieceã€‚è¿™äº›æ–¹æ³•ä¸­çš„æ¯ä¸€ç§éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œé€‚ç”¨äºç‰¹å®šçš„ç”¨ä¾‹ã€‚BPE ä¹‹æ‰€ä»¥è¢«æ™®éä½¿ç”¨ï¼Œæ˜¯å› ä¸ºå®ƒå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å„ç§è¯æ±‡ï¼ŒåŒæ—¶ä¿æŒä»¤ç‰Œçš„æ•°é‡å¯ç®¡ç†ã€‚</p>
<p>BPE begins by viewing a text as a series of individual characters. Over time, it combines characters that frequently appear together into single units, or tokens. To understand this better, consider the wordÂ <em>apple</em>. Initially, BPE might see it asÂ <em>a</em>,Â <em>p</em>,Â <em>p</em>,Â <em>l</em>, andÂ <em>e</em>. But after noticing thatÂ <em>p</em>Â often comes afterÂ <em>a</em>Â and beforeÂ <em>l</em>Â in the dataset, it might combine them and treatÂ <em>appl</em>Â as a single token in future instances.<br>
BPE é¦–å…ˆå°†æ–‡æœ¬è§†ä¸ºä¸€ç³»åˆ—å•ä¸ªå­—ç¬¦ã€‚éšç€æ—¶é—´çš„æµé€ï¼Œå®ƒå°†ç»å¸¸ä¸€èµ·å‡ºç°çš„å­—ç¬¦ç»„åˆæˆå•ä¸ªå•ä½æˆ–æ ‡è®°ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸€ç‚¹ï¼Œè¯·è€ƒè™‘è‹¹æœè¿™ä¸ªè¯ã€‚æœ€åˆï¼ŒBPE å¯èƒ½å°†å…¶è§†ä¸º aã€pã€pã€l å’Œ eã€‚ä½†æ˜¯ï¼Œåœ¨æ³¨æ„åˆ° p é€šå¸¸ä½äºæ•°æ®é›†ä¸­ a ä¹‹åå’Œ l ä¹‹å‰ä¹‹åï¼Œå®ƒå¯èƒ½ä¼šå°†å®ƒä»¬ç»„åˆåœ¨ä¸€èµ·ï¼Œå¹¶åœ¨å°†æ¥çš„å®ä¾‹ä¸­å°† appl è§†ä¸ºå•ä¸ªæ ‡è®°ã€‚</p>
<p>This approach helps LLMs recognize and generate words or phrases, even if they werenâ€™t common in the training data, making the models more adaptable andÂ versatile.<br>
è¿™ç§æ–¹æ³•æœ‰åŠ©äº LLMs è¯†åˆ«å’Œç”Ÿæˆå•è¯æˆ–çŸ­è¯­ï¼Œå³ä½¿å®ƒä»¬åœ¨è®­ç»ƒæ•°æ®ä¸­å¹¶ä¸å¸¸è§ï¼Œä½¿æ¨¡å‹æ›´å…·é€‚åº”æ€§å’Œé€šç”¨æ€§ã€‚</p>
<p>Understanding the workings of LLMs requires a grasp of the underlying mathematical principles that power these systems. Although the computations can be complex, we can simplify the core elements to provide an intuitive understanding of how these models operate. Particularly within a business context, the accuracy and reliability of LLMs are paramount.<br>
è¦äº†è§£ LLMs çš„å·¥ä½œåŸç†ï¼Œéœ€è¦æŒæ¡ä¸ºè¿™äº›ç³»ç»Ÿæä¾›åŠ¨åŠ›çš„åŸºæœ¬æ•°å­¦åŸç†ã€‚å°½ç®¡è®¡ç®—å¯èƒ½å¾ˆå¤æ‚ï¼Œä½†æˆ‘ä»¬å¯ä»¥ç®€åŒ–æ ¸å¿ƒå…ƒç´ ï¼Œä»¥ä¾¿ç›´è§‚åœ°äº†è§£è¿™äº›æ¨¡å‹çš„è¿è¡Œæ–¹å¼ã€‚ç‰¹åˆ«æ˜¯åœ¨ä¸šåŠ¡ç¯å¢ƒä¸­ï¼ŒLLMs çš„å‡†ç¡®æ€§å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚</p>
<p>A significant part of achieving this reliability lies in the pretraining and fine-tuning phases of LLM development. Initially, models are trained on vast datasets during the pretraining phase, acquiring a broad understanding of language. Subsequently, in the fine-tuning phase, models are adapted for specific tasks, honing their capabilities to provide accurate and reliable outputs for specialized applications.<br>
å®ç°è¿™ç§å¯é æ€§çš„ä¸€ä¸ªé‡è¦éƒ¨åˆ†åœ¨äº LLM å¼€å‘çš„é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µã€‚æœ€åˆï¼Œæ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µåœ¨å¤§é‡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œè·å¾—å¯¹è¯­è¨€çš„å¹¿æ³›ç†è§£ã€‚éšåï¼Œåœ¨å¾®è°ƒé˜¶æ®µï¼Œæ¨¡å‹ä¼šé’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼Œç£¨ç»ƒå…¶èƒ½åŠ›ï¼Œä¸ºä¸“ä¸šåº”ç”¨æä¾›å‡†ç¡®å¯é çš„è¾“å‡ºã€‚</p>
<h2 id="vector-representations-the-numerical-essence-of-language">Vector Representations: The Numerical Essence of Language</h2>
<p>å‘é‡è¡¨ç¤ºï¼šè¯­è¨€çš„æ•°å­—æœ¬è´¨</p>
<p>In the realm of NLP, words arenâ€™t justÂ alphabetic symbols. They can be tokenized and then represented in a numerical form, known asÂ <em>vectors</em>. These vectors are multi-dimensional arrays of numbers that capture the semantic and syntactic relations:<br>
åœ¨NLPé¢†åŸŸï¼Œå•è¯ä¸ä»…ä»…æ˜¯å­—æ¯ç¬¦å·ã€‚å®ƒä»¬å¯ä»¥è¢«æ ‡è®°åŒ–ï¼Œç„¶åä»¥æ•°å­—å½¢å¼è¡¨ç¤ºï¼Œç§°ä¸ºå‘é‡ã€‚è¿™äº›å‘é‡æ˜¯æ•è·è¯­ä¹‰å’Œå¥æ³•å…³ç³»çš„å¤šç»´æ•°å­—æ•°ç»„ï¼š</p>
<p>ğ‘¤â†’ğ¯=[ğ‘£1,ğ‘£2,&hellip;,ğ‘£ğ‘›]</p>
<p>Creating word vectors, also known asÂ <em>word embeddings</em>, reliesÂ on intricate patterns within language. During an intensive training phase, models are designed to identify and learn these patterns, ensuring that words with similar meanings are mapped close to one another in a high-dimensional space (<a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch02.html#figure-2-1">FigureÂ 2-1</a>).<br>
åˆ›å»ºè¯å‘é‡ï¼ˆä¹Ÿç§°ä¸ºè¯åµŒå…¥ï¼‰ä¾èµ–äºè¯­è¨€ä¸­å¤æ‚çš„æ¨¡å¼ã€‚åœ¨å¼ºåŒ–è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹è¢«è®¾è®¡æ¥è¯†åˆ«å’Œå­¦ä¹ è¿™äº›æ¨¡å¼ï¼Œç¡®ä¿å…·æœ‰ç›¸ä¼¼å«ä¹‰çš„å•è¯åœ¨é«˜ç»´ç©ºé—´ä¸­å½¼æ­¤é è¿‘ï¼ˆå›¾ 2-1ï¼‰ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0201.png" alt="Word Embeddings"></p>
<h6 id="figure-2-1semantic-proximity-of-word-vectors-within-a-word-embedding-space">Figure 2-1.Â Semantic proximity of word vectors within a word embedding space</h6>
<p>å›¾ 2-1ã€‚è¯åµŒå…¥ç©ºé—´ä¸­è¯å‘é‡çš„è¯­ä¹‰æ¥è¿‘åº¦</p>
<p>The beauty of this approach is its ability to capture nuanced relationships between words and calculate their distance. When we examine word embeddings, it becomes evident that words with similar or related meanings likeÂ <em>virtue</em>Â andÂ <em>moral</em>Â orÂ <em>walked</em>Â andÂ <em>walking</em>Â are situated near each other. This spatial closeness in the embedding space becomes a powerful tool in various NLP tasks, enabling models to understand context, semantics, and the intricate web of relationships that form language.<br>
è¿™ç§æ–¹æ³•çš„ç¾å¦™ä¹‹å¤„åœ¨äºå®ƒèƒ½å¤Ÿæ•æ‰å•è¯ä¹‹é—´çš„ç»†å¾®å…³ç³»å¹¶è®¡ç®—å®ƒä»¬çš„è·ç¦»ã€‚å½“æˆ‘ä»¬æ£€æŸ¥å•è¯åµŒå…¥æ—¶ï¼Œå¾ˆæ˜æ˜¾ï¼Œå…·æœ‰ç›¸ä¼¼æˆ–ç›¸å…³å«ä¹‰çš„å•è¯ï¼Œå¦‚ç¾å¾·å’Œé“å¾·æˆ–æ­¥è¡Œå’Œè¡Œèµ°ï¼Œå½¼æ­¤é è¿‘ã€‚åµŒå…¥ç©ºé—´ä¸­çš„è¿™ç§ç©ºé—´ç´§å¯†æ€§æˆä¸ºå„ç§ NLP ä»»åŠ¡ä¸­çš„å¼ºå¤§å·¥å…·ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£ä¸Šä¸‹æ–‡ã€è¯­ä¹‰å’Œå½¢æˆè¯­è¨€çš„é”™ç»¼å¤æ‚çš„å…³ç³»ç½‘ç»œã€‚</p>
<h2 id="transformer-architecture-orchestrating-contextual-relationships">Transformer Architecture: Orchestrating Contextual Relationships</h2>
<p>Transformer æ¶æ„ï¼šç¼–æ’ä¸Šä¸‹æ–‡å…³ç³»</p>
<p>Before we go deep into the mechanics ofÂ transformer architectures, letâ€™s build a foundational understanding. In simple terms, when we have a sentence, say,Â <em>The cat sat on the mat</em>, each word in this sentence gets converted into its numerical vector representation. So,Â <em>cat</em>Â might become a series of numbers, as doesÂ <em>sat</em>,Â <em>on</em>, andÂ <em>mat</em>.<br>
åœ¨æˆ‘ä»¬æ·±å…¥ç ”ç©¶å˜å‹å™¨æ¶æ„çš„æœºåˆ¶ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆå»ºç«‹ä¸€ä¸ªåŸºæœ¬çš„ç†è§£ã€‚ç®€å•æ¥è¯´ï¼Œå½“æˆ‘ä»¬æœ‰ä¸€ä¸ªå¥å­æ—¶ï¼Œæ¯”å¦‚è¯´ï¼ŒçŒ«ååœ¨å«å­ä¸Šï¼Œè¿™å¥è¯ä¸­çš„æ¯ä¸ªå•è¯éƒ½ä¼šè¢«è½¬æ¢ä¸ºå…¶æ•°å­—å‘é‡è¡¨ç¤ºã€‚å› æ­¤ï¼Œcat å¯èƒ½ä¼šå˜æˆä¸€ç³»åˆ—æ•°å­—ï¼Œå°±åƒ satã€on å’Œ mat ä¸€æ ·ã€‚</p>
<p>As youâ€™ll explore in detail later in this chapter, the transformer architecture takes these word vectors and understands their relationshipsâ€”both in structure (syntax) and meaning (semantics). There are many types of transformers;Â <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch02.html#figure-2-2">FigureÂ 2-2</a>Â showcases both BERT and GPTâ€™s architecture. Additionally, a transformer doesnâ€™t just see words in isolation; it looks atÂ <em>cat</em>Â and knows itâ€™s related toÂ <em>sat</em>Â andÂ <em>mat</em>Â in a specific way in this sentence.<br>
æ­£å¦‚æ‚¨å°†åœ¨æœ¬ç« åé¢è¯¦ç»†æ¢è®¨çš„é‚£æ ·ï¼ŒTransformer æ¶æ„é‡‡ç”¨è¿™äº›è¯å‘é‡å¹¶ç†è§£å®ƒä»¬ä¹‹é—´çš„å…³ç³»â€”â€”åŒ…æ‹¬ç»“æ„ï¼ˆè¯­æ³•ï¼‰å’Œå«ä¹‰ï¼ˆè¯­ä¹‰ï¼‰ã€‚å˜å‹å™¨çš„ç§ç±»å¾ˆå¤š;å›¾ 2-2 å±•ç¤ºäº† BERT å’Œ GPT çš„æ¶æ„ã€‚æ­¤å¤–ï¼Œè½¬æ¢å™¨ä¸ä»…å­¤ç«‹åœ°çœ‹å¾…å•è¯;å®ƒçœ‹ç€çŒ«ï¼ŒçŸ¥é“å®ƒåœ¨è¿™å¥è¯ä¸­ä»¥ç‰¹å®šçš„æ–¹å¼ä¸ SAT å’Œ MAT æœ‰å…³ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0202.png" alt="BERT and GPT architecture"></p>
<h6 id="figure-2-2bert-uses-an-encoder-for-input-data-while-gpt-has-a-decoder-for-output">Figure 2-2.Â BERT uses an encoder for input data, while GPT has a decoder for output</h6>
<p>å›¾ 2-2ã€‚BERT ä½¿ç”¨ç¼–ç å™¨æ¥è¾“å…¥æ•°æ®ï¼Œè€Œ GPT ä½¿ç”¨è§£ç å™¨æ¥è¾“å‡º</p>
<p>When the transformer processes these vectors, it uses mathematical operations to understand the relationships between the words, thereby producing new vectors with rich, contextual information:<br>
å½“è½¬æ¢å™¨å¤„ç†è¿™äº›å‘é‡æ—¶ï¼Œå®ƒä½¿ç”¨æ•°å­¦è¿ç®—æ¥ç†è§£å•è¯ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œç”Ÿæˆå…·æœ‰ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ–°å‘é‡ï¼š</p>
<p>ğ¯ğ‘–&rsquo;=Transformer(ğ¯1,ğ¯2,&hellip;,ğ¯ğ‘š)</p>
<p>One of the remarkable features of transformers is their ability to comprehend the nuanced contextual meanings of words. TheÂ <a href="https://oreil.ly/xuovP">self-attention</a>Â mechanism in transformers lets each word in a sentence look at all other words to understand its context better. Think of it like each word casting votes on the importance of other words for its meaning. By considering the entire sentence, transformers can more accurately determine the role and meaning ofÂ each word, making theirÂ <em>interpretations more contextually rich.</em><br>
å˜å½¢é‡‘åˆšçš„ä¸€ä¸ªæ˜¾ç€ç‰¹ç‚¹æ˜¯å®ƒä»¬èƒ½å¤Ÿç†è§£å•è¯ç»†å¾®çš„ä¸Šä¸‹æ–‡å«ä¹‰ã€‚Transformer ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶è®©å¥å­ä¸­çš„æ¯ä¸ªå•è¯éƒ½æŸ¥çœ‹æ‰€æœ‰å…¶ä»–å•è¯ï¼Œä»¥æ›´å¥½åœ°ç†è§£å…¶ä¸Šä¸‹æ–‡ã€‚æŠŠå®ƒæƒ³è±¡æˆæ¯ä¸ªå•è¯éƒ½å¯¹å…¶ä»–å•è¯çš„å«ä¹‰çš„é‡è¦æ€§è¿›è¡ŒæŠ•ç¥¨ã€‚é€šè¿‡è€ƒè™‘æ•´ä¸ªå¥å­ï¼Œè½¬æ¢å™¨å¯ä»¥æ›´å‡†ç¡®åœ°ç¡®å®šæ¯ä¸ªå•è¯çš„ä½œç”¨å’Œå«ä¹‰ï¼Œä½¿ä»–ä»¬çš„è§£é‡Šæ›´åŠ ä¸Šä¸‹æ–‡ä¸°å¯Œã€‚</p>
<h2 id="probabilistic-text-generation-the-decision-mechanism">Probabilistic Text Generation: The Decision Mechanism</h2>
<p>æ¦‚ç‡æ–‡æœ¬ç”Ÿæˆï¼šå†³ç­–æœºåˆ¶</p>
<p>After the transformer understands theÂ context of the given text, it moves on to generating new text, guided by the concept of likelihood or probability. In mathematical terms, the model calculates how likely each possible next word is to follow the current sequence of words and picks the one that is most likely:<br>
åœ¨è½¬æ¢å™¨ç†è§£ç»™å®šæ–‡æœ¬çš„ä¸Šä¸‹æ–‡åï¼Œå®ƒä¼šåœ¨å¯èƒ½æ€§æˆ–æ¦‚ç‡æ¦‚å¿µçš„æŒ‡å¯¼ä¸‹ç»§ç»­ç”Ÿæˆæ–°æ–‡æœ¬ã€‚ç”¨æ•°å­¦æœ¯è¯­æ¥è¯´ï¼Œè¯¥æ¨¡å‹è®¡ç®—æ¯ä¸ªå¯èƒ½çš„ä¸‹ä¸€ä¸ªå•è¯éµå¾ªå½“å‰å•è¯åºåˆ—çš„å¯èƒ½æ€§ï¼Œå¹¶é€‰æ‹©æœ€æœ‰å¯èƒ½çš„å•è¯ï¼š</p>
<p>ğ‘¤next=argmaxğ‘ƒ(ğ‘¤|ğ‘¤1,ğ‘¤2,&hellip;,ğ‘¤ğ‘š)</p>
<p>By repeating this process, as shown inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch02.html#figure-2-3">FigureÂ 2-3</a>, the model generates a coherent and contextually relevant string of text as its output.<br>
é€šè¿‡é‡å¤æ­¤è¿‡ç¨‹ï¼Œå¦‚å›¾ 2-3 æ‰€ç¤ºï¼Œæ¨¡å‹ä¼šç”Ÿæˆä¸€ä¸ªè¿è´¯ä¸”ä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„æ–‡æœ¬å­—ç¬¦ä¸²ä½œä¸ºå…¶è¾“å‡ºã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0203.png" alt="An illustrative overview of how text is generated using transformer models like GPT-4."></p>
<h6 id="figure-2-3how-text-is-generated-using-transformer-models-such-as-gpt-4">Figure 2-3.Â How text is generated using transformer models such as GPT-4</h6>
<p>å›¾ 2-3ã€‚å¦‚ä½•ä½¿ç”¨ GPT-4 ç­‰è½¬æ¢å™¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬</p>
<p>The mechanisms driving LLMs are rooted inÂ vector mathematics, linear transformations, and probabilistic models. While the under-the-hood operations are computationally intensive, the core concepts are built on these mathematical principles, offering a foundational understanding that bridges the gap between technical complexity and businessÂ applicability.<br>
é©±åŠ¨ LLMs çš„æœºåˆ¶æ¤æ ¹äºå‘é‡æ•°å­¦ã€çº¿æ€§å˜æ¢å’Œæ¦‚ç‡æ¨¡å‹ã€‚è™½ç„¶åº•å±‚æ“ä½œæ˜¯è®¡ç®—å¯†é›†å‹çš„ï¼Œä½†æ ¸å¿ƒæ¦‚å¿µæ˜¯å»ºç«‹åœ¨è¿™äº›æ•°å­¦åŸç†ä¹‹ä¸Šçš„ï¼Œæä¾›äº†ä¸€ç§åŸºæœ¬çš„ç†è§£ï¼Œå¼¥åˆäº†æŠ€æœ¯å¤æ‚æ€§å’Œä¸šåŠ¡é€‚ç”¨æ€§ä¹‹é—´çš„å·®è·ã€‚</p>
<h1 id="historical-underpinnings-the-rise-oftransformer-architectures">Historical Underpinnings: The Rise ofÂ Transformer Architectures</h1>
<p>å†å²åŸºç¡€ï¼šå˜å‹å™¨æ¶æ„çš„å…´èµ·</p>
<p>Language models like ChatGPT (theÂ <em>GPT</em>Â stands forÂ <em>generative pretrained transformer</em>) didnâ€™t magicallyÂ emerge. Theyâ€™re the culmination of years of progress in the field of NLP, with particular acceleration since the late 2010s. At the heart of this advancement is the introduction of transformer architectures, which were detailed in the groundbreaking paperÂ <a href="https://oreil.ly/6NNbg">â€œAttention Is All You Needâ€</a>Â by the Google Brain team.<br>
åƒ ChatGPTï¼ˆGPT ä»£è¡¨ç”Ÿæˆå¼é¢„è®­ç»ƒè½¬æ¢å™¨ï¼‰è¿™æ ·çš„è¯­è¨€æ¨¡å‹å¹¶æ²¡æœ‰ç¥å¥‡åœ°å‡ºç°ã€‚å®ƒä»¬æ˜¯ NLP é¢†åŸŸå¤šå¹´è¿›æ­¥çš„ç»“æ™¶ï¼Œè‡ª 2010 å¹´ä»£åæœŸä»¥æ¥å°¤å…¶åŠ é€Ÿã€‚è¿™ä¸€è¿›æ­¥çš„æ ¸å¿ƒæ˜¯ transformer æ¶æ„çš„å¼•å…¥ï¼ŒGoogle Brain å›¢é˜Ÿåœ¨å¼€åˆ›æ€§çš„è®ºæ–‡â€œAttention Is All You Needâ€ä¸­å¯¹æ­¤è¿›è¡Œäº†è¯¦ç»†ä»‹ç»ã€‚</p>
<p>The real breakthrough of transformer architectures was the concept ofÂ <em>attention</em>. Traditional modelsÂ processed text sequentially, which limited their understanding of language structure especially over long distances of text. Attention transformed this by allowing models to directly relate distant words to one another irrespective of their positions in the text. This was a groundbreaking proposition. It meant that words and their context didnâ€™t have to move through the entire model to affect each other. This not only significantly improved the modelsâ€™ text comprehension but also made them much more efficient.<br>
å˜å‹å™¨æ¶æ„çš„çœŸæ­£çªç ´æ˜¯æ³¨æ„åŠ›çš„æ¦‚å¿µã€‚ä¼ ç»Ÿæ¨¡å‹æŒ‰é¡ºåºå¤„ç†æ–‡æœ¬ï¼Œè¿™é™åˆ¶äº†ä»–ä»¬å¯¹è¯­è¨€ç»“æ„çš„ç†è§£ï¼Œå°¤å…¶æ˜¯åœ¨é•¿è·ç¦»æ–‡æœ¬ä¸Šã€‚æ³¨æ„åŠ›æ”¹å˜äº†è¿™ä¸€ç‚¹ï¼Œå®ƒå…è®¸æ¨¡å‹ç›´æ¥å°†é¥è¿œçš„å•è¯ç›¸äº’å…³è”ï¼Œè€Œä¸ç®¡å®ƒä»¬åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®å¦‚ä½•ã€‚è¿™æ˜¯ä¸€ä¸ªå¼€åˆ›æ€§çš„ä¸»å¼ ã€‚è¿™æ„å‘³ç€å•è¯åŠå…¶ä¸Šä¸‹æ–‡ä¸å¿…åœ¨æ•´ä¸ªæ¨¡å‹ä¸­ç§»åŠ¨ä»¥ç›¸äº’å½±å“ã€‚è¿™ä¸ä»…æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œè€Œä¸”æé«˜äº†æ•ˆç‡ã€‚</p>
<p>This attention mechanism played a vital role in expanding the modelsâ€™ capacity to detect long-range dependencies in text. This was crucial for generating outputs that were not just contextually accurate and fluent, but also coherent over longer stretches.<br>
è¿™ç§æ³¨æ„åŠ›æœºåˆ¶åœ¨æ‰©å±•æ¨¡å‹æ£€æµ‹æ–‡æœ¬ä¸­é•¿ç¨‹ä¾èµ–å…³ç³»çš„èƒ½åŠ›æ–¹é¢å‘æŒ¥äº†è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è¿™å¯¹äºç”Ÿæˆè¾“å‡ºè‡³å…³é‡è¦ï¼Œè¿™äº›è¾“å‡ºä¸ä»…åœ¨ä¸Šä¸‹æ–‡ä¸­å‡†ç¡®å’Œæµç•…ï¼Œè€Œä¸”åœ¨è¾ƒé•¿æ—¶é—´å†…å…·æœ‰è¿è´¯æ€§ã€‚</p>
<p>According to AI pioneer and educatorÂ <a href="https://oreil.ly/JQd53">Andrew Ng</a>, much of the early NLP research, including the fundamental work on transformers, received significant funding from United States military intelligence agencies. Their keen interest in tools like machine translation and speech recognition, primarily for intelligence purposes, inadvertently paved the way for developments that transcended just translation.<br>
æ ¹æ®äººå·¥æ™ºèƒ½å…ˆé©±å’Œæ•™è‚²å®¶å´æ©è¾¾ï¼ˆAndrew Ngï¼‰çš„è¯´æ³•ï¼Œè®¸å¤šæ—©æœŸçš„NLPç ”ç©¶ï¼ŒåŒ…æ‹¬å…³äºå˜å‹å™¨çš„åŸºç¡€å·¥ä½œï¼Œéƒ½å¾—åˆ°äº†ç¾å›½å†›äº‹æƒ…æŠ¥æœºæ„çš„å¤§é‡èµ„åŠ©ã€‚ä»–ä»¬å¯¹æœºå™¨ç¿»è¯‘å’Œè¯­éŸ³è¯†åˆ«ç­‰å·¥å…·çš„æµ“åšå…´è¶£ï¼Œä¸»è¦ç”¨äºæ™ºèƒ½ç›®çš„ï¼Œæ— æ„ä¸­ä¸ºè¶…è¶Šç¿»è¯‘çš„å‘å±•é“ºå¹³äº†é“è·¯ã€‚</p>
<p>Training LLMs requires extensiveÂ computational resources. These models are fed with vast amounts of data, ranging from terabytes to petabytes, including internet content, academic papers, books, and more niche datasets tailored for specific purposes. Itâ€™s important to note, however, that theÂ data used to train LLMs can carryÂ <em>inherent biases from their sources</em>. Thus, users should exercise caution and ideally employ human oversight when leveraging these models, ensuring responsible and ethical AI applications.<br>
è®­ç»ƒ LLMs éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚è¿™äº›æ¨¡å‹æä¾›äº†å¤§é‡æ•°æ®ï¼Œä» TB åˆ° PB ä¸ç­‰ï¼ŒåŒ…æ‹¬äº’è”ç½‘å†…å®¹ã€å­¦æœ¯è®ºæ–‡ã€ä¹¦ç±ä»¥åŠä¸ºç‰¹å®šç›®çš„é‡èº«å®šåˆ¶çš„æ›´å¤šåˆ©åŸºæ•°æ®é›†ã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç”¨äºè®­ç»ƒ LLMs çš„æ•°æ®å¯èƒ½å¸¦æœ‰æ¥è‡ªå…¶æ¥æºçš„å›ºæœ‰åè§ã€‚å› æ­¤ï¼Œç”¨æˆ·åœ¨åˆ©ç”¨è¿™äº›æ¨¡å‹æ—¶åº”è°¨æ…è¡Œäº‹ï¼Œæœ€å¥½é‡‡ç”¨äººå·¥ç›‘ç£ï¼Œç¡®ä¿è´Ÿè´£ä»»å’Œåˆä¹é“å¾·çš„äººå·¥æ™ºèƒ½åº”ç”¨ç¨‹åºã€‚</p>
<p>OpenAIâ€™s GPT-4, for example, boasts an estimatedÂ <a href="https://oreil.ly/pZvMo">1.7 trillion parameters</a>, which is equivalent to an Excel spreadsheet that stretches across thirty thousand soccer fields.Â <em>Parameters</em>Â in the context of neural networksÂ are the weights and biases adjusted throughout the training process, allowing the model to represent and generate complex patterns based on the data itâ€™s trained on. The training cost for GPT-4 was estimated to be in the order ofÂ <a href="https://oreil.ly/_NAq5">$63 million</a>, and the training data would fill aboutÂ <a href="https://oreil.ly/D7jL5">650 kilometers of bookshelves full of books</a>.<br>
ä¾‹å¦‚ï¼ŒOpenAI çš„ GPT-4 ä¼°è®¡æ‹¥æœ‰ 1.7 ä¸‡äº¿ä¸ªå‚æ•°ï¼Œç›¸å½“äºä¸€ä¸ªæ¨ªè·¨ä¸‰ä¸‡ä¸ªè¶³çƒåœºçš„ Excel ç”µå­è¡¨æ ¼ã€‚ç¥ç»ç½‘ç»œä¸Šä¸‹æ–‡ä¸­çš„å‚æ•°æ˜¯åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´çš„æƒé‡å’Œåå·®ï¼Œå…è®¸æ¨¡å‹æ ¹æ®å…¶è®­ç»ƒçš„æ•°æ®è¡¨ç¤ºå’Œç”Ÿæˆå¤æ‚çš„æ¨¡å¼ã€‚GPT-4 çš„è®­ç»ƒæˆæœ¬ä¼°è®¡çº¦ä¸º 6300 ä¸‡ç¾å…ƒï¼Œè®­ç»ƒæ•°æ®å°†å¡«æ»¡å¤§çº¦ 650 å…¬é‡Œçš„ä¹¦æ¶ã€‚</p>
<p>To meet these requirements, major technological companies such as Microsoft, Meta, and Google have invested heavily, making LLM development a high-stakes endeavor.<br>
ä¸ºäº†æ»¡è¶³è¿™äº›è¦æ±‚ï¼ŒMicrosoftã€Meta å’Œ Google ç­‰ä¸»è¦ç§‘æŠ€å…¬å¸æŠ•å…¥äº†å¤§é‡èµ„é‡‘ï¼Œä½¿ LLM å¼€å‘æˆä¸ºä¸€é¡¹é«˜é£é™©çš„å·¥ä½œã€‚</p>
<p>The rise of LLMs has provided an increased demand for the hardware industry, particularly companies specializing inÂ graphics processing units (GPUs). NVIDIA, for instance, has become almost synonymous with high-performance GPUs that are essential for training LLMs.<br>
LLMs çš„å…´èµ·ä¸ºç¡¬ä»¶è¡Œä¸šæä¾›äº†æ›´å¤§çš„éœ€æ±‚ï¼Œå°¤å…¶æ˜¯ä¸“é—¨ä»äº‹å›¾å½¢å¤„ç†å•å…ƒ ï¼ˆGPUï¼‰ çš„å…¬å¸ã€‚ä¾‹å¦‚ï¼ŒNVIDIA å‡ ä¹å·²æˆä¸ºé«˜æ€§èƒ½ GPU çš„ä»£åè¯ï¼Œè€Œé«˜æ€§èƒ½ GPU å¯¹äºè®­ç»ƒ LLMs è‡³å…³é‡è¦ã€‚</p>
<p>The demand for powerful, efficient GPUs has skyrocketed as companies strive to build ever-larger and more complex models. Itâ€™s not just the raw computational power thatâ€™s sought after. GPUs also need to be fine-tuned for tasks endemic to machine learning, likeÂ tensor operations.Â <em>Tensors</em>, in a machine learning context, are multidimensional arrays of data, and operations on them are foundational to neural network computations. This emphasis on specialized capabilities has given rise to tailored hardware such as NVIDIAâ€™s H100 Tensor Core GPUs, explicitly crafted to expedite machine learning workloads.<br>
éšç€å…¬å¸åŠªåŠ›æ„å»ºæ›´å¤§ã€æ›´å¤æ‚çš„æ¨¡å‹ï¼Œå¯¹å¼ºå¤§ã€é«˜æ•ˆçš„ GPU çš„éœ€æ±‚çŒ›å¢ã€‚äººä»¬è¿½æ§çš„ä¸ä»…ä»…æ˜¯åŸå§‹çš„è®¡ç®—èƒ½åŠ›ã€‚GPU è¿˜éœ€è¦é’ˆå¯¹æœºå™¨å­¦ä¹ ç‰¹æœ‰çš„ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œä¾‹å¦‚å¼ é‡æ“ä½œã€‚åœ¨æœºå™¨å­¦ä¹ ç¯å¢ƒä¸­ï¼Œå¼ é‡æ˜¯å¤šç»´æ•°æ®æ•°ç»„ï¼Œå¯¹å®ƒä»¬çš„æ“ä½œæ˜¯ç¥ç»ç½‘ç»œè®¡ç®—çš„åŸºç¡€ã€‚è¿™ç§å¯¹ä¸“ä¸šåŠŸèƒ½çš„å¼ºè°ƒå‚¬ç”Ÿäº†é‡èº«å®šåˆ¶çš„ç¡¬ä»¶ï¼Œä¾‹å¦‚ NVIDIA çš„ H100 Tensor Core GPUï¼Œè¿™äº›ç¡¬ä»¶ä¸“ä¸ºåŠ å¿«æœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½è€Œè®¾è®¡ã€‚</p>
<p>Furthermore, the overwhelming demand often outstrips the supply of these top-tier GPUs, sending prices on an upward trajectory. This supply-demand interplay has transformed the GPU market into a fiercely competitive and profitable arena. Here, an eclectic clientele, ranging from tech behemoths to academic researchers, scramble to procure the most advanced hardware.<br>
æ­¤å¤–ï¼Œå‹å€’æ€§çš„éœ€æ±‚å¾€å¾€è¶…è¿‡è¿™äº›é¡¶çº§ GPU çš„ä¾›åº”ï¼Œä½¿ä»·æ ¼èµ°ä¸Šä¸Šæ¶¨è½¨é“ã€‚è¿™ç§ä¾›éœ€ç›¸äº’ä½œç”¨å·²å°† GPU å¸‚åœºè½¬å˜ä¸ºä¸€ä¸ªç«äº‰æ¿€çƒˆä¸”æœ‰åˆ©å¯å›¾çš„èˆå°ã€‚åœ¨è¿™é‡Œï¼Œä»ç§‘æŠ€å·¨å¤´åˆ°å­¦æœ¯ç ”ç©¶äººå‘˜ï¼Œä¸æ‹˜ä¸€æ ¼çš„å®¢æˆ·äº‰å…ˆæååœ°é‡‡è´­æœ€å…ˆè¿›çš„ç¡¬ä»¶ã€‚</p>
<p>This surge in demand has sparked a wave of innovation beyond just GPUs. Companies are now focusing on creatingÂ dedicated AI hardware, such as Googleâ€™s Tensor Processing Units (TPUs), to cater to the growing computational needs of AI models.<br>
è¿™ç§éœ€æ±‚çš„æ¿€å¢å¼•å‘äº†ä¸€æ³¢åˆ›æ–°æµªæ½®ï¼Œè€Œä¸ä»…ä»…æ˜¯ GPUã€‚å…¬å¸ç°åœ¨æ­£ä¸“æ³¨äºåˆ›å»ºä¸“ç”¨çš„äººå·¥æ™ºèƒ½ç¡¬ä»¶ï¼Œä¾‹å¦‚è°·æ­Œçš„å¼ é‡å¤„ç†å•å…ƒï¼ˆTPUï¼‰ï¼Œä»¥æ»¡è¶³äººå·¥æ™ºèƒ½æ¨¡å‹æ—¥ç›Šå¢é•¿çš„è®¡ç®—éœ€æ±‚ã€‚</p>
<p>This evolving landscape underscores not just the symbiotic ties between software and hardware in the AI sphere but also spotlights the ripple effect of the LLMÂ <em>gold rush</em>. Itâ€™s steering innovations and funneling investments into various sectors, especially thoseÂ offering the fundamental components for crafting these models.<br>
è¿™ç§ä¸æ–­å‘å±•çš„æ ¼å±€ä¸ä»…å¼ºè°ƒäº†äººå·¥æ™ºèƒ½é¢†åŸŸè½¯ä»¶å’Œç¡¬ä»¶ä¹‹é—´çš„å…±ç”Ÿå…³ç³»ï¼Œè¿˜å‡¸æ˜¾äº†LLMæ·˜é‡‘çƒ­çš„è¿é”ååº”ã€‚å®ƒæ­£åœ¨å¼•å¯¼åˆ›æ–°å¹¶å°†æŠ•èµ„æ±‡é›†åˆ°å„ä¸ªé¢†åŸŸï¼Œå°¤å…¶æ˜¯é‚£äº›ä¸ºåˆ¶ä½œè¿™äº›æ¨¡å‹æä¾›åŸºæœ¬ç»„ä»¶çš„è¡Œä¸šã€‚</p>
<h1 id="openais-generative-pretrained-transformers">OpenAIâ€™s Generative Pretrained Transformers</h1>
<p>OpenAI çš„ç”Ÿæˆå¼é¢„è®­ç»ƒè½¬æ¢å™¨</p>
<p>Founded with a mission to ensure that artificial general intelligence benefits all of humanity,Â <a href="https://openai.com/">OpenAI</a>Â has recently been at the forefront of the AI revolution. One of their most groundbreaking contributions has been the GPT series of models, which have substantially redefined the boundaries of what LLMs can achieve.<br>
OpenAI æˆç«‹çš„ä½¿å‘½æ˜¯ç¡®ä¿é€šç”¨äººå·¥æ™ºèƒ½é€ ç¦å…¨äººç±»ï¼Œæœ€è¿‘ä¸€ç›´å¤„äºäººå·¥æ™ºèƒ½é©å‘½çš„æœ€å‰æ²¿ã€‚ä»–ä»¬æœ€å…·å¼€åˆ›æ€§çš„è´¡çŒ®ä¹‹ä¸€æ˜¯ GPT ç³»åˆ—æ¨¡å‹ï¼Œå®ƒä»¬ä»æ ¹æœ¬ä¸Šé‡æ–°å®šä¹‰äº† LLMs å¯ä»¥å®ç°çš„è¾¹ç•Œã€‚</p>
<p>The original GPT model by OpenAI was more than a mere research output; it was a compelling demonstration of the potential of transformer-based architectures. This model showcased the initial steps toward making machines understand and generate human-like language, laying the foundation for future advancements.<br>
OpenAI æœ€åˆçš„ GPT æ¨¡å‹ä¸ä»…ä»…æ˜¯ä¸€é¡¹ç ”ç©¶æˆæœ;è¿™æ˜¯å¯¹åŸºäºTransformerçš„æ¶æ„æ½œåŠ›çš„æœ‰åŠ›è¯æ˜ã€‚è¯¥æ¨¡å‹å±•ç¤ºäº†ä½¿æœºå™¨ç†è§£å’Œç”Ÿæˆç±»ä¼¼äººç±»è¯­è¨€çš„åˆæ­¥æ­¥éª¤ï¼Œä¸ºæœªæ¥çš„è¿›æ­¥å¥ å®šäº†åŸºç¡€ã€‚</p>
<p>The unveiling of GPT-2 was met withÂ both anticipation and caution. Recognizing the modelâ€™s powerful capabilities, OpenAI initially hesitated in releasing it due to concerns about its potential misuse. Such was the might of GPT-2 that ethical concerns took center stage, which might look quaint compared to the power of todayâ€™s models. However, when OpenAI decided to release the project asÂ <a href="https://oreil.ly/evOQE">open-source</a>, it didnâ€™t just mean making the code public. It allowed businesses and researchers to use these pretrained models as building blocks, incorporating AI into their applications without starting from scratch. This move democratized access to high-level natural language processing capabilities, spurring innovation across various domains.<br>
GPT-2 çš„æ­å¹•æ—¢ä»¤äººæœŸå¾…ï¼Œä¹Ÿä»¤äººè°¨æ…ã€‚è®¤è¯†åˆ°è¯¥æ¨¡å‹çš„å¼ºå¤§åŠŸèƒ½ï¼ŒOpenAI æœ€åˆå¯¹å‘å¸ƒå®ƒçŠ¹è±«ä¸å†³ï¼Œå› ä¸ºæ‹…å¿ƒå®ƒå¯èƒ½è¢«æ»¥ç”¨ã€‚GPT-2 çš„å¨åŠ›å¦‚æ­¤ä¹‹å¤§ï¼Œä»¥è‡³äºé“å¾·é—®é¢˜å æ®äº†ä¸­å¿ƒä½ç½®ï¼Œä¸å½“ä»Šæ¨¡å‹çš„åŠ›é‡ç›¸æ¯”ï¼Œè¿™å¯èƒ½çœ‹èµ·æ¥å¾ˆå¤æ€ªã€‚ç„¶è€Œï¼Œå½“ OpenAI å†³å®šå°†è¯¥é¡¹ç›®ä½œä¸ºå¼€æºå‘å¸ƒæ—¶ï¼Œå®ƒå¹¶ä¸ä»…ä»…æ„å‘³ç€å…¬å¼€ä»£ç ã€‚å®ƒå…è®¸ä¼ä¸šå’Œç ”ç©¶äººå‘˜ä½¿ç”¨è¿™äº›é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºæ„å»ºå—ï¼Œå°†äººå·¥æ™ºèƒ½æ•´åˆåˆ°ä»–ä»¬çš„åº”ç”¨ç¨‹åºä¸­ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹ã€‚æ­¤ä¸¾ä½¿å¯¹é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›çš„è®¿é—®æ°‘ä¸»åŒ–ï¼Œåˆºæ¿€äº†å„ä¸ªé¢†åŸŸçš„åˆ›æ–°ã€‚</p>
<p>After GPT-2, OpenAI decided to focus on releasing paid, closed-source models. GPT-3â€™s arrival marked a monumental stride in the progression of LLMs. It garnered significant media attention, not just for its technical prowess but also for the societal implications of its capabilities. This model could produce text so convincing that it often became indistinguishable from human-written content. From crafting intricate pieces of literature to churning out operational code snippets, GPT-3 exemplified the seemingly boundless potential of AI.<br>
åœ¨ GPT-2 ä¹‹åï¼ŒOpenAI å†³å®šä¸“æ³¨äºå‘å¸ƒä»˜è´¹çš„é—­æºæ¨¡å‹ã€‚GPT-3 çš„åˆ°æ¥æ ‡å¿—ç€ LLMs çš„è¿›æ­¥è¿ˆå‡ºäº†å·¨å¤§çš„ä¸€æ­¥ã€‚å®ƒå¼•èµ·äº†åª’ä½“çš„å¹¿æ³›å…³æ³¨ï¼Œä¸ä»…å› ä¸ºå®ƒçš„æŠ€æœ¯å®åŠ›ï¼Œè¿˜å› ä¸ºå®ƒçš„èƒ½åŠ›å¯¹ç¤¾ä¼šçš„å½±å“ã€‚è¿™ç§æ¨¡å‹å¯ä»¥äº§ç”Ÿå¦‚æ­¤ä»¤äººä¿¡æœçš„æ–‡æœ¬ï¼Œä»¥è‡³äºå®ƒé€šå¸¸ä¸äººç±»ç¼–å†™çš„å†…å®¹æ— æ³•åŒºåˆ†ã€‚ä»åˆ¶ä½œé”™ç»¼å¤æ‚çš„æ–‡çŒ®åˆ°åˆ¶ä½œå¯æ“ä½œçš„ä»£ç ç‰‡æ®µï¼ŒGPT-3 ä½“ç°äº† AI çœ‹ä¼¼æ— é™çš„æ½œåŠ›ã€‚</p>
<h2 id="gpt-35-turbo-and-chatgpt">GPT-3.5-turbo and ChatGPT</h2>
<p>GPT-3.5-turbo å’Œ ChatGPT</p>
<p>Bolstered by Microsoftâ€™s significantÂ investment in their company, OpenAI introduced GPT-3.5-turbo, an optimized version of its already exceptional predecessor. Following aÂ <a href="https://oreil.ly/1C8qm">$1 billion injection</a>Â from Microsoft in 2019, which later increased to a hefty $13 billion for a 49% stake in OpenAIâ€™s for-profit arm, OpenAI used these resources to develop GPT-3.5-turbo, which offered improved efficiency and affordability, effectively making LLMs more accessible for a broader range of use cases.<br>
åœ¨ Microsoft å¯¹å…¶å…¬å¸çš„é‡å¤§æŠ•èµ„çš„æ”¯æŒä¸‹ï¼ŒOpenAI æ¨å‡ºäº† GPT-3.5-turboï¼Œè¿™æ˜¯å…¶å·²ç»éå¸¸å‡ºè‰²çš„å‰èº«çš„ä¼˜åŒ–ç‰ˆæœ¬ã€‚åœ¨ Microsoft äº 2019 å¹´æ³¨èµ„ 10 äº¿ç¾å…ƒåï¼ŒOpenAI çš„è¥åˆ©æ€§éƒ¨é—¨ 49% çš„è‚¡ä»½å¢åŠ åˆ° 130 äº¿ç¾å…ƒï¼ŒOpenAI åˆ©ç”¨è¿™äº›èµ„æºå¼€å‘äº† GPT-3.5-turboï¼Œå®ƒæä¾›äº†æ›´é«˜çš„æ•ˆç‡å’Œå¯è´Ÿæ‹…æ€§ï¼Œæœ‰æ•ˆåœ°ä½¿ LLMs æ›´å®¹æ˜“ç”¨äºæ›´å¹¿æ³›çš„ç”¨ä¾‹ã€‚</p>
<p>OpenAI wanted to gather more world feedback for fine-tuning, and soÂ <a href="https://chat.openai.com/">ChatGPT</a>Â was born. Unlike its general-purpose siblings,Â <a href="https://oreil.ly/6ib-Q">ChatGPT was fine-tuned</a>Â to excel in conversational contexts, enabling a dialogue between humans and machines that felt natural and meaningful.<br>
OpenAI å¸Œæœ›æ”¶é›†æ›´å¤šä¸–ç•Œåé¦ˆä»¥è¿›è¡Œå¾®è°ƒï¼Œå› æ­¤ ChatGPT è¯ç”Ÿäº†ã€‚ä¸é€šç”¨çš„å…„å¼Ÿå§å¦¹ä¸åŒï¼ŒChatGPT ç»è¿‡å¾®è°ƒï¼Œåœ¨å¯¹è¯ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä½¿äººä¸æœºå™¨ä¹‹é—´çš„å¯¹è¯å˜å¾—è‡ªç„¶è€Œæœ‰æ„ä¹‰ã€‚</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch02.html#figure-2-4">FigureÂ 2-4</a>Â shows the training process for ChatGPT, which involves three main steps:<br>
ChatGPT çš„è®­ç»ƒè¿‡ç¨‹å¦‚å›¾ 2-4 æ‰€ç¤ºï¼Œä¸»è¦åˆ†ä¸º 3 ä¸ªæ­¥éª¤ï¼š</p>
<p>Collection of demonstration data<br>
æ”¶é›†æ¼”ç¤ºæ•°æ®</p>
<p>In thisÂ step, human labelers provide examples of the desired model behavior on a distribution of prompts. The labelers are trained on the project and follow specific instructions to annotate the prompts accurately.<br>
åœ¨æ­¤æ­¥éª¤ä¸­ï¼Œäººå·¥æ ‡è®°å™¨åœ¨æç¤ºåˆ†å¸ƒä¸Šæä¾›æ‰€éœ€æ¨¡å‹è¡Œä¸ºçš„ç¤ºä¾‹ã€‚è´´æ ‡å‘˜æ¥å—è¿‡è¯¥é¡¹ç›®çš„åŸ¹è®­ï¼Œå¹¶æŒ‰ç…§ç‰¹å®šè¯´æ˜å‡†ç¡®æ³¨é‡Šæç¤ºã€‚</p>
<p>Training a supervised policy<br>
åŸ¹è®­å—ç›‘ç£çš„ç­–ç•¥</p>
<p>The demonstrationÂ data collected in the previous step is used to fine-tune a pretrained GPT-3 model using supervised learning. In supervised learning, models are trained on a labeled dataset where the correct answers are provided. This step helps the model to learn to follow the given instructions and produce outputs that align with the desired behavior.<br>
ä¸Šä¸€æ­¥ä¸­æ”¶é›†çš„æ¼”ç¤ºæ•°æ®ç”¨äºä½¿ç”¨ç›‘ç£å­¦ä¹ å¾®è°ƒé¢„è®­ç»ƒçš„ GPT-3 æ¨¡å‹ã€‚åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œæ¨¡å‹åœ¨æ ‡è®°çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­æä¾›äº†æ­£ç¡®çš„ç­”æ¡ˆã€‚æ­¤æ­¥éª¤å¯å¸®åŠ©æ¨¡å‹å­¦ä¹ éµå¾ªç»™å®šçš„æŒ‡ä»¤å¹¶ç”Ÿæˆä¸æ‰€éœ€è¡Œä¸ºä¸€è‡´çš„è¾“å‡ºã€‚</p>
<p>Collection of comparison data and reinforcement learning<br>
æ¯”è¾ƒæ•°æ®çš„æ”¶é›†å’Œå¼ºåŒ–å­¦ä¹ </p>
<p>In this step, a dataset of model outputs is collected,Â and human labelers rank the outputs based on their preference. A reward model is then trained to predict which outputs the labelers would prefer. Finally, reinforcement learning techniques, specifically the Proximal Policy Optimization (PPO) algorithm, are used to optimize the supervised policy to maximize the reward from the reward model.<br>
åœ¨æ­¤æ­¥éª¤ä¸­ï¼Œå°†æ”¶é›†æ¨¡å‹è¾“å‡ºçš„æ•°æ®é›†ï¼Œäººå·¥æ ‡è®°äººå‘˜æ ¹æ®ä»–ä»¬çš„åå¥½å¯¹è¾“å‡ºè¿›è¡Œæ’åã€‚ç„¶åè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä»¥é¢„æµ‹æ ‡è®°è€…æ›´å–œæ¬¢å“ªäº›è¾“å‡ºã€‚æœ€åï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•ï¼Œå¯¹ç›‘ç£ç­–ç•¥è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æœ€å¤§åŒ–å¥–åŠ±æ¨¡å‹çš„å¥–åŠ±ã€‚</p>
<p>This training process allows the ChatGPT model toÂ align its behavior with human intent. The use of reinforcement learning with human feedback helped create a model that is more helpful, honest, and safe compared to the pretrained GPT-3 model.<br>
è¿™ä¸ªè®­ç»ƒè¿‡ç¨‹å…è®¸ ChatGPT æ¨¡å‹å°†å…¶è¡Œä¸ºä¸äººç±»æ„å›¾ä¿æŒä¸€è‡´ã€‚ä¸é¢„è®­ç»ƒçš„ GPT-3 æ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œäººç±»åé¦ˆæœ‰åŠ©äºåˆ›å»ºä¸€ä¸ªæ›´æœ‰å¸®åŠ©ã€æ›´è¯šå®ã€æ›´å®‰å…¨çš„æ¨¡å‹ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0204.png" alt="ChatGPT Fine Tuning Approach"></p>
<h6 id="figure-2-4the-fine-tuning-process-for-chatgpt">Figure 2-4.Â The fine-tuning process for ChatGPT</h6>
<p>å›¾ 2-4ã€‚ChatGPT çš„å¾®è°ƒè¿‡ç¨‹</p>
<p>According to aÂ <a href="https://oreil.ly/2Ivq2">UBS study</a>, by January 2023 ChatGPT set a new benchmark, amassing 100 million active users and becoming the fastest-growing consumer application in internet history. ChatGPT is now a go-to for customer service, virtual assistance, and numerous other applications that require the finesse of human-like conversation.<br>
æ ¹æ®ç‘é“¶çš„ä¸€é¡¹ç ”ç©¶ï¼Œåˆ° 2023 å¹´ 1 æœˆï¼ŒChatGPT æ ‘ç«‹äº†æ–°çš„æ ‡æ†ï¼Œç§¯ç´¯äº† 1 äº¿æ´»è·ƒç”¨æˆ·ï¼Œæˆä¸ºäº’è”ç½‘å†å²ä¸Šå¢é•¿æœ€å¿«çš„æ¶ˆè´¹è€…åº”ç”¨ç¨‹åºã€‚ChatGPT ç°åœ¨æ˜¯å®¢æˆ·æœåŠ¡ã€è™šæ‹ŸååŠ©å’Œè®¸å¤šå…¶ä»–éœ€è¦ç±»ä¼¼äººç±»å¯¹è¯æŠ€å·§çš„åº”ç”¨ç¨‹åºçš„é¦–é€‰ã€‚</p>
<h1 id="gpt-4gpt-4çš„">GPT-4Â GPT-4çš„</h1>
<p>In 2024, OpenAI released GPT-4, whichÂ excels in understanding complex queries and generating contextually relevant and coherent text. For example, GPT-4 scored in the 90th percentile of the bar exam with a score of 298 out of 400. Currently, GPT-3.5-turbo is free to use in ChatGPT, but GPT-4 requires aÂ <a href="https://oreil.ly/UOEBM">monthly payment</a>.<br>
2024 å¹´ï¼ŒOpenAI å‘å¸ƒäº† GPT-4ï¼Œå®ƒæ“…é•¿ç†è§£å¤æ‚çš„æŸ¥è¯¢å’Œç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³ä¸”è¿è´¯çš„æ–‡æœ¬ã€‚ä¾‹å¦‚ï¼ŒGPT-4 åœ¨å¾‹å¸ˆè€ƒè¯•çš„ç¬¬ 90 ä¸ªç™¾åˆ†ä½å¾—åˆ†ä¸º 298 åˆ†ï¼ˆæ»¡åˆ† 400 åˆ†ï¼‰ã€‚ç›®å‰ï¼ŒGPT-3.5-turbo å¯ä»¥åœ¨ ChatGPT ä¸­å…è´¹ä½¿ç”¨ï¼Œä½† GPT-4 éœ€è¦æŒ‰æœˆä»˜è´¹ã€‚</p>
<p>GPT-4 uses aÂ <a href="https://oreil.ly/v45LZ">mixture-of-experts approach</a>; it goes beyond relying on a single modelâ€™s inference to produce even more accurate and insightful results.<br>
GPT-4 é‡‡ç”¨ä¸“å®¶æ··åˆæ–¹æ³•;å®ƒè¶…è¶Šäº†ä¾èµ–å•ä¸ªæ¨¡å‹çš„æ¨ç†æ¥äº§ç”Ÿæ›´å‡†ç¡®ã€æ›´æœ‰æ´å¯ŸåŠ›çš„ç»“æœã€‚</p>
<p>On May 13, 2024, OpenAI introducedÂ <a href="https://oreil.ly/4ttmq">GPT-4o</a>, an advanced model capable of processing and reasoning across text, audio, and vision inputs in real time. This model offers enhanced performance, particularly in vision and audio understanding; it is also faster and more cost-effective than its predecessors due to its ability to process all three modalities in one neural network.<br>
2024 å¹´ 5 æœˆ 13 æ—¥ï¼ŒOpenAI æ¨å‡ºäº† GPT-4oï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿå®æ—¶å¤„ç†å’Œæ¨ç†æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰è¾“å…¥çš„é«˜çº§æ¨¡å‹ã€‚è¯¥æ¨¡å‹æä¾›äº†å¢å¼ºçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å’ŒéŸ³é¢‘ç†è§£æ–¹é¢;ç”±äºå®ƒèƒ½å¤Ÿåœ¨ä¸€ä¸ªç¥ç»ç½‘ç»œä¸­å¤„ç†æ‰€æœ‰ä¸‰ç§æ¨¡å¼ï¼Œå› æ­¤å®ƒä¹Ÿæ¯”å…¶å‰è¾ˆæ›´å¿«ã€æ›´å…·æˆæœ¬æ•ˆç›Šã€‚</p>
<h1 id="googles-geminiè°·æ­Œçš„åŒå­åº§">Googleâ€™s GeminiÂ è°·æ­Œçš„åŒå­åº§</h1>
<p>After Google lost search marketÂ share due to ChatGPT usage, it initially released Bard on March 21, 2023. Bard was a bitÂ <a href="https://oreil.ly/Sj24h">rough around the edges</a>Â and definitely didnâ€™t initially have the same high-quality LLM responses that ChatGPT offered (<a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch02.html#figure-2-5">FigureÂ 2-5</a>).<br>
åœ¨è°·æ­Œå› ä½¿ç”¨ ChatGPT è€Œå¤±å»æœç´¢å¸‚åœºä»½é¢åï¼Œå®ƒæœ€åˆäº 2023 å¹´ 3 æœˆ 21 æ—¥å‘å¸ƒäº† Bardã€‚Bard çš„è¾¹ç¼˜æœ‰ç‚¹ç²—ç³™ï¼Œæœ€åˆè‚¯å®šæ²¡æœ‰ ChatGPT æä¾›çš„é«˜è´¨é‡ LLM å“åº”ï¼ˆå›¾ 2-5ï¼‰ã€‚</p>
<p>Google has kept adding extra features over time including code generation, visual AI, real-time search, and voice into Bard, bringing it closer to ChatGPT in terms of quality.<br>
éšç€æ—¶é—´çš„æ¨ç§»ï¼Œè°·æ­Œä¸€ç›´åœ¨å‘ Bard æ·»åŠ é¢å¤–çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€è§†è§‰ AIã€å®æ—¶æœç´¢å’Œè¯­éŸ³ï¼Œä½¿å…¶åœ¨è´¨é‡æ–¹é¢æ›´æ¥è¿‘ ChatGPTã€‚</p>
<p>On March 14, 2023, GoogleÂ releasedÂ <a href="https://oreil.ly/EbI8-">PaLM API</a>, allowing developers to access it on Google Cloud Platform. In April 2023, Amazon Web Services (AWS) releasedÂ similar services such asÂ <a href="https://oreil.ly/4fNQX">Amazon Bedrock</a>Â andÂ <a href="https://oreil.ly/FJ-7D">Amazonâ€™s Titan FMs</a>. GoogleÂ <a href="https://oreil.ly/EO42O">rebranded Bard to Gemini</a>Â for their v1.5 release in February 2024 and started to get results similar to GPT-4.<br>
2023 å¹´ 3 æœˆ 14 æ—¥ï¼ŒGoogle å‘å¸ƒäº† PaLM APIï¼Œå…è®¸å¼€å‘è€…åœ¨ Google Cloud Platform ä¸Šè®¿é—®å®ƒã€‚2023 å¹´ 4 æœˆï¼Œäºšé©¬é€Šç½‘ç»œæœåŠ¡ ï¼ˆAWSï¼‰ å‘å¸ƒäº†ç±»ä¼¼çš„æœåŠ¡ï¼Œä¾‹å¦‚ Amazon Bedrock å’Œäºšé©¬é€Šçš„ Titan FMã€‚ è°·æ­Œåœ¨ 2024 å¹´ 2 æœˆçš„ v1.5 ç‰ˆæœ¬ä¸­å°† Bard æ›´åä¸º Geminiï¼Œå¹¶å¼€å§‹è·å¾—ç±»ä¼¼äº GPT-4 çš„ç»“æœã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0205.png" alt="Google&rsquo;s bard which is a similar application to ChatGPT."></p>
<h6 id="figure-2-5bard-hallucinating-results-about-the-james-webb-space-telescope">Figure 2-5.Â Bard hallucinating results about the James Webb Space Telescope</h6>
<p>å›¾ 2-5ã€‚åŸæ¸¸è¯—äººå…³äºè©¹å§†æ–¯Â·éŸ¦ä¼¯å¤ªç©ºæœ›è¿œé•œçš„å¹»è§‰ç»“æœ</p>
<p>Also, Google released two smallerÂ <a href="https://oreil.ly/LWIwv">open source models</a>Â based on the same architecture as Gemini. OpenAI is finally no longer the only obvious option for software engineers to integrate state-of-the-art LLMs into theirÂ applications.<br>
æ­¤å¤–ï¼Œè°·æ­Œè¿˜å‘å¸ƒäº†ä¸¤ä¸ªè¾ƒå°çš„å¼€æºæ¨¡å‹ï¼ŒåŸºäºä¸ Gemini ç›¸åŒçš„æ¶æ„ã€‚OpenAI ç»ˆäºä¸å†æ˜¯è½¯ä»¶å·¥ç¨‹å¸ˆå°†æœ€å…ˆè¿›çš„ LLMs é›†æˆåˆ°ä»–ä»¬çš„åº”ç”¨ç¨‹åºä¸­çš„å”¯ä¸€æ˜æ˜¾é€‰æ‹©ã€‚</p>
<h1 id="metas-llama-and-open-source">Metaâ€™s Llama and Open Source</h1>
<p>Meta çš„ Llama å’Œå¼€æº</p>
<p>Metaâ€™s approach to language models differsÂ significantly from other competitors in the industry. By sequentially releasing open source modelsÂ <a href="https://oreil.ly/LroPn">Llama</a>,Â <a href="https://oreil.ly/NeZLw">Llama 2</a>Â andÂ <a href="https://oreil.ly/Vwlo-">Llama 3</a>, Meta aims to foster a more inclusive and collaborative AI development ecosystem.<br>
Meta çš„è¯­è¨€æ¨¡å‹æ–¹æ³•ä¸ä¸šå†…å…¶ä»–ç«äº‰å¯¹æ‰‹æœ‰å¾ˆå¤§ä¸åŒã€‚Meta é€šè¿‡ä¾æ¬¡å‘å¸ƒå¼€æºæ¨¡å‹ Llamaã€Llama 2 å’Œ Llama 3ï¼Œæ—¨åœ¨æ‰“é€ ä¸€ä¸ªæ›´å…·åŒ…å®¹æ€§å’Œåä½œæ€§çš„ AI å¼€å‘ç”Ÿæ€ç³»ç»Ÿã€‚</p>
<p>The open source nature of Llama 2 and Llama 3 has significant implications for theÂ broader tech industry, especially for large enterprises. The transparency and collaborative ethos encourage rapid innovation, as problems and vulnerabilities can be quickly identified and addressed by the global developer community. As these models become more robust and secure, large corporations can adopt them with increased confidence.<br>
Llama 2 å’Œ Llama 3 çš„å¼€æºæ€§è´¨å¯¹æ›´å¹¿æ³›çš„ç§‘æŠ€è¡Œä¸šå…·æœ‰é‡å¤§å½±å“ï¼Œå°¤å…¶æ˜¯å¯¹å¤§å‹ä¼ä¸šè€Œè¨€ã€‚é€æ˜åº¦å’Œåä½œç²¾ç¥é¼“åŠ±å¿«é€Ÿåˆ›æ–°ï¼Œå› ä¸ºå…¨çƒå¼€å‘è€…ç¤¾åŒºå¯ä»¥å¿«é€Ÿè¯†åˆ«å’Œè§£å†³é—®é¢˜å’Œæ¼æ´ã€‚éšç€è¿™äº›æ¨¡å‹å˜å¾—æ›´åŠ å¼ºå¤§å’Œå®‰å…¨ï¼Œå¤§å…¬å¸å¯ä»¥æ›´æœ‰ä¿¡å¿ƒåœ°é‡‡ç”¨å®ƒä»¬ã€‚</p>
<p>Metaâ€™s open source strategy not only democratizes access to state-of-the-art AI technologies but also has the potential to make a meaningful impact across the industry. By setting the stage for a collaborative, transparent, and decentralized development process, Llama 2 and Llama 3 are pioneering models that could very well define the future of generative AI. The models are available in 7, 8 and 70 billion parameter versions on AWS, Google Cloud, Hugging Face, and other platforms.<br>
Meta çš„å¼€æºæˆ˜ç•¥ä¸ä»…ä½¿è·å¾—æœ€å…ˆè¿›çš„ AI æŠ€æœ¯æ°‘ä¸»åŒ–ï¼Œè€Œä¸”è¿˜æœ‰å¯èƒ½å¯¹æ•´ä¸ªè¡Œä¸šäº§ç”Ÿæœ‰æ„ä¹‰çš„å½±å“ã€‚Llama 2 å’Œ Llama 3 ä¸ºåä½œã€é€æ˜å’Œå»ä¸­å¿ƒåŒ–çš„å¼€å‘è¿‡ç¨‹å¥ å®šäº†åŸºç¡€ï¼Œæ˜¯å¯ä»¥å¾ˆå¥½åœ°å®šä¹‰ç”Ÿæˆå¼ AI æœªæ¥çš„å¼€åˆ›æ€§æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹åœ¨ AWSã€Google Cloudã€Hugging Face å’Œå…¶ä»–å¹³å°ä¸Šæä¾› 7ã€8 å’Œ 700 äº¿å‚æ•°ç‰ˆæœ¬ã€‚</p>
<p>The open source nature of these models presents a double-edged sword. On one hand, it levels the playing field. This means that even smaller developers have the opportunity to contribute to innovation, improving and applying open source models to practical business applications. This kind of decentralized innovation could lead to breakthroughs that might not occur within the walled gardens of a single organization, enhancing the modelsâ€™ capabilities and applications.<br>
è¿™äº›æ¨¡å‹çš„å¼€æºæ€§è´¨æ˜¯ä¸€æŠŠåŒåˆƒå‰‘ã€‚ä¸€æ–¹é¢ï¼Œå®ƒåˆ›é€ äº†å…¬å¹³çš„ç«äº‰ç¯å¢ƒã€‚è¿™æ„å‘³ç€å³ä½¿æ˜¯è¾ƒå°çš„å¼€å‘äººå‘˜ä¹Ÿæœ‰æœºä¼šä¸ºåˆ›æ–°åšå‡ºè´¡çŒ®ï¼Œæ”¹è¿›å¼€æºæ¨¡å‹å¹¶å°†å…¶åº”ç”¨äºå®é™…çš„ä¸šåŠ¡åº”ç”¨ç¨‹åºã€‚è¿™ç§å»ä¸­å¿ƒåŒ–çš„åˆ›æ–°å¯èƒ½ä¼šå¸¦æ¥çªç ´ï¼Œè€Œè¿™äº›çªç ´å¯èƒ½ä¸ä¼šå‘ç”Ÿåœ¨å•ä¸ªç»„ç»‡çš„å›´å¢™èŠ±å›­ä¸­ï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„èƒ½åŠ›å’Œåº”ç”¨ã€‚</p>
<p>However, the same openness that makes this possible also poses potential risks, as it could allow malicious actors to exploit this technology for detrimental ends. This indeed is a concern that organizations like OpenAI share, suggesting that some degree of control and restriction can actually serve to mitigate the dangerous applications of these powerful tools.<br>
ç„¶è€Œï¼Œä½¿è¿™æˆä¸ºå¯èƒ½çš„å¼€æ”¾æ€§ä¹Ÿå¸¦æ¥äº†æ½œåœ¨çš„é£é™©ï¼Œå› ä¸ºå®ƒå¯èƒ½å…è®¸æ¶æ„è¡Œä¸ºè€…åˆ©ç”¨è¿™é¡¹æŠ€æœ¯è¾¾åˆ°æœ‰å®³ç›®çš„ã€‚è¿™ç¡®å®æ˜¯OpenAIç­‰ç»„ç»‡å…±åŒå…³æ³¨çš„é—®é¢˜ï¼Œè¿™è¡¨æ˜ä¸€å®šç¨‹åº¦çš„æ§åˆ¶å’Œé™åˆ¶å®é™…ä¸Šå¯ä»¥å‡è½»è¿™äº›å¼ºå¤§å·¥å…·çš„å±é™©åº”ç”¨ã€‚</p>
<h1 id="leveraging-quantization-and-lora">Leveraging Quantization and LoRA</h1>
<p>åˆ©ç”¨é‡åŒ–å’Œ LoRA</p>
<p>One of the game-changing aspects of these open sourceÂ models is the potential forÂ <a href="https://oreil.ly/bkWXk">quantization</a>Â and the use ofÂ <a href="https://oreil.ly/zORsB">LoRA</a>Â (low-rank approximations). These techniques allow developers to fit the models into smaller hardware footprints. Quantization helps to reduce the numerical precision of the modelâ€™s parameters, thereby shrinking the overall size of the model without a significant loss in performance. Meanwhile, LoRA assists in optimizing the networkâ€™s architecture, making it more efficient to run on consumer-grade hardware.<br>
è¿™äº›å¼€æºæ¨¡å‹æ”¹å˜æ¸¸æˆè§„åˆ™çš„æ–¹é¢ä¹‹ä¸€æ˜¯é‡åŒ–å’Œ LoRAï¼ˆä½ç§©è¿‘ä¼¼ï¼‰çš„æ½œåŠ›ã€‚è¿™äº›æŠ€æœ¯ä½¿å¼€å‘äººå‘˜èƒ½å¤Ÿå°†æ¨¡å‹æ‹Ÿåˆåˆ°æ›´å°çš„ç¡¬ä»¶å ç”¨ç©ºé—´ä¸­ã€‚é‡åŒ–æœ‰åŠ©äºé™ä½æ¨¡å‹å‚æ•°çš„æ•°å€¼ç²¾åº¦ï¼Œä»è€Œç¼©å°æ¨¡å‹çš„æ•´ä½“å¤§å°ï¼Œè€Œä¸ä¼šæ˜¾è‘—é™ä½æ€§èƒ½ã€‚åŒæ—¶ï¼ŒLoRAæœ‰åŠ©äºä¼˜åŒ–ç½‘ç»œæ¶æ„ï¼Œä½¿å…¶åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿è¡Œæ•ˆç‡æ›´é«˜ã€‚</p>
<p>Such optimizations make fine-tuning these LLMs increasingly feasible on consumer hardware. This is a critical development because it allows for greater experimentation and adaptability. No longer confined to high-powered data centers, individual developers, small businesses, and start-ups can now work on these models in more resource-constrained environments.<br>
è¿™ç§ä¼˜åŒ–ä½¿å¾—åœ¨æ¶ˆè´¹ç±»ç¡¬ä»¶ä¸Šå¾®è°ƒè¿™äº› LLMs å˜å¾—è¶Šæ¥è¶Šå¯è¡Œã€‚è¿™æ˜¯ä¸€ä¸ªå…³é”®çš„å‘å±•ï¼Œå› ä¸ºå®ƒå…è®¸æ›´å¤§çš„å®éªŒå’Œé€‚åº”æ€§ã€‚ä¸ªäººå¼€å‘äººå‘˜ã€å°å‹ä¼ä¸šå’Œåˆåˆ›ä¼ä¸šä¸å†å±€é™äºé«˜æ€§èƒ½æ•°æ®ä¸­å¿ƒï¼Œç°åœ¨å¯ä»¥åœ¨èµ„æºæ›´å—é™çš„ç¯å¢ƒä¸­ä½¿ç”¨è¿™äº›æ¨¡å‹ã€‚</p>
<h1 id="mistralç±³æ–¯ç‰¹æ‹‰å°”">MistralÂ ç±³æ–¯ç‰¹æ‹‰å°”</h1>
<p>Mistral 7B, a brainchild ofÂ French start-upÂ <a href="https://mistral.ai/">Mistral AI</a>, emerges as a powerhouse in the generative AI domain, with its 7.3 billion parameters making a significant impact. This model is not just about size; itâ€™s about efficiency and capability, promising a bright future for open source large language models and their applicability across a myriad of use cases. The key to its efficiency is the implementation of sliding window attention, a technique released under a permissive Apache open source license. Many AI engineers have fine-tuned on top of this model as a base, including the impressiveÂ <a href="https://oreil.ly/Lg6_r">Zephr 7b beta</a>Â model. There is alsoÂ <a href="https://oreil.ly/itsJG">Mixtral 8x7b</a>, a mixture of experts model (similar to the architecture of GPT-4), which achieves results similar to GPT-3.5-turbo.<br>
Mistral 7B æ˜¯æ³•å›½åˆåˆ›å…¬å¸ Mistral AI çš„å¿ƒè¡€ç»“æ™¶ï¼Œå‡­å€Ÿå…¶ 73 äº¿ä¸ªå‚æ•°äº§ç”Ÿäº†é‡å¤§å½±å“ï¼Œæˆä¸ºç”Ÿæˆå¼ AI é¢†åŸŸçš„å¼ºè€…ã€‚è¿™ä¸ªæ¨¡å‹ä¸ä»…ä»…æ˜¯å°ºå¯¸;å®ƒå…³ä¹æ•ˆç‡å’Œèƒ½åŠ›ï¼Œä¸ºå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åŠå…¶åœ¨æ— æ•°ç”¨ä¾‹ä¸­çš„é€‚ç”¨æ€§å¸¦æ¥äº†å…‰æ˜çš„æœªæ¥ã€‚å…¶æ•ˆç‡çš„å…³é”®æ˜¯å®ç°æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼Œè¿™æ˜¯ä¸€ç§åœ¨å®½æ¾çš„Apacheå¼€æºè®¸å¯ä¸‹å‘å¸ƒçš„æŠ€æœ¯ã€‚è®¸å¤š AI å·¥ç¨‹å¸ˆéƒ½ä»¥æ­¤æ¨¡å‹ä¸ºåŸºç¡€è¿›è¡Œäº†å¾®è°ƒï¼ŒåŒ…æ‹¬ä»¤äººå°è±¡æ·±åˆ»çš„ Zephr 7b æµ‹è¯•ç‰ˆã€‚è¿˜æœ‰ Mixtral 8x7bï¼Œä¸€ä¸ªæ··åˆçš„ä¸“å®¶æ¨¡å‹ï¼ˆç±»ä¼¼äº GPT-4 çš„æ¶æ„ï¼‰ï¼Œå®ƒå®ç°äº†ç±»ä¼¼äº GPT-3.5-turbo çš„ç»“æœã€‚</p>
<p>For a more detailed and up-to-date comparison of open source models and their performance metrics, visit the ChatbotÂ <a href="https://oreil.ly/ttiji">Arena Leaderboard</a>Â hosted by Hugging Face.<br>
æœ‰å…³å¼€æºæ¨¡å‹åŠå…¶æ€§èƒ½æŒ‡æ ‡çš„æ›´è¯¦ç»†å’Œæœ€æ–°æ¯”è¾ƒï¼Œè¯·è®¿é—®ç”± Hugging Face ä¸»åŠçš„ Chatbot Arena æ’è¡Œæ¦œã€‚</p>
<h1 id="anthropic-claudeanthropic-å…‹åŠ³å¾·">Anthropic: ClaudeÂ Anthropicï¼š å…‹åŠ³å¾·</h1>
<p>Released on July 11, 2023,Â <a href="https://claude.ai/login">Claude 2</a>Â is setting itself apart from other prominent LLMs such as ChatGPT and LLaMA, with its pioneeringÂ <a href="https://oreil.ly/Tim9W">Constitutional AI</a>Â approachÂ to AI safety and alignmentâ€”training the model using a list of rules or values. A notable enhancement in Claude 2 was its expanded context window of 100,000 tokens, as well as the ability to upload files. In theÂ realm of generative AI, aÂ <em>context window</em>Â refers to the amount of text or data the model can actively consider or keep in mind when generating a response. With a larger context window, the model can understand and generate based on a broader context.<br>
Claude 2 äº 2023 å¹´ 7 æœˆ 11 æ—¥å‘å¸ƒï¼Œä¸ ChatGPT å’Œ LLaMA ç­‰å…¶ä»–è‘—åçš„ LLMs åŒºåˆ†å¼€æ¥ï¼Œå…¶å¼€åˆ›æ€§çš„ Constitutional AI æ–¹æ³•å®ç°äº† AI å®‰å…¨å’Œå¯¹é½â€”â€”ä½¿ç”¨è§„åˆ™æˆ–å€¼åˆ—è¡¨è®­ç»ƒæ¨¡å‹ã€‚Claude 2 çš„ä¸€ä¸ªæ˜¾ç€æ”¹è¿›æ˜¯å…¶æ‰©å±•çš„ 100,000 ä¸ªä»¤ç‰Œçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œä»¥åŠä¸Šä¼ æ–‡ä»¶çš„èƒ½åŠ›ã€‚åœ¨ç”Ÿæˆå¼ AI é¢†åŸŸï¼Œä¸Šä¸‹æ–‡çª—å£æ˜¯æŒ‡æ¨¡å‹åœ¨ç”Ÿæˆå“åº”æ—¶å¯ä»¥ä¸»åŠ¨è€ƒè™‘æˆ–ç‰¢è®°çš„æ–‡æœ¬æˆ–æ•°æ®é‡ã€‚ä½¿ç”¨æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡è¿›è¡Œç†è§£å’Œç”Ÿæˆã€‚</p>
<p>This advancement garnered significant enthusiasm from AI engineers, as it opened up avenues for new and more intricate use cases. For instance, Claude 2â€™s augmented ability to process more information at once makes it adept at summarizing extensive documents or sustaining in-depth conversations. The advantage was short-lived, as OpenAI released their 128K version of GPT-4 onlyÂ <a href="https://oreil.ly/BWxrn">six months later</a>. However, the fierce competition between rivals is pushing the field forward.<br>
è¿™ä¸€è¿›æ­¥å¼•èµ·äº†äººå·¥æ™ºèƒ½å·¥ç¨‹å¸ˆçš„æå¤§çƒ­æƒ…ï¼Œå› ä¸ºå®ƒä¸ºæ–°çš„å’Œæ›´å¤æ‚çš„ç”¨ä¾‹å¼€è¾Ÿäº†é€”å¾„ã€‚ä¾‹å¦‚ï¼ŒClaude 2 ä¸€æ¬¡å¤„ç†æ›´å¤šä¿¡æ¯çš„èƒ½åŠ›å¢å¼ºï¼Œä½¿å…¶æ“…é•¿æ€»ç»“å¤§é‡æ–‡æ¡£æˆ–è¿›è¡Œæ·±å…¥å¯¹è¯ã€‚è¿™ç§ä¼˜åŠ¿æ˜¯çŸ­æš‚çš„ï¼Œå› ä¸º OpenAI ä»…åœ¨å…­ä¸ªæœˆåå‘å¸ƒäº†ä»–ä»¬çš„ 128K ç‰ˆæœ¬çš„ GPT-4ã€‚ç„¶è€Œï¼Œç«äº‰å¯¹æ‰‹ä¹‹é—´çš„æ¿€çƒˆç«äº‰æ­£åœ¨æ¨åŠ¨è¯¥é¢†åŸŸå‘å‰å‘å±•ã€‚</p>
<p>The next generationÂ of Claude includedÂ <a href="https://oreil.ly/NH0jh">Opus</a>, the first model to rival GPT-4 in terms of intelligence, as well as Haiku, a smaller model that is lightning-fast with the competitive price of $0.25 per million tokens (half the cost of GPT-3.5-turbo at the time).<br>
ä¸‹ä¸€ä»£ Claude åŒ…æ‹¬ Opusï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨æ™ºèƒ½æ–¹é¢ä¸ GPT-4 ç›¸åª²ç¾çš„æ¨¡å‹ï¼Œä»¥åŠ Haikuï¼Œè¿™æ˜¯ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼Œé€Ÿåº¦å¿«å¦‚é—ªç”µï¼Œæ¯ç™¾ä¸‡ä¸ªä»£å¸çš„ç«äº‰ä»·æ ¼ä¸º 0.25 ç¾å…ƒï¼ˆå½“æ—¶æ˜¯ GPT-3.5-turbo æˆæœ¬çš„ä¸€åŠï¼‰ã€‚</p>
<h1 id="gpt-4visiongpt-4vision">GPT-4V(ision)Â GPT-4Vï¼ˆisionï¼‰</h1>
<p>In a significant leapÂ forward, on September 23, 2023, OpenAI expanded the capabilities of GPT-4 with the introduction of Vision, enabling users to instruct GPT-4 to analyze images alongside text. This innovation was also reflected in the update to ChatGPTâ€™s interface, which now supports the inclusion of both images and text as user inputs. This development signifies a major trend towardÂ <em>multimodal models</em>, whichÂ can seamlessly process and understand multiple types of data, such as images and text, within a single context.<br>
2023 å¹´ 9 æœˆ 23 æ—¥ï¼ŒOpenAI é€šè¿‡å¼•å…¥ Vision æ‰©å±•äº† GPT-4 çš„åŠŸèƒ½ï¼Œä½¿ç”¨æˆ·èƒ½å¤ŸæŒ‡ç¤º GPT-4 åˆ†æå›¾åƒå’Œæ–‡æœ¬ã€‚è¿™ä¸€åˆ›æ–°ä¹Ÿåæ˜ åœ¨ ChatGPT ç•Œé¢çš„æ›´æ–°ä¸­ï¼Œè¯¥ç•Œé¢ç°åœ¨æ”¯æŒå°†å›¾åƒå’Œæ–‡æœ¬ä½œä¸ºç”¨æˆ·è¾“å…¥ã€‚è¿™ä¸€å‘å±•æ ‡å¿—ç€å¤šæ¨¡æ€æ¨¡å‹çš„ä¸»è¦è¶‹åŠ¿ï¼Œå®ƒå¯ä»¥åœ¨å•ä¸ªä¸Šä¸‹æ–‡ä¸­æ— ç¼å¤„ç†å’Œç†è§£å¤šç§ç±»å‹çš„æ•°æ®ï¼Œä¾‹å¦‚å›¾åƒå’Œæ–‡æœ¬ã€‚</p>
<h1 id="model-comparisonæ¨¡å‹æ¯”è¾ƒ">Model ComparisonÂ æ¨¡å‹æ¯”è¾ƒ</h1>
<p>The market for LLMs is dominated by OpenAI at the time of writing, with its state-of-the-art GPT-4 model widely considered to have a significant lead. The closest competitor is Anthropic, and there is widespread excitement at the potential of smaller open source models such as Llama and Mistral, particularly with respects to fine-tuning. Although commentators expect OpenAI to continue to deliver world-beating models in the future, as open source models getÂ <em>good enough</em>Â at more tasks, AI workloads may shift toward local fine-tuned models. With advances in model performance and quantization (methods for trading off accuracy versus size and compute cost), it may be possible to one day run LLMs on your mobile phone or other devices.<br>
åœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼ŒLLMs çš„å¸‚åœºç”± OpenAI ä¸»å¯¼ï¼Œå…¶æœ€å…ˆè¿›çš„ GPT-4 æ¨¡å‹è¢«å¹¿æ³›è®¤ä¸ºå…·æœ‰æ˜¾ç€çš„é¢†å…ˆä¼˜åŠ¿ã€‚æœ€æ¥è¿‘çš„ç«äº‰å¯¹æ‰‹æ˜¯ Anthropicï¼Œäººä»¬å¯¹ Llama å’Œ Mistral ç­‰å°å‹å¼€æºæ¨¡å‹çš„æ½œåŠ›æ™®éæ„Ÿåˆ°å…´å¥‹ï¼Œå°¤å…¶æ˜¯åœ¨å¾®è°ƒæ–¹é¢ã€‚å°½ç®¡è¯„è®ºå‘˜é¢„è®¡ OpenAI æœªæ¥å°†ç»§ç»­æä¾›ä¸–ç•Œä¸€æµçš„æ¨¡å‹ï¼Œä½†éšç€å¼€æºæ¨¡å‹åœ¨æ›´å¤šä»»åŠ¡ä¸­å˜å¾—è¶³å¤Ÿå¥½ï¼ŒAI å·¥ä½œè´Ÿè½½å¯èƒ½ä¼šè½¬å‘æœ¬åœ°å¾®è°ƒæ¨¡å‹ã€‚éšç€æ¨¡å‹æ€§èƒ½å’Œé‡åŒ–ï¼ˆæƒè¡¡ç²¾åº¦ä¸å¤§å°å’Œè®¡ç®—æˆæœ¬çš„æ–¹æ³•ï¼‰çš„è¿›æ­¥ï¼Œæœ‰æœä¸€æ—¥æœ‰å¯èƒ½åœ¨æ‰‹æœºæˆ–å…¶ä»–è®¾å¤‡ä¸Šè¿è¡ŒLLMsã€‚</p>
<p>For now, the best way to get a sense for what the models are good at is to run the same prompt across multiple models and compare the responses. One thing that regularly stands out in our work is that GPT-4 is much better at following instructions, as is demonstrated in the following example where it was the only model to respond in the right format, with names that matched the examples (starting with the letterÂ <em>i</em>), as desired.<br>
ç›®å‰ï¼Œäº†è§£æ¨¡å‹æ“…é•¿ä»€ä¹ˆçš„æœ€å¥½æ–¹æ³•æ˜¯åœ¨å¤šä¸ªæ¨¡å‹ä¸­è¿è¡Œç›¸åŒçš„æç¤ºå¹¶æ¯”è¾ƒå“åº”ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ç»å¸¸çªå‡ºçš„ä¸€ä»¶äº‹æ˜¯ï¼ŒGPT-4 åœ¨éµå¾ªæŒ‡ä»¤æ–¹é¢è¦å¥½å¾—å¤šï¼Œæ­£å¦‚ä»¥ä¸‹ç¤ºä¾‹æ‰€ç¤ºï¼Œå®ƒæ˜¯å”¯ä¸€ä»¥æ­£ç¡®æ ¼å¼å“åº”çš„æ¨¡å‹ï¼Œå…¶åç§°ä¸ç¤ºä¾‹åŒ¹é…ï¼ˆä»¥å­—æ¯ i å¼€å¤´ï¼‰ï¼Œæ­£å¦‚æ‰€å¸Œæœ›çš„é‚£æ ·ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Brainstorm a list of product names for a shoe that fits any foot size,
in the style of Steve Jobs.</p>
<p>Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of three product names]</p>
<h2 id="examples-3">Examples</h2>
<p>Product description: A refrigerator that dispenses beer
Product names: iBarFridge, iFridgeBeer, iDrinkBeerFridge</p>
<p>Product description: A watch that can tell accurate time in space
Product names: iNaut, iSpace, iTime</p>
<p>Product description: A home milkshake maker
Product names: iShake, iSmoothie, iShake Mini</p>
<p>Output (GPT-4):Â è¾“å‡º ï¼ˆGPT-4ï¼‰ï¼š</p>
<p>Product description: A shoe that fits any foot size
Product names: iFit, iShoe, iFlexible</p>
<p>Output (Claude 3):Â è¾“å‡º ï¼ˆClaude 3ï¼‰ï¼š</p>
<p>Product description: A shoe that fits any foot size
Product names: iFit, iComfort, iSole</p>
<p>Output (Llama 3 70b):<br>
è¾“å‡º ï¼ˆLlama 3 70bï¼‰ï¼š</p>
<p>Here is the list of product names for a shoe that fits any foot size,
in the style of Steve Jobs:</p>
<p>Product description: A shoe that fits any foot size Product names:
iFit, OneSize, iWalkFree</p>
<h1 id="summaryæ€»ç»“">SummaryÂ æ€»ç»“</h1>
<p>The journey of LLMs from their inception to their current advanced states is a tale of relentless innovation, collaboration, and intense competition. As these models continue to evolve, they are likely to become even more integral parts of our daily lives, changing the way we interact with technology and even with each other.<br>
LLMs ä»æˆç«‹åˆ°ç°åœ¨çš„å…ˆè¿›çŠ¶æ€ï¼Œæ˜¯ä¸€ä¸ªä¸æ‡ˆåˆ›æ–°ã€åä½œå’Œæ¿€çƒˆç«äº‰çš„æ•…äº‹ã€‚éšç€è¿™äº›æ¨¡å‹çš„ä¸æ–­å‘å±•ï¼Œå®ƒä»¬å¯èƒ½ä¼šæˆä¸ºæˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ï¼Œæ”¹å˜æˆ‘ä»¬ä¸æŠ€æœ¯ç”šè‡³å½¼æ­¤äº’åŠ¨çš„æ–¹å¼ã€‚</p>
<p>By understanding the historical context and capabilities of these models, you can better appreciate the tools at our disposal for various applications, from prompt engineering to the development of intelligent virtual agents. Itâ€™s important to note, however, that while these models offer expansive possibilities, data privacy remains a crucial concern. If these models use your data for retraining or fine-tuning, exercise caution and refrain from inputting sensitive information.<br>
é€šè¿‡äº†è§£è¿™äº›æ¨¡å‹çš„å†å²èƒŒæ™¯å’ŒåŠŸèƒ½ï¼Œæ‚¨å¯ä»¥æ›´å¥½åœ°äº†è§£æˆ‘ä»¬ä¸ºå„ç§åº”ç”¨ç¨‹åºæä¾›çš„å·¥å…·ï¼Œä»æç¤ºå·¥ç¨‹åˆ°æ™ºèƒ½è™šæ‹Ÿä»£ç†çš„å¼€å‘ã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶è¿™äº›æ¨¡å‹æä¾›äº†å¹¿æ³›çš„å¯èƒ½æ€§ï¼Œä½†æ•°æ®éšç§ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚å¦‚æœè¿™äº›æ¨¡å‹ä½¿ç”¨æ‚¨çš„æ•°æ®è¿›è¡Œå†è®­ç»ƒæˆ–å¾®è°ƒï¼Œè¯·è°¨æ…è¡Œäº‹ï¼Œä¸è¦è¾“å…¥æ•æ„Ÿä¿¡æ¯ã€‚</p>
<p>In the next chapter, you will learn all the basic prompt engineering techniques for working with text LLMs. Youâ€™ll learn the essential skills needed to get the most out of powerful language models like GPT-4. Exciting insights and practical methods await you as you unlock the true potential of generative AI.<br>
åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¤„ç†æ–‡æœ¬ LLMs çš„æ‰€æœ‰åŸºæœ¬æç¤ºå·¥ç¨‹æŠ€æœ¯ã€‚æ‚¨å°†å­¦ä¹ å……åˆ†åˆ©ç”¨ GPT-4 ç­‰å¼ºå¤§è¯­è¨€æ¨¡å‹æ‰€éœ€çš„åŸºæœ¬æŠ€èƒ½ã€‚æ¿€åŠ¨äººå¿ƒçš„è§è§£å’Œå®ç”¨æ–¹æ³•ç­‰å¾…ç€æ‚¨ï¼Œä¸ºæ‚¨é‡Šæ”¾ç”Ÿæˆå¼ AI çš„çœŸæ­£æ½œåŠ›ã€‚</p>
<h1 id="chapter3-standard-practices-for-text-generation-with-chatgptç¬¬-3-ç« ä½¿ç”¨-chatgpt-ç”Ÿæˆæ–‡æœ¬çš„æ ‡å‡†åšæ³•"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html">Chapter3. Standard Practices For Text Generation With ChatGPT</a>ç¬¬ 3 ç« :ä½¿ç”¨ ChatGPT ç”Ÿæˆæ–‡æœ¬çš„æ ‡å‡†åšæ³•</h1>
<p>Simple prompting techniques will help you to maximize the output and formats from LLMs. Youâ€™ll start by tailoring the prompts to explore all of the common practices used for text generation.</p>
<blockquote>
</blockquote>
<p>ç®€å•çš„æç¤ºæŠ€æœ¯å°†å¸®åŠ©æ‚¨æœ€å¤§åŒ– LLMs çš„è¾“å‡ºå’Œæ ¼å¼ã€‚é¦–å…ˆï¼Œæ‚¨å°†å®šåˆ¶æç¤ºï¼Œä»¥æ¢ç´¢ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„æ‰€æœ‰å¸¸è§åšæ³•ã€‚</p>
<h2 id="generating-listsç”Ÿæˆåˆ—è¡¨">Generating ListsÂ ç”Ÿæˆåˆ—è¡¨</h2>
<p>Automatically generating lists isÂ incredibly powerful and enables you to focus on higher-level tasks while GPT can automatically generate, refine, rerank, and de-duplicate lists on your behalf.</p>
<blockquote>
</blockquote>
<p>è‡ªåŠ¨ç”Ÿæˆåˆ—è¡¨éå¸¸å¼ºå¤§ï¼Œä½¿æ‚¨èƒ½å¤Ÿä¸“æ³¨äºæ›´é«˜çº§åˆ«çš„ä»»åŠ¡ï¼Œè€Œ GPT å¯ä»¥ä»£è¡¨æ‚¨è‡ªåŠ¨ç”Ÿæˆã€ä¼˜åŒ–ã€é‡æ–°æ’åå’Œåˆ é™¤é‡å¤åˆ—è¡¨ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Generate a list of Disney characters<span style="color:#f92672">.</span>
</span></span></code></pre></div><p>Output:Â è¾“å‡ºï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Sure, here <span style="color:#f92672">is</span> a list of some popular Disney characters:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1.</span> Mickey Mouse
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2.</span> Minnie Mouse
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">30.</span> Bagheera (The Jungle Book)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    This will output:Â è¿™å°†è¾“å‡º<span style="color:#960050;background-color:#1e0010">ï¼š</span>
</span></span></code></pre></div><p>GPT-4 is perfectly capable of providing a list of characters. However, there are some pitfalls with this approach:<br>
GPT-4 å®Œå…¨èƒ½å¤Ÿæä¾›å­—ç¬¦åˆ—è¡¨ã€‚ä½†æ˜¯ï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨ä¸€äº›ç¼ºé™·ï¼š</p>
<ul>
<li>
<p>==<em>GPT has decided to provide 30 examples as a numbered list, separated byÂ \nÂ characters. However, if your downstream Python code was expecting to split on bullet points, then youâ€™ll likely end up with undesirable results or a runtime error.==</em></p>
<p>GPT å†³å®šæä¾› 30 ä¸ªç¤ºä¾‹ä½œä¸ºç¼–å·åˆ—è¡¨ï¼Œä»¥Â <code>\n</code>Â å­—ç¬¦åˆ†éš”ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ‚¨çš„ä¸‹æ¸¸ Python ä»£ç æœŸæœ›åœ¨é¡¹ç›®ç¬¦å·ä¸Šæ‹†åˆ†ï¼Œé‚£ä¹ˆæ‚¨æœ€ç»ˆå¯èƒ½ä¼šå¾—åˆ°ä¸å¸Œæœ›çš„ç»“æœæˆ–è¿è¡Œæ—¶é”™è¯¯ã€‚</p>
<ul>
<li>GPT has provided preceding commentary; removing any preceding/succeeding commentary would make parsing the output easier.<br>
GPT æä¾›äº†å…ˆå‰çš„è¯„è®º;åˆ é™¤ä»»ä½•ä¹‹å‰/åé¢çš„æ³¨é‡Šå°†ä½¿è§£æè¾“å‡ºæ›´å®¹æ˜“ã€‚</li>
</ul>
<ul>
<li></li>
</ul>
</li>
</ul>
<p>*==- <em>The list size wasnâ€™t controlled and was left to the language model.==</em><br>
åˆ—è¡¨å¤§å°ä¸å—æ§åˆ¶ï¼Œç•™ç»™è¯­è¨€æ¨¡å‹ã€‚
<em>==- Some of the characters have the name of their corresponding film withinÂ bracketsâ€”forÂ example,Â Bagheera (The Jungle Book)â€”and others donâ€™t. This makes names harder to extract because you would need to remove the movie titles.==</em><br>
æœ‰äº›è§’è‰²åœ¨æ‹¬å·å†…æœ‰ç›¸åº”ç”µå½±çš„åç§°ï¼Œä¾‹å¦‚ï¼ŒBagheeraï¼ˆã€Šä¸›æ—ä¹‹ä¹¦ã€‹ï¼‰ï¼Œè€Œå¦ä¸€äº›åˆ™æ²¡æœ‰ã€‚è¿™ä½¿å¾—åç§°æ›´éš¾æå–ï¼Œå› ä¸ºæ‚¨éœ€è¦åˆ é™¤ç”µå½±æ ‡é¢˜ã€‚</p>
<ul>
<li>==<em>No filtering or selection has been applied to the LLM generation based on our desired result.==</em><br>
æ²¡æœ‰æ ¹æ®æˆ‘ä»¬æƒ³è¦çš„ç»“æœå¯¹LLMä¸€ä»£åº”ç”¨ä»»ä½•è¿‡æ»¤æˆ–é€‰æ‹©ã€‚
Following youâ€™ll find an optimized prompt.<br>
ä¸‹é¢ä½ ä¼šå‘ç°ä¸€ä¸ªä¼˜åŒ–çš„æç¤ºã€‚
Input:Â è¾“å…¥ï¼š</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Generate a bullet<span style="color:#f92672">-</span>point list of <span style="color:#ae81ff">5</span> male Disney characters<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>Only include the name of the character <span style="color:#66d9ef">for</span> each line<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>Never include the film <span style="color:#66d9ef">for</span> each Disney character<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>Only <span style="color:#66d9ef">return</span> the Disney characters, never include any commentary<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Below <span style="color:#f92672">is</span> an example list:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Aladdin
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Simba
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Beast
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Hercules
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Tarzan
</span></span></code></pre></div><p>Output:Â è¾“å‡ºï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">*</span> Woody
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Buzz Lightyear
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Stitch
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Jack Sparrow
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Prince Charming
</span></span></code></pre></div><h3 id="provide-examplesä¸¾ä¾‹è¯´æ˜">PROVIDE EXAMPLESÂ ä¸¾ä¾‹è¯´æ˜</h3>
<p>Simply rephrasing your promptÂ to include examplesÂ <em>(few-shot prompting)</em>Â can greatly impact the desired output.<br>
ç®€å•åœ°æ”¹å†™ä½ çš„æç¤ºä»¥åŒ…å«ç¤ºä¾‹ï¼ˆå°‘é‡æç¤ºï¼‰å¯ä»¥æå¤§åœ°å½±å“æ‰€éœ€çš„è¾“å‡ºã€‚</p>
<p>By optimizing the prompt, youâ€™ve achieved the following:<br>
é€šè¿‡ä¼˜åŒ–æç¤ºï¼Œæ‚¨å·²ç»å®ç°äº†ä»¥ä¸‹ç›®æ ‡ï¼š</p>
<ul>
<li>
<p>Restricted the list to a fixed size of five<br>
å°†åˆ—è¡¨é™åˆ¶ä¸ºå›ºå®šå¤§å° 5</p>
</li>
<li>
<p>Generated only male characters<br>
ä»…ç”Ÿæˆç”·æ€§è§’è‰²</p>
</li>
<li>
<p>Correctly formatted the list with bullet points<br>
ä½¿ç”¨é¡¹ç›®ç¬¦å·æ­£ç¡®è®¾ç½®åˆ—è¡¨æ ¼å¼</p>
</li>
<li>
<p>Removed any preceding commentary<br>
åˆ é™¤äº†ä¹‹å‰çš„ä»»ä½•è¯„è®º</p>
<p>Simple lists are fine for most tasks; however, they are less structured, and for some tasks itâ€™s beneficial to obtain nested dataÂ structures from a GPT-4 output.</p>
</li>
</ul>
<p>ç®€å•çš„åˆ—è¡¨é€‚ç”¨äºå¤§å¤šæ•°ä»»åŠ¡;ä½†æ˜¯ï¼Œå®ƒä»¬çš„ç»“æ„åŒ–ç¨‹åº¦è¾ƒä½ï¼Œå¯¹äºæŸäº›ä»»åŠ¡ï¼Œä» GPT-4 è¾“å‡ºä¸­è·å–åµŒå¥—æ•°æ®ç»“æ„æ˜¯æœ‰ç›Šçš„ã€‚</p>
<p>Three typical data structures include:<br>
ä¸‰ç§å…¸å‹çš„æ•°æ®ç»“æ„åŒ…æ‹¬ï¼š</p>
<ul>
<li>
<p>Nested text data (hierarchical lists)<br>
åµŒå¥—æ–‡æœ¬æ•°æ®ï¼ˆåˆ†å±‚åˆ—è¡¨ï¼‰</p>
</li>
<li>
<p>JSONÂ </p>
</li>
<li>
<p>YAML</p>
</li>
</ul>
<h1 id="hierarchical-list-generation">Hierarchical List Generation</h1>
<p>åˆ†å±‚åˆ—è¡¨ç”Ÿæˆ</p>
<p>Hierarchical lists are useful for whenÂ your desired output is nested. A good example of this would be a detailed article structure.<br>
åˆ†å±‚åˆ—è¡¨åœ¨åµŒå¥—æ‰€éœ€è¾“å‡ºæ—¶éå¸¸æœ‰ç”¨ã€‚ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­æ˜¯è¯¦ç»†çš„æ–‡ç« ç»“æ„ã€‚
Input:Â è¾“å…¥ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Generate a hierarchical <span style="color:#f92672">and</span> incredibly detailed article outline on:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>What are the benefits of data engineering<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>See an example of the hierarchical structure below:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Article Title: What are the benefits of digital marketing<span style="color:#960050;background-color:#1e0010">?</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Introduction
</span></span><span style="display:flex;"><span>    a<span style="color:#f92672">.</span> Explanation of digital marketing
</span></span><span style="display:flex;"><span>    b<span style="color:#f92672">.</span> Importance of digital marketing <span style="color:#f92672">in</span> today<span style="color:#e6db74">&#39;s business world</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Increased Brand Awareness
</span></span><span style="display:flex;"><span>    a<span style="color:#f92672">.</span> Definition of brand awareness
</span></span><span style="display:flex;"><span>    b<span style="color:#f92672">.</span> How digital marketing helps <span style="color:#f92672">in</span> increasing brand awareness
</span></span></code></pre></div><p>Output:Â è¾“å‡ºï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Article Title: What are the benefits of data engineering<span style="color:#960050;background-color:#1e0010">?</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Introduction
</span></span><span style="display:flex;"><span>    a<span style="color:#f92672">.</span> Explanation of data engineering
</span></span><span style="display:flex;"><span>    b<span style="color:#f92672">.</span> Importance of data engineering <span style="color:#f92672">in</span> today<span style="color:#960050;background-color:#1e0010">â€™</span>s data<span style="color:#f92672">-</span>driven world
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>(<span style="color:#ae81ff">10</span> sections later)<span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">*</span> Conclusion
</span></span><span style="display:flex;"><span>    a<span style="color:#f92672">.</span> Importance of data engineering <span style="color:#f92672">in</span> the modern business world
</span></span><span style="display:flex;"><span>    b<span style="color:#f92672">.</span> Future of data engineering <span style="color:#f92672">and</span> its impact on the data ecosystem
</span></span></code></pre></div><p>To generate an effective article outline in the preceding output, youâ€™ve included two key phrases:<br>
è‹¥è¦åœ¨å‰é¢çš„è¾“å‡ºä¸­ç”Ÿæˆæœ‰æ•ˆçš„æ–‡ç« å¤§çº²ï¼Œè¯·åŒ…å«ä¸¤ä¸ªå…³é”®çŸ­è¯­ï¼š
HierarchicalÂ å±‚æ¬¡</p>
<p>To suggest that the articleÂ outline needs to produce a nested structure.<br>
å»ºè®®æ–‡ç« å¤§çº²éœ€è¦äº§ç”ŸåµŒå¥—ç»“æ„ã€‚</p>
<p>Incredibly detailedÂ éš¾ä»¥ç½®ä¿¡çš„ç»†èŠ‚</p>
<p>To guide the language model towards producing a larger output. Other words that you could include that have the same effect would beÂ <em>very long</em>Â or by specifying a large number of subheadings,Â <em>include at least 10 top-level headings</em>.<br>
å¼•å¯¼è¯­è¨€æ¨¡å‹äº§ç”Ÿæ›´å¤§çš„è¾“å‡ºã€‚æ‚¨å¯ä»¥åŒ…å«å…·æœ‰ç›¸åŒæ•ˆæœçš„å…¶ä»–å•è¯ä¼šå¾ˆé•¿ï¼Œæˆ–è€…é€šè¿‡æŒ‡å®šå¤§é‡å‰¯æ ‡é¢˜ï¼Œè‡³å°‘åŒ…æ‹¬ 10 ä¸ªé¡¶çº§æ ‡é¢˜ã€‚</p>
<h6 id="noteæ³¨æ„">NOTEÂ æ³¨æ„</h6>
<p>Asking a language model for a fixed number of items doesnâ€™t guarantee the language model will produce the same length. For example, if you ask for 10 headings, you might receive only 8. Therefore, your code should either validate that 10 headings exist or be flexible to handle varying lengths from the LLM.<br>
å‘è¯­è¨€æ¨¡å‹è¯·æ±‚å›ºå®šæ•°é‡çš„é¡¹å¹¶ä¸èƒ½ä¿è¯è¯­è¨€æ¨¡å‹å°†ç”Ÿæˆç›¸åŒçš„é•¿åº¦ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨è¦æ±‚æä¾› 10 ä¸ªæ ‡é¢˜ï¼Œæ‚¨å¯èƒ½åªæ”¶åˆ° 8 ä¸ªã€‚å› æ­¤ï¼Œæ‚¨çš„ä»£ç åº”è¯¥éªŒè¯æ˜¯å¦å­˜åœ¨ 10 ä¸ªæ ‡é¢˜ï¼Œæˆ–è€…çµæ´»åœ°å¤„ç† LLM çš„ä¸åŒé•¿åº¦ã€‚</p>
<p>So youâ€™ve successfully produced a hierarchical article outline, but how could you parse the stringÂ into structured data?<br>
å› æ­¤ï¼Œæ‚¨å·²ç»æˆåŠŸåœ°ç”Ÿæˆäº†åˆ†å±‚æ–‡ç« å¤§çº²ï¼Œä½†æ˜¯å¦‚ä½•å°†å­—ç¬¦ä¸²è§£æä¸ºç»“æ„åŒ–æ•°æ®ï¼Ÿ</p>
<p>Letâ€™s exploreÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#parsing_a_hierarchical_list_one">ExampleÂ 3-1</a>Â using Python, where youâ€™ve previously made a successful API call against OpenAIâ€™s GPT-4. Two regular expressions are used toÂ extract the headings and subheadings fromÂ <code>openai_result</code>. TheÂ <code>re</code>Â module in Python is used for working with regular expressions.<br>
è®©æˆ‘ä»¬ä½¿ç”¨ Python æ¢ç´¢ç¤ºä¾‹ 3-1ï¼Œæ‚¨ä¹‹å‰å·²æˆåŠŸå¯¹ OpenAI çš„ GPT-4 è¿›è¡Œäº† API è°ƒç”¨ã€‚ä½¿ç”¨ä¸¤ä¸ªæ­£åˆ™è¡¨è¾¾å¼ä»Â <code>openai_result</code>Â ä¸­æå–æ ‡é¢˜å’Œå‰¯æ ‡é¢˜ã€‚Python ä¸­çš„Â <code>re</code>Â æ¨¡å—ç”¨äºå¤„ç†æ­£åˆ™è¡¨è¾¾å¼ã€‚</p>
<h5 id="example-3-1parsing-a-hierarchical-list">Example 3-1.Â <a href="https://oreil.ly/A0otS">Parsing a hierarchical list</a></h5>
<p>ä¾‹ 3-1.åˆ†æåˆ†å±‚åˆ—è¡¨</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> re
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># openai_result = generate_article_outline(prompt)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Commented out to focus on a fake LLM response, see below:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>openai_result <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">* Introduction
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    a. Explanation of data engineering
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    b. Importance of data engineering in todayâ€™s data-driven world
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">* Efficient Data Management
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    a. Definition of data management
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    b. How data engineering helps in efficient data management
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">* Conclusion
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    a. Importance of data engineering in the modern business world
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    b. Future of data engineering and its impact on the data ecosystem
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Regular expression patterns</span>
</span></span><span style="display:flex;"><span>heading_pattern <span style="color:#f92672">=</span> <span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;\* (.+)&#39;</span>
</span></span><span style="display:flex;"><span>subheading_pattern <span style="color:#f92672">=</span> <span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;\s+[a-z]\. (.+)&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Extract headings and subheadings</span>
</span></span><span style="display:flex;"><span>headings <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>findall(heading_pattern, openai_result)
</span></span><span style="display:flex;"><span>subheadings <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>findall(subheading_pattern, openai_result)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Print results</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Headings:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> heading <span style="color:#f92672">in</span> headings:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;* </span><span style="color:#e6db74">{</span>heading<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Subheadings:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> subheading <span style="color:#f92672">in</span> subheadings:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;* </span><span style="color:#e6db74">{</span>subheading<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>This code will output:<br>
æ­¤ä»£ç å°†è¾“å‡ºï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Headings:
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> Introduction
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> Efficient Data Management
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> Conclusion
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Subheadings:
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> Explanation of data engineering
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> Importance of data engineering <span style="color:#f92672">in</span> today<span style="color:#960050;background-color:#1e0010">â€™</span>s data<span style="color:#f92672">-</span>driven world
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> Definition of data management
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> How data engineering helps <span style="color:#f92672">in</span> efficient data management
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> Importance of data engineering <span style="color:#f92672">in</span> the modern business world
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> Future of data engineering <span style="color:#f92672">and</span> its impact on the data ecosystem
</span></span></code></pre></div><p>The use of regular expressions allows for efficientÂ pattern matching, making it possible to handle variations in the input text, such as the presence or absence of leading spaces or tabs. Letâ€™s explore howÂ these patterns work:<br>
ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¯ä»¥è¿›è¡Œæœ‰æ•ˆçš„æ¨¡å¼åŒ¹é…ï¼Œä»è€Œå¯ä»¥å¤„ç†è¾“å…¥æ–‡æœ¬ä¸­çš„å˜ä½“ï¼Œä¾‹å¦‚å‰å¯¼ç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦çš„å­˜åœ¨ä¸å¦ã€‚è®©æˆ‘ä»¬æ¥æ¢è®¨ä¸€ä¸‹è¿™äº›æ¨¡å¼æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span> <span style="color:#960050;background-color:#1e0010">`</span>heading_pattern <span style="color:#f92672">=</span> <span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;\* (.+)&#39;</span><span style="color:#960050;background-color:#1e0010">`</span>
</span></span></code></pre></div><p>This pattern is designed to extract the main headings and consists of:<br>
æ­¤æ¨¡å¼æ—¨åœ¨æå–ä¸»è¦æ ‡é¢˜ï¼ŒåŒ…æ‹¬ï¼š</p>
<ul>
<li>
<p><code>\*</code>Â matches the asteriskÂ <code>(*)</code>Â symbol at the beginning of a heading. The backslash is used to escape the asterisk, as the asterisk has a special meaning in regular expressions (zero or more occurrences of the preceding character).<br>
<code>\*</code>Â ä¸æ ‡é¢˜å¼€å¤´çš„æ˜Ÿå·Â <code>(*)</code>Â ç¬¦å·åŒ¹é…ã€‚åæ–œæ ç”¨äºè½¬ä¹‰æ˜Ÿå·ï¼Œå› ä¸ºæ˜Ÿå·åœ¨æ­£åˆ™è¡¨è¾¾å¼ä¸­å…·æœ‰ç‰¹æ®Šå«ä¹‰ï¼ˆå‰ä¸€ä¸ªå­—ç¬¦å‡ºç°é›¶æ¬¡æˆ–å¤šæ¬¡ï¼‰ã€‚</p>
</li>
<li>
<p>A space character will match after the asterisk.<br>
ç©ºæ ¼å­—ç¬¦å°†åœ¨æ˜Ÿå·ååŒ¹é…ã€‚</p>
</li>
<li>
<p><code>(.+)</code>: matches one or more characters, and the parentheses create a capturing group. TheÂ <code>.</code>Â is a wildcard that matches any character except a newline, and theÂ <code>+</code>Â is a quantifier that meansÂ <em>one or more</em>Â occurrences of the preceding element (the dot, in this case).<br>
<code>(.+)</code>Â ï¼šåŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªå­—ç¬¦ï¼Œæ‹¬å·å†…å°†åˆ›å»ºä¸€ä¸ªæ•è·ç»„ã€‚Â <code>.</code>Â æ˜¯ä¸€ä¸ªé€šé…ç¬¦ï¼Œä¸é™¤æ¢è¡Œç¬¦ä»¥å¤–çš„ä»»ä½•å­—ç¬¦åŒ¹é…ï¼ŒÂ <code>+</code>Â æ˜¯ä¸€ä¸ªé‡è¯ï¼Œè¡¨ç¤ºå‰ä¸€ä¸ªå…ƒç´ ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸ºç‚¹ï¼‰çš„ä¸€æ¬¡æˆ–å¤šæ¬¡å‡ºç°ã€‚</p>
</li>
</ul>
<p>By applying this pattern you can easily extract allÂ of the main headings into a list without the asterisk.<br>
é€šè¿‡åº”ç”¨æ­¤æ¨¡å¼ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å°†æ‰€æœ‰ä¸»è¦æ ‡é¢˜æå–åˆ°ä¸å¸¦æ˜Ÿå·çš„åˆ—è¡¨ä¸­ã€‚</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">`</span>subheading_pattern <span style="color:#f92672">=</span> <span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;\s+[a-z]\. (.+)`</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">&lt;</span>mark style<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;background: #FF5582A6;&#34;</span><span style="color:#f92672">&gt;</span><span style="color:#960050;background-color:#1e0010">```</span>
</span></span><span style="display:flex;"><span>TheÂ <span style="color:#960050;background-color:#1e0010">`</span>subheading pattern<span style="color:#960050;background-color:#1e0010">`</span>Â will <span style="color:#66d9ef">match</span> all of the subheadings within theÂ <span style="color:#960050;background-color:#1e0010">`</span>openai_result<span style="color:#960050;background-color:#1e0010">`</span>Â string:  
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">`</span>subheading pattern<span style="color:#960050;background-color:#1e0010">`</span>Â å°†åŒ¹é…Â <span style="color:#960050;background-color:#1e0010">`</span>openai_result<span style="color:#960050;background-color:#1e0010">`</span>Â å­—ç¬¦ä¸²ä¸­çš„æ‰€æœ‰å‰¯æ ‡é¢˜<span style="color:#960050;background-color:#1e0010">ï¼š</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#960050;background-color:#1e0010">`</span>\s<span style="color:#f92672">+</span><span style="color:#960050;background-color:#1e0010">`</span>Â matches one <span style="color:#f92672">or</span> more whitespace characters (spaces, tabs, <span style="color:#f92672">and</span> so on)<span style="color:#f92672">.</span> TheÂ <span style="color:#960050;background-color:#1e0010">`</span><span style="color:#f92672">+</span><span style="color:#960050;background-color:#1e0010">`</span>Â meansÂ _one <span style="color:#f92672">or</span> more_Â occurrences of the preceding element (theÂ <span style="color:#960050;background-color:#1e0010">`</span>\s<span style="color:#960050;background-color:#1e0010">`</span>, <span style="color:#f92672">in</span> this <span style="color:#66d9ef">case</span>)<span style="color:#f92672">.</span>  
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">`</span>\s<span style="color:#f92672">+</span><span style="color:#960050;background-color:#1e0010">`</span>Â åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªç©ºæ ¼å­—ç¬¦<span style="color:#960050;background-color:#1e0010">ï¼ˆ</span>ç©ºæ ¼<span style="color:#960050;background-color:#1e0010">ã€</span>åˆ¶è¡¨ç¬¦ç­‰<span style="color:#960050;background-color:#1e0010">ï¼‰ã€‚</span>Â <span style="color:#960050;background-color:#1e0010">`</span><span style="color:#f92672">+</span><span style="color:#960050;background-color:#1e0010">`</span>Â è¡¨ç¤ºå‰ä¸€ä¸ªå…ƒç´ çš„ä¸€ä¸ªæˆ–å¤šä¸ªå‡ºç°<span style="color:#960050;background-color:#1e0010">ï¼ˆ</span>åœ¨æœ¬ä¾‹ä¸­ä¸ºÂ <span style="color:#960050;background-color:#1e0010">`</span>\s<span style="color:#960050;background-color:#1e0010">`</span>Â <span style="color:#960050;background-color:#1e0010">ï¼‰ã€‚</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#960050;background-color:#1e0010">`</span>[a<span style="color:#f92672">-</span>z]<span style="color:#960050;background-color:#1e0010">`</span>Â matches a single lowercase letter <span style="color:#f92672">from</span>Â _a_Â toÂ _z_<span style="color:#f92672">.</span>  
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">`</span>[a<span style="color:#f92672">-</span>z]<span style="color:#960050;background-color:#1e0010">`</span>Â åŒ¹é…ä» a åˆ° z çš„å•ä¸ªå°å†™å­—æ¯<span style="color:#960050;background-color:#1e0010">ã€‚</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#960050;background-color:#1e0010">`</span>\<span style="color:#f92672">.</span><span style="color:#960050;background-color:#1e0010">`</span>Â matches a period character<span style="color:#f92672">.</span> The backslash <span style="color:#f92672">is</span> used to escape the period, <span style="color:#66d9ef">as</span> it has a special meaning <span style="color:#f92672">in</span> regular expressions (matches any character <span style="color:#66d9ef">except</span> a newline)<span style="color:#f92672">.</span>  
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">`</span>\<span style="color:#f92672">.</span><span style="color:#960050;background-color:#1e0010">`</span>Â åŒ¹é…å¥ç‚¹å­—ç¬¦<span style="color:#960050;background-color:#1e0010">ã€‚</span>åæ–œæ ç”¨äºè½¬ä¹‰å¥ç‚¹<span style="color:#960050;background-color:#1e0010">ï¼Œ</span>å› ä¸ºå®ƒåœ¨æ­£åˆ™è¡¨è¾¾å¼ä¸­å…·æœ‰ç‰¹æ®Šå«ä¹‰<span style="color:#960050;background-color:#1e0010">ï¼ˆ</span>åŒ¹é…é™¤æ¢è¡Œç¬¦ä»¥å¤–çš„ä»»ä½•å­—ç¬¦<span style="color:#960050;background-color:#1e0010">ï¼‰ã€‚</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> _A space character will <span style="color:#66d9ef">match</span> after the period<span style="color:#f92672">.</span>  
</span></span><span style="display:flex;"><span>    å¥ç‚¹åå°†åŒ¹é…ç©ºæ ¼å­—ç¬¦<span style="color:#960050;background-color:#1e0010">ã€‚</span>_
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#960050;background-color:#1e0010">`</span>(<span style="color:#f92672">.+</span>)<span style="color:#960050;background-color:#1e0010">`</span>Â matches one <span style="color:#f92672">or</span> more characters, <span style="color:#f92672">and</span> the parentheses create a capturing group<span style="color:#f92672">.</span> TheÂ <span style="color:#960050;background-color:#1e0010">`</span><span style="color:#f92672">.</span><span style="color:#960050;background-color:#1e0010">`</span>Â <span style="color:#f92672">is</span> a wildcard that matches any character <span style="color:#66d9ef">except</span> a newline, <span style="color:#f92672">and</span> theÂ <span style="color:#960050;background-color:#1e0010">`</span><span style="color:#f92672">+</span><span style="color:#960050;background-color:#1e0010">`</span>Â <span style="color:#f92672">is</span> a quantifier that meansÂ _one <span style="color:#f92672">or</span> more_Â occurrences of the preceding element (the dot, <span style="color:#f92672">in</span> this <span style="color:#66d9ef">case</span>)<span style="color:#f92672">.</span>  
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">`</span>(<span style="color:#f92672">.+</span>)<span style="color:#960050;background-color:#1e0010">`</span>Â åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªå­—ç¬¦<span style="color:#960050;background-color:#1e0010">ï¼Œ</span>æ‹¬å·å†…å°†åˆ›å»ºä¸€ä¸ªæ•è·ç»„<span style="color:#960050;background-color:#1e0010">ã€‚</span>Â <span style="color:#960050;background-color:#1e0010">`</span><span style="color:#f92672">.</span><span style="color:#960050;background-color:#1e0010">`</span>Â æ˜¯ä¸€ä¸ªé€šé…ç¬¦<span style="color:#960050;background-color:#1e0010">ï¼Œ</span>ä¸é™¤æ¢è¡Œç¬¦ä»¥å¤–çš„ä»»ä½•å­—ç¬¦åŒ¹é…<span style="color:#960050;background-color:#1e0010">ï¼Œ</span>Â <span style="color:#960050;background-color:#1e0010">`</span><span style="color:#f92672">+</span><span style="color:#960050;background-color:#1e0010">`</span>Â æ˜¯ä¸€ä¸ªé‡è¯<span style="color:#960050;background-color:#1e0010">ï¼Œ</span>è¡¨ç¤ºå‰ä¸€ä¸ªå…ƒç´ <span style="color:#960050;background-color:#1e0010">ï¼ˆ</span>åœ¨æœ¬ä¾‹ä¸­ä¸ºç‚¹<span style="color:#960050;background-color:#1e0010">ï¼‰</span>çš„ä¸€æ¬¡æˆ–å¤šæ¬¡å‡ºç°<span style="color:#960050;background-color:#1e0010">ã€‚</span><span style="color:#f92672">&lt;/</span>mark<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Additionally theÂ <span style="color:#960050;background-color:#1e0010">`</span>re<span style="color:#f92672">.</span>findall()<span style="color:#960050;background-color:#1e0010">`</span>Â function <span style="color:#f92672">is</span>Â used to find all non<span style="color:#f92672">-</span>overlapping matches of the patterns <span style="color:#f92672">in</span> the input string <span style="color:#f92672">and</span> <span style="color:#66d9ef">return</span> them <span style="color:#66d9ef">as</span> a list<span style="color:#f92672">.</span> The extracted headings <span style="color:#f92672">and</span> subheadings are then printed<span style="color:#f92672">.</span>  
</span></span><span style="display:flex;"><span>æ­¤å¤–<span style="color:#960050;background-color:#1e0010">ï¼Œ</span>Â <span style="color:#960050;background-color:#1e0010">`</span>re<span style="color:#f92672">.</span>findall()<span style="color:#960050;background-color:#1e0010">`</span>Â å‡½æ•°ç”¨äºæŸ¥æ‰¾è¾“å…¥å­—ç¬¦ä¸²ä¸­æ¨¡å¼çš„æ‰€æœ‰éé‡å åŒ¹é…é¡¹<span style="color:#960050;background-color:#1e0010">ï¼Œ</span>å¹¶å°†å®ƒä»¬ä½œä¸ºåˆ—è¡¨è¿”å›<span style="color:#960050;background-color:#1e0010">ã€‚</span>ç„¶åæ‰“å°æå–çš„æ ‡é¢˜å’Œå‰¯æ ‡é¢˜<span style="color:#960050;background-color:#1e0010">ã€‚</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>So now you<span style="color:#960050;background-color:#1e0010">â€™</span>re able to extract headings <span style="color:#f92672">and</span> subheadings <span style="color:#f92672">from</span> hierarchical article outlines; however, you can further refine the regular expressions so that each heading <span style="color:#f92672">is</span> associated <span style="color:#66d9ef">with</span> correspondingÂ <span style="color:#960050;background-color:#1e0010">`</span>subheadings<span style="color:#960050;background-color:#1e0010">`</span><span style="color:#f92672">.</span>  
</span></span><span style="display:flex;"><span>å› æ­¤<span style="color:#960050;background-color:#1e0010">ï¼Œ</span>ç°åœ¨æ‚¨å¯ä»¥ä»åˆ†å±‚æ–‡ç« å¤§çº²ä¸­æå–æ ‡é¢˜å’Œå‰¯æ ‡é¢˜;ä½†æ˜¯<span style="color:#960050;background-color:#1e0010">ï¼Œ</span>æ‚¨å¯ä»¥è¿›ä¸€æ­¥ç»†åŒ–æ­£åˆ™è¡¨è¾¾å¼<span style="color:#960050;background-color:#1e0010">ï¼Œ</span>ä»¥ä¾¿æ¯ä¸ªæ ‡é¢˜éƒ½ä¸ç›¸åº”çš„Â <span style="color:#960050;background-color:#1e0010">`</span>subheadings<span style="color:#960050;background-color:#1e0010">`</span>Â ç›¸å…³è”<span style="color:#960050;background-color:#1e0010">ã€‚</span>
</span></span><span style="display:flex;"><span>InÂ [ExampleÂ <span style="color:#ae81ff">3</span><span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>](https:<span style="color:#f92672">//</span>learning<span style="color:#f92672">.</span>oreilly<span style="color:#f92672">.</span>com<span style="color:#f92672">/</span>library<span style="color:#f92672">/</span>view<span style="color:#f92672">/</span>prompt<span style="color:#f92672">-</span>engineering<span style="color:#f92672">-</span><span style="color:#66d9ef">for</span><span style="color:#f92672">/</span><span style="color:#ae81ff">9781098153427</span><span style="color:#f92672">/</span>ch03<span style="color:#f92672">.</span>html<span style="color:#75715e">#parsing_a_hierarchical_list_two), the regex has been slightly modified so that each subheading is attached directly with its appropriate subheading.  </span>
</span></span><span style="display:flex;"><span>åœ¨ç¤ºä¾‹ <span style="color:#ae81ff">3</span><span style="color:#f92672">-</span><span style="color:#ae81ff">2</span> ä¸­<span style="color:#960050;background-color:#1e0010">ï¼Œ</span>æ­£åˆ™è¡¨è¾¾å¼ç¨ä½œä¿®æ”¹<span style="color:#960050;background-color:#1e0010">ï¼Œ</span>ä»¥ä¾¿æ¯ä¸ªå­æ ‡é¢˜éƒ½ç›´æ¥é™„åŠ å…¶é€‚å½“çš„å­æ ‡é¢˜<span style="color:#960050;background-color:#1e0010">ã€‚</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">##### Example 3-2.Â [Parsing a hierarchical list into a Python dictionary](https://oreil.ly/LcMtv)  </span>
</span></span><span style="display:flex;"><span>ä¾‹ <span style="color:#ae81ff">3</span><span style="color:#f92672">-</span><span style="color:#ae81ff">2.</span>å°†åˆ†å±‚åˆ—è¡¨è§£æä¸º Python å­—å…¸
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">```</span>python
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> re
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>openai_result <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">* Introduction
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  a. Explanation of data engineering
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  b. Importance of data engineering in todayâ€™s data-driven world
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">* Efficient Data Management
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    a. Definition of data management
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    b. How data engineering helps in efficient data management
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    c. Why data engineering is important for data management
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">* Conclusion
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    a. Importance of data engineering in the modern business world
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    b. Future of data engineering and its impact on the data ecosystem
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>section_regex <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;\* (.+)&#34;</span>)
</span></span><span style="display:flex;"><span>subsection_regex <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;\s*([a-z]\..+)&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>result_dict <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>current_section <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> openai_result<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>):
</span></span><span style="display:flex;"><span>    section_match <span style="color:#f92672">=</span> section_regex<span style="color:#f92672">.</span><span style="color:#66d9ef">match</span>(line)
</span></span><span style="display:flex;"><span>    subsection_match <span style="color:#f92672">=</span> subsection_regex<span style="color:#f92672">.</span><span style="color:#66d9ef">match</span>(line)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> section_match:
</span></span><span style="display:flex;"><span>        current_section <span style="color:#f92672">=</span> section_match<span style="color:#f92672">.</span>group(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        result_dict[current_section] <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> subsection_match <span style="color:#f92672">and</span> current_section <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        result_dict[current_section]<span style="color:#f92672">.</span>append(subsection_match<span style="color:#f92672">.</span>group(<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(result_dict)
</span></span></code></pre></div><p>This will output:Â è¿™å°†è¾“å‡ºï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Introduction&#34;</span>: [
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;a. Explanation of data engineering&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;b. Importance of data engineering in todayâ€™s data-driven world&#34;</span>
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Efficient Data Management&#34;</span>: [
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;a. Definition of data management&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;b. How data engineering helps in efficient data management&#34;</span>
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Conclusion&#34;</span>: [
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;a. Importance of data engineering in the modern business world&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;b. Future of data engineering and its impact on the data ecosystem&#34;</span>
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>The section title regex,Â <code>r'\* (.+)'</code>, matches anÂ asterisk followed by a space and then one or more characters. The parentheses capture the text following the asterisk and space to be used later in the code.<br>
ç« èŠ‚æ ‡é¢˜æ­£åˆ™è¡¨è¾¾å¼Â <code>r'\* (.+)'</code>Â åŒ¹é…æ˜Ÿå·åè·Ÿç©ºæ ¼ï¼Œç„¶åæ˜¯ä¸€ä¸ªæˆ–å¤šä¸ªå­—ç¬¦ã€‚æ‹¬å·æ•è·æ˜Ÿå·åé¢çš„æ–‡æœ¬å’Œç¨ååœ¨ä»£ç ä¸­ä½¿ç”¨çš„ç©ºæ ¼ã€‚</p>
<p>The subsection regex,Â <code>r'\s*([a-z]\..+)'</code>, startsÂ withÂ <code>\s*</code>, which matches zero or more whitespace characters (spaces or tabs). This allows the regex to match subsections with or without leading spaces or tabs. The following part,Â <code>([a-z]\..+)</code>, matches a lowercase letter followed by a period and then one or more characters. The parentheses capture the entire matched subsection text for later use in the code.<br>
å­éƒ¨åˆ†æ­£åˆ™è¡¨è¾¾å¼Â <code>r'\s*([a-z]\..+)'</code>Â ä»¥Â <code>\s*</code>Â å¼€å¤´ï¼Œå®ƒåŒ¹é…é›¶ä¸ªæˆ–å¤šä¸ªç©ºæ ¼å­—ç¬¦ï¼ˆç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦ï¼‰ã€‚è¿™å…è®¸æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¸¦æœ‰æˆ–ä¸å¸¦æœ‰å‰å¯¼ç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦çš„å°èŠ‚ã€‚ä»¥ä¸‹éƒ¨åˆ†Â <code>([a-z]\..+)</code>Â åŒ¹é…ä¸€ä¸ªå°å†™å­—æ¯ï¼Œåè·Ÿå¥ç‚¹ï¼Œç„¶åæ˜¯ä¸€ä¸ªæˆ–å¤šä¸ªå­—ç¬¦ã€‚æ‹¬å·æ•è·æ•´ä¸ªåŒ¹é…çš„å°èŠ‚æ–‡æœ¬ï¼Œä»¥ä¾¿ä»¥ååœ¨ä»£ç ä¸­ä½¿ç”¨ã€‚</p>
<p>TheÂ <code>for</code>Â loop iterates over eachÂ line in the input string,Â <code>openai_result</code>. Upon encountering a line that matches the section title regex, the loop sets the matched title as the current section and assigns an empty list as its value in theÂ <code>result_dict</code>Â dictionary. When a line matches the subsection regex, the matched subsection text is appended to the list corresponding to the current section.<br>
<code>for</code>Â å¾ªç¯éå†è¾“å…¥å­—ç¬¦ä¸²Â <code>openai_result</code>Â ä¸­çš„æ¯ä¸€è¡Œã€‚å½“é‡åˆ°ä¸ç« èŠ‚æ ‡é¢˜æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…çš„è¡Œæ—¶ï¼Œå¾ªç¯ä¼šå°†åŒ¹é…çš„æ ‡é¢˜è®¾ç½®ä¸ºå½“å‰ç« èŠ‚ï¼Œå¹¶åœ¨Â <code>result_dict</code>Â å­—å…¸ä¸­åˆ†é…ä¸€ä¸ªç©ºåˆ—è¡¨ä½œä¸ºå…¶å€¼ã€‚å½“ä¸€è¡Œä¸å°èŠ‚æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ—¶ï¼ŒåŒ¹é…çš„å°èŠ‚æ–‡æœ¬å°†è¿½åŠ åˆ°ä¸å½“å‰èŠ‚å¯¹åº”çš„åˆ—è¡¨ä¸­ã€‚</p>
<p>Consequently, the loop processes theÂ <em>input string line by line</em>, categorizes lines as section titles or subsections, andÂ constructs the intended dictionary structure.<br>
å› æ­¤ï¼Œå¾ªç¯é€è¡Œå¤„ç†è¾“å…¥å­—ç¬¦ä¸²ï¼Œå°†è¡Œåˆ†ç±»ä¸ºéƒ¨åˆ†æ ‡é¢˜æˆ–å­éƒ¨åˆ†ï¼Œå¹¶æ„é€ é¢„æœŸçš„å­—å…¸ç»“æ„ã€‚</p>
<h1 id="when-to-avoid-using-regular-expressions">When to Avoid Using Regular Expressions</h1>
<p>ä½•æ—¶é¿å…ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼</p>
<p>As you work to extract more structuredÂ data from LLM responses, relying solely on regular expressions can make the control flowÂ <em>become increasingly complicated.</em>Â However, there are other formats that can facilitate the parsing of structured data from LLM responses with ease. Two common formats areÂ <em>.json</em>Â andÂ <em>.yml</em>Â files.<br>
å½“æ‚¨åŠªåŠ›ä» LLM å“åº”ä¸­æå–æ›´å¤šç»“æ„åŒ–æ•°æ®æ—¶ï¼Œä»…ä¾èµ–æ­£åˆ™è¡¨è¾¾å¼ä¼šä½¿æ§åˆ¶æµå˜å¾—è¶Šæ¥è¶Šå¤æ‚ã€‚ä½†æ˜¯ï¼Œè¿˜æœ‰å…¶ä»–æ ¼å¼å¯ä»¥æ–¹ä¾¿è½»æ¾åœ°è§£ææ¥è‡ª LLM å“åº”çš„ç»“æ„åŒ–æ•°æ®ã€‚ä¸¤ç§å¸¸è§çš„æ ¼å¼æ˜¯ .json å’Œ .yml æ–‡ä»¶ã€‚</p>
<h1 id="generating-jsonç”Ÿæˆ-json">Generating JSONÂ ç”Ÿæˆ JSON</h1>
<p>Letâ€™s start by experimenting with some promptÂ design that will direct an LLM to return a JSON response.<br>
è®©æˆ‘ä»¬å…ˆå°è¯•ä¸€äº›æç¤ºè®¾è®¡ï¼Œè¿™äº›è®¾è®¡å°†æŒ‡å¯¼ LLM è¿”å› JSON å“åº”ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Compose a very detailed article outline on &ldquo;The benefits of learning code&rdquo; with a
JSON payload structure that highlights key points.</p>
<p>Only return valid JSON.</p>
<p>Here is an example of the JSON structure:
{
&ldquo;Introduction&rdquo;: [
&ldquo;a. Explanation of data engineering&rdquo;,
&ldquo;b. Importance of data engineering in todayâ€™s data-driven world&rdquo;],
&hellip;
&ldquo;Conclusion&rdquo;: [
&ldquo;a. Importance of data engineering in the modern business world&rdquo;,
&ldquo;b. Future of data engineering and its impact on the data ecosystem&rdquo;]
}</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>{
&ldquo;Introduction&rdquo;: [
&ldquo;a. Overview of coding and programming languages&rdquo;,
&ldquo;b. Importance of coding in today&rsquo;s technology-driven world&rdquo;],
&hellip;
&ldquo;Conclusion&rdquo;: [
&ldquo;a. Recap of the benefits of learning code&rdquo;,
&ldquo;b. The ongoing importance of coding skills in the modern world&rdquo;]
}</p>
<h1 id="give-direction-and-provide-examples">GIVE DIRECTION AND PROVIDE EXAMPLES</h1>
<p>ç»™å‡ºæ–¹å‘å¹¶æä¾›ç¤ºä¾‹</p>
<p>Notice that in the preceding prompt, youâ€™veÂ provided direction on the type of task, the format, and an example JSON output.<br>
è¯·æ³¨æ„ï¼Œåœ¨å‰é¢çš„æç¤ºä¸­ï¼Œæ‚¨å·²ç»æä¾›äº†æœ‰å…³ä»»åŠ¡ç±»å‹ã€æ ¼å¼å’Œç¤ºä¾‹ JSON è¾“å‡ºçš„è¯´æ˜ã€‚</p>
<p>Common errors that youâ€™ll encounter when working with JSON involve invalid payloads, or the JSON being wrapped within triple backticks (```) , such as:<br>
ä½¿ç”¨ JSON æ—¶ä¼šé‡åˆ°çš„å¸¸è§é”™è¯¯æ¶‰åŠæ— æ•ˆçš„æœ‰æ•ˆè´Ÿè½½ï¼Œæˆ–è€… JSON è¢«åŒ…è£…åœ¨ä¸‰é‡åå¼•å· ï¼ˆ&rsquo;&rsquo;&rsquo;ï¼‰ ä¸­ï¼Œä¾‹å¦‚ï¼š</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Sure here&rsquo;s the JSON:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{<span style="color:#f92672">&#34;Name&#34;</span>: <span style="color:#e6db74">&#34;John Smith&#34;</span>} <span style="color:#960050;background-color:#1e0010">#</span> <span style="color:#960050;background-color:#1e0010">valid</span> <span style="color:#960050;background-color:#1e0010">payload</span>
</span></span><span style="display:flex;"><span>{<span style="color:#f92672">&#34;Name&#34;</span>: <span style="color:#e6db74">&#34;John Smith&#34;</span>, <span style="color:#f92672">&#34;some_key&#34;</span>:} <span style="color:#960050;background-color:#1e0010">#</span> <span style="color:#960050;background-color:#1e0010">invalid</span> <span style="color:#960050;background-color:#1e0010">payload</span>
</span></span></code></pre></div><p>Ideally you would like the model to respond like so:<br>
ç†æƒ³æƒ…å†µä¸‹ï¼Œæ‚¨å¸Œæœ›æ¨¡å‹çš„å“åº”å¦‚ä¸‹ï¼š</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>{&ldquo;Name&rdquo;: &ldquo;John Smith&rdquo;}</p>
<p>This is important because with the first output, youâ€™d have to split afterÂ <code>json</code>Â and then parse the exact part of the string that contained valid JSON. There are several points that are worth adding to your prompts to improve JSON parsing:<br>
è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºåœ¨ç¬¬ä¸€ä¸ªè¾“å‡ºä¸­ï¼Œå¿…é¡»åœ¨Â <code>json</code>Â ä¹‹åæ‹†åˆ†ï¼Œç„¶åè§£æåŒ…å«æœ‰æ•ˆ JSON çš„å­—ç¬¦ä¸²çš„ç¡®åˆ‡éƒ¨åˆ†ã€‚æœ‰å‡ ç‚¹å€¼å¾—æ·»åŠ åˆ°æ‚¨çš„æç¤ºä¸­ï¼Œä»¥æ”¹è¿› JSON è§£æï¼š</p>
<p>You must follow the following principles:</p>
<ul>
<li>Only return valid JSON</li>
<li>Never include backtick symbols such as: `</li>
<li>The response will be parsed with json.loads(), therefore it must be valid JSON.</li>
</ul>
<p>Now letâ€™s examine how you can parse aÂ <a href="https://oreil.ly/MoJHn">JSON output with Python</a>:<br>
ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ Python è§£æ JSON è¾“å‡ºï¼š</p>
<pre tabindex="0"><code>import
</code></pre><p>Well done, youâ€™ve successfully parsed some JSON.<br>
å¹²å¾—å¥½ï¼Œä½ å·²ç»æˆåŠŸè§£æäº†ä¸€äº›JSONã€‚</p>
<p>As showcased, structuring data from an LLM response is streamlined whenÂ requestingÂ the response in valid JSON format. Compared to the previously demonstrated regular expression parsing, this method is less cumbersome and moreÂ straightforward.<br>
å¦‚å›¾æ‰€ç¤ºï¼Œå½“ä»¥æœ‰æ•ˆçš„ JSON æ ¼å¼è¯·æ±‚å“åº”æ—¶ï¼Œä» LLM å“åº”æ„å»ºæ•°æ®çš„è¿‡ç¨‹ä¼šç®€åŒ–ã€‚ä¸å‰é¢æ¼”ç¤ºçš„æ­£åˆ™è¡¨è¾¾å¼è§£æç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•ä¸é‚£ä¹ˆç¹çï¼Œè€Œä¸”æ›´ç›´æ¥ã€‚</p>
<p>So what could go wrong?<br>
é‚£ä¹ˆä¼šå‡ºä»€ä¹ˆé—®é¢˜å‘¢ï¼Ÿ</p>
<ul>
<li>
<p>The language model accidentally adds extra text to the response such asÂ <code>json output:</code>Â and your application logic only handles for valid JSON.<br>
è¯­è¨€æ¨¡å‹ä¼šæ„å¤–åœ°å‘å“åº”æ·»åŠ é¢å¤–çš„æ–‡æœ¬ï¼Œä¾‹å¦‚Â <code>json output:</code>Â ï¼Œå¹¶ä¸”åº”ç”¨ç¨‹åºé€»è¾‘ä»…å¤„ç†æœ‰æ•ˆçš„ JSONã€‚</p>
</li>
<li>
<p>The JSON produced isnâ€™t valid and fails upon parsing (either due to the size or simply for not escaping certain characters).<br>
ç”Ÿæˆçš„ JSON æ— æ•ˆï¼Œå¹¶ä¸”åœ¨è§£ææ—¶å¤±è´¥ï¼ˆç”±äºå¤§å°æˆ–åªæ˜¯å› ä¸ºæœªè½¬ä¹‰æŸäº›å­—ç¬¦ï¼‰ã€‚</p>
</li>
</ul>
<p>Later on you will examine strategies to gracefully handle for such edge cases.<br>
ç¨åï¼Œæ‚¨å°†ç ”ç©¶ä¼˜é›…åœ°å¤„ç†æ­¤ç±»è¾¹ç¼˜æƒ…å†µçš„ç­–ç•¥ã€‚</p>
<h2 id="yamlyamlå…¬å¸">YAMLÂ YAMLå…¬å¸</h2>
<p><em>.yml</em>Â files are a structured data format that offer different benefits overÂ <em>.json</em>:<br>
.yml æ–‡ä»¶æ˜¯ä¸€ç§ç»“æ„åŒ–æ•°æ®æ ¼å¼ï¼Œä¸.jsonç›¸æ¯”å…·æœ‰ä¸åŒçš„ä¼˜åŠ¿ï¼š</p>
<p>No need to escape characters<br>
æ— éœ€è½¬ä¹‰å­—ç¬¦</p>
<p>YAMLâ€™s indentationÂ pattern eliminates the need for braces, brackets, and commas to denote structure. This can lead to cleaner and less error-prone files, as thereâ€™s less risk of mismatched or misplaced punctuation.<br>
YAML çš„ç¼©è¿›æ¨¡å¼æ¶ˆé™¤äº†ä½¿ç”¨å¤§æ‹¬å·ã€æ‹¬å·å’Œé€—å·æ¥è¡¨ç¤ºç»“æ„çš„éœ€è¦ã€‚è¿™å¯ä»¥ä½¿æ–‡ä»¶æ›´å¹²å‡€ã€æ›´ä¸å®¹æ˜“å‡ºé”™ï¼Œå› ä¸ºæ ‡ç‚¹ç¬¦å·ä¸åŒ¹é…æˆ–æ”¾é”™ä½ç½®çš„é£é™©è¾ƒå°ã€‚</p>
<p>ReadabilityÂ å¯è¯»æ€§</p>
<p>YAML is designed to be human-readable, with a simpler syntax and structure compared to JSON. This makes it easier for you to create, read, and edit prompts, especially when dealing with complex or nested structures.<br>
YAML è¢«è®¾è®¡ä¸ºäººç±»å¯è¯»çš„ï¼Œä¸ JSON ç›¸æ¯”ï¼Œå…·æœ‰æ›´ç®€å•çš„è¯­æ³•å’Œç»“æ„ã€‚è¿™ä½¿æ‚¨å¯ä»¥æ›´è½»æ¾åœ°åˆ›å»ºã€é˜…è¯»å’Œç¼–è¾‘æç¤ºï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æˆ–åµŒå¥—ç»“æ„æ—¶ã€‚</p>
<p>CommentsÂ è¯„è®º</p>
<p>Unlike JSON, YAML supports comments, allowing you to add annotations or explanations to the prompts directly in the file. This can be extremely helpful when working in a team or when revisiting the prompts after some time, as it allows for better understanding and collaboration.<br>
ä¸ JSON ä¸åŒï¼ŒYAML æ”¯æŒæ³¨é‡Šï¼Œå…è®¸æ‚¨ç›´æ¥åœ¨æ–‡ä»¶ä¸­ä¸ºæç¤ºæ·»åŠ æ³¨é‡Šæˆ–è§£é‡Šã€‚è¿™åœ¨å›¢é˜Ÿä¸­å·¥ä½œæˆ–ä¸€æ®µæ—¶é—´åé‡æ–°è®¿é—®æç¤ºæ—¶éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥æ›´å¥½åœ°ç†è§£å’Œåä½œã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<ul>
<li>Below you&rsquo;ll find the current yaml schema.</li>
<li>You can update the quantities based on a User Query.</li>
<li>Filter the User Query based on the schema below, if it doesn&rsquo;t match and
there are no items left then return <code>&quot;No Items&quot;</code>.</li>
<li>If there is a partial match, then return only the items that are
within the schema below:</li>
</ul>
<h1 id="schema">schema:</h1>
<ul>
<li>item: Apple Slices
quantity: 5
unit: pieces</li>
<li>item: Milk
quantity: 1
unit: gallon</li>
<li>item: Bread
quantity: 2
unit: loaves</li>
<li>item: Eggs
quantity: 1
unit: dozen</li>
</ul>
<p>User Query: &ldquo;5 apple slices, and 2 dozen eggs.&rdquo;</p>
<p>Given the schema below, please return only a valid .yml based on the User
Query.If there&rsquo;s no match, return <code>&quot;No Items&quot;</code>. Do not provide any
commentary or explanations.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<ul>
<li>item: Apple Slices
quantity: 5
unit: pieces</li>
<li>item: Eggs
quantity: 2
unit: dozen</li>
</ul>
<p>Notice with the preceding example how an LLMÂ is able to infer the correctÂ <em>.yml</em>Â format from theÂ <code>User Query</code>Â string.<br>
è¯·æ³¨æ„å‰é¢çš„ç¤ºä¾‹ï¼ŒLLM å¦‚ä½•èƒ½å¤Ÿä»Â <code>User Query</code>Â å­—ç¬¦ä¸²æ¨æ–­å‡ºæ­£ç¡®çš„.ymlæ ¼å¼ã€‚</p>
<p>Additionally, youâ€™ve given the LLM an opportunityÂ to either:<br>
æ­¤å¤–ï¼Œæ‚¨è¿˜ä¸º LLM æä¾›äº†ä»¥ä¸‹ä»»ä¸€æœºä¼šï¼š</p>
<ul>
<li>
<p>Return a validÂ <em>.yml</em>Â response<br>
è¿”å›æœ‰æ•ˆçš„.ymlå“åº”</p>
</li>
<li>
<p>Return a filteredÂ <em>.yml</em>Â response<br>
è¿”å›ç­›é€‰çš„.ymlå“åº”</p>
</li>
</ul>
<p>If after filtering, there are noÂ <em>.yml</em>Â items left, then returnÂ <em>No Items</em>.<br>
å¦‚æœç­›é€‰åæ²¡æœ‰å‰©ä½™.ymlé¡¹ï¼Œåˆ™è¿”å›æ— é¡¹ã€‚</p>
<h1 id="filtering-yaml-payloadsç­›é€‰-yaml-æœ‰æ•ˆè´Ÿè½½">Filtering YAML PayloadsÂ ç­›é€‰ YAML æœ‰æ•ˆè´Ÿè½½</h1>
<p>You might decide to use this sameÂ prompt for cleaning/filtering aÂ <em>.yml</em>Â payload.<br>
æ‚¨å¯èƒ½å†³å®šä½¿ç”¨ç›¸åŒçš„æç¤ºæ¥æ¸…ç†/ç­›é€‰.ymlæœ‰æ•ˆè´Ÿè½½ã€‚</p>
<p>First, letâ€™s focus on a payload that contains both valid and invalidÂ <code>schema</code>Â in reference to our desiredÂ <code>schema</code>.Â <code>Apple slices</code>Â fit the criteria; however,Â <code>Bananas</code>Â doesnâ€™t exist, and you should expect for theÂ <code>User Query</code>Â to be appropriately filtered.<br>
é¦–å…ˆï¼Œè®©æˆ‘ä»¬å…³æ³¨ä¸€ä¸ªæœ‰æ•ˆè´Ÿè½½ï¼Œå®ƒåŒæ—¶åŒ…å«æœ‰æ•ˆå’Œæ— æ•ˆçš„Â <code>schema</code>Â ä»¥å¼•ç”¨æˆ‘ä»¬æƒ³è¦çš„Â <code>schema</code>Â ã€‚Â <code>Apple slices</code>Â ç¬¦åˆæ ‡å‡†;ä½†æ˜¯ï¼ŒÂ <code>Bananas</code>Â ä¸å­˜åœ¨ï¼Œæ‚¨åº”è¯¥æœŸæœ›Â <code>User Query</code>Â è¢«é€‚å½“è¿‡æ»¤ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<h1 id="user-query">User Query:</h1>
<ul>
<li>item: Apple Slices
quantity: 5
unit: pieces</li>
<li>item: Bananas
quantity: 3
unit: pieces</li>
</ul>
<p>Output:Â è¾“å‡ºï¼š</p>
<h1 id="updated-yaml-list">Updated yaml list</h1>
<ul>
<li>item: Apple Slices
quantity: 5
unit: pieces</li>
</ul>
<p>In the preceding example, youâ€™ve successfully filtered the userâ€™s payload against a set criteria and have used the language model as aÂ <em>reasoning engine</em>.<br>
åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­ï¼Œä½ å·²æˆåŠŸæ ¹æ®è®¾ç½®çš„æ¡ä»¶ç­›é€‰äº†ç”¨æˆ·çš„æœ‰æ•ˆè´Ÿè½½ï¼Œå¹¶å°†è¯­è¨€æ¨¡å‹ç”¨ä½œæ¨ç†å¼•æ“ã€‚</p>
<p>By providing the LLM with a set of instructions within the prompt, the response is closely related to what a human might do if they were manually cleaning the data.<br>
é€šè¿‡åœ¨æç¤ºä¸­å‘ LLM æä¾›ä¸€ç»„æŒ‡ä»¤ï¼Œå“åº”ä¸æ‰‹åŠ¨æ¸…ç†æ•°æ®æ—¶äººç±»å¯èƒ½æ‰§è¡Œçš„æ“ä½œå¯†åˆ‡ç›¸å…³ã€‚</p>
<p>The input prompt facilitates theÂ delegation of more control flow tasks to a language learning model (LLM), tasks that would typically require coding in a programming language like Python or JavaScript.<br>
è¾“å…¥æç¤ºæœ‰åŠ©äºå°†æ›´å¤šæ§åˆ¶æµä»»åŠ¡å§”æ´¾ç»™è¯­è¨€å­¦ä¹ æ¨¡å‹ ï¼ˆLLMï¼‰ï¼Œè¿™äº›ä»»åŠ¡é€šå¸¸éœ€è¦ä½¿ç”¨ Python æˆ– JavaScript ç­‰ç¼–ç¨‹è¯­è¨€è¿›è¡Œç¼–ç ã€‚</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#figure-3-1">FigureÂ 3-1</a>Â provides a detailed overview of theÂ logic applied when processing user queries by an LLM.<br>
å›¾ 3-1 è¯¦ç»†ä»‹ç»äº†åœ¨ LLM å¤„ç†ç”¨æˆ·æŸ¥è¯¢æ—¶åº”ç”¨çš„é€»è¾‘ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0301.png" alt="Using an LLM to determine the control flow of an application instead of directly using code."></p>
<h6 id="figure-3-1using-an-llm-to-determine-the-control-flow-of-an-application-instead-of-code">Figure 3-1.Â Using an LLM to determine the control flow of an application instead of code</h6>
<p>å›¾ 3-1ã€‚ä½¿ç”¨ LLM ç¡®å®šåº”ç”¨ç¨‹åºçš„æ§åˆ¶æµï¼Œè€Œä¸æ˜¯ä»£ç </p>
<h1 id="handling-invalid-payloads-in-yaml">Handling Invalid Payloads in YAML</h1>
<p>åœ¨ YAML ä¸­å¤„ç†æ— æ•ˆçš„è´Ÿè½½</p>
<p>A completely invalid payloadÂ might look like this:<br>
å®Œå…¨æ— æ•ˆçš„æœ‰æ•ˆè´Ÿè½½å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<h1 id="user-query-1">User Query:</h1>
<ul>
<li>item: Bananas
quantity: 3
unit: pieces</li>
</ul>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>No Items</p>
<p>As expected, the LLM returnedÂ <code>No Items</code>Â as none of theÂ <code>User Query</code>Â items matched against the previously definedÂ <code>schema</code>.<br>
æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼ŒLLM è¿”å›Â <code>No Items</code>Â ï¼Œå› ä¸ºÂ <code>User Query</code>Â é¡¹ä¸å…ˆå‰å®šä¹‰çš„Â <code>schema</code>Â ä¸åŒ¹é…ã€‚</p>
<p>Letâ€™s create a Python script that gracefully accommodates for the various types of LLM results returned. The core parts of the script will focus on:<br>
è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª Python è„šæœ¬ï¼Œè¯¥è„šæœ¬å¯ä»¥æ­£å¸¸é€‚åº”è¿”å›çš„å„ç§ç±»å‹çš„ LLM ç»“æœã€‚è„šæœ¬çš„æ ¸å¿ƒéƒ¨åˆ†å°†ä¾§é‡äºï¼š</p>
<ul>
<li>
<p>Creating custom exceptions for each type of error that might occur due to the three LLM response scenarios<br>
ä¸ºç”±äºä¸‰ç§ LLM å“åº”æ–¹æ¡ˆè€Œå¯èƒ½å‘ç”Ÿçš„æ¯ç§ç±»å‹çš„é”™è¯¯åˆ›å»ºè‡ªå®šä¹‰å¼‚å¸¸</p>
</li>
<li>
<p>Parsing the proposed schema<br>
è§£æå»ºè®®çš„æ¶æ„</p>
</li>
<li>
<p>Running a serious of custom checks against the response so you can be sure that the YML response can be safely passed to downstream softwareÂ applications/microservices<br>
å¯¹å“åº”è¿›è¡Œä¸¥æ ¼çš„è‡ªå®šä¹‰æ£€æŸ¥ï¼Œä»¥ç¡®ä¿ YML å“åº”å¯ä»¥å®‰å…¨åœ°ä¼ é€’åˆ°ä¸‹æ¸¸è½¯ä»¶åº”ç”¨ç¨‹åº/å¾®æœåŠ¡</p>
</li>
</ul>
<p>You could define six specific errors that would handle for all of the edge cases:<br>
æ‚¨å¯ä»¥å®šä¹‰å…­ä¸ªç‰¹å®šé”™è¯¯ï¼Œä»¥å¤„ç†æ‰€æœ‰è¾¹ç¼˜æƒ…å†µï¼š</p>
<pre tabindex="0"><code>class
</code></pre><p>Then provide the previously proposedÂ <code>YML schema</code>Â as a string:<br>
ç„¶åå°†å‰é¢å»ºè®®çš„Â <code>YML schema</code>Â ä½œä¸ºå­—ç¬¦ä¸²æä¾›ï¼š</p>
<pre tabindex="0"><code># Provided schema
</code></pre><p>Import theÂ <code>yaml</code>Â module and create a customÂ parser function calledÂ <code>validate_``response</code>Â that allows you to easily determine whether an LLM output is valid:<br>
å¯¼å…¥Â <code>yaml</code>Â æ¨¡å—å¹¶åˆ›å»ºä¸€ä¸ªåä¸ºÂ <code>validate_</code>Â <code>response</code>Â çš„è‡ªå®šä¹‰è§£æå™¨å‡½æ•°ï¼Œè¯¥å‡½æ•°å…è®¸æ‚¨è½»æ¾ç¡®å®š LLM è¾“å‡ºæ˜¯å¦æœ‰æ•ˆï¼š</p>
<pre tabindex="0"><code>import
</code></pre><p>To test these edge cases, following youâ€™ll find severalÂ mocked LLM responses:<br>
ä¸ºäº†æµ‹è¯•è¿™äº›è¾¹ç¼˜æƒ…å†µï¼Œä½ ä¼šå‘ç°å‡ ä¸ªè¢«å˜²ç¬‘çš„LLMå“åº”ï¼š</p>
<pre tabindex="0"><code># Fake responses
</code></pre><p>Finally, now you can:<br>
æœ€åï¼Œç°åœ¨æ‚¨å¯ä»¥ï¼š</p>
<ul>
<li>
<p>UseÂ <code>yaml.safe_load(response)</code>Â to safely parse theÂ <em>.yml</em>Â schema<br>
ä½¿ç”¨Â <code>yaml.safe_load(response)</code>Â å®‰å…¨åœ°è§£æ .yml æ¶æ„</p>
</li>
<li>
<p>Call theÂ <code>validate_response</code>Â function for each LLM responseÂ to test it against customÂ <em>.yml</em>Â validation logic<br>
ä¸ºæ¯ä¸ª LLM å“åº”è°ƒç”¨Â <code>validate_response</code>Â å‡½æ•°ï¼Œä»¥æ ¹æ®è‡ªå®šä¹‰.ymléªŒè¯é€»è¾‘å¯¹å…¶è¿›è¡Œæµ‹è¯•</p>
</li>
</ul>
<pre tabindex="0"><code># Parse the fake responses
</code></pre><h1 id="diverse-format-generation-with-chatgpt">Diverse Format Generation with ChatGPT</h1>
<p>ä½¿ç”¨ ChatGPT ç”Ÿæˆå¤šç§æ ¼å¼</p>
<p>ChatGPT is not only capable of generatingÂ <em>.json</em>Â orÂ <em>.yml</em>Â files but can also support many other dataÂ formats. For example, it can aid you in creating a mermaid diagram or generating a mockÂ <em>.csv</em>Â document. Its versatility stretches to even generating code, conversations, and scripts.<br>
ChatGPT ä¸ä»…èƒ½å¤Ÿç”Ÿæˆ.jsonæˆ–.ymlæ–‡ä»¶ï¼Œè¿˜å¯ä»¥æ”¯æŒè®¸å¤šå…¶ä»–æ•°æ®æ ¼å¼ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥å¸®åŠ©æ‚¨åˆ›å»ºç¾äººé±¼å›¾æˆ–ç”Ÿæˆæ¨¡æ‹Ÿ.csvæ–‡æ¡£ã€‚å®ƒçš„å¤šåŠŸèƒ½æ€§ç”šè‡³å¯ä»¥ç”Ÿæˆä»£ç ã€å¯¹è¯å’Œè„šæœ¬ã€‚</p>
<p>Letâ€™s explore in detail how ChatGPT can produce diverse formatted content. Mermaid is a markdown-like scripting language for generating charts, diagrams, and other visualizations.<br>
è®©æˆ‘ä»¬è¯¦ç»†æ¢è®¨ ChatGPT å¦‚ä½•ç”Ÿæˆå¤šæ ·åŒ–çš„æ ¼å¼å†…å®¹ã€‚Mermaid æ˜¯ä¸€ç§ç±»ä¼¼ Markdown çš„è„šæœ¬è¯­è¨€ï¼Œç”¨äºç”Ÿæˆå›¾è¡¨ã€å›¾è¡¨å’Œå…¶ä»–å¯è§†åŒ–æ•ˆæœã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Create a flowchart diagram in mermaid syntax showing a simple food ordering
process that starts with &ldquo;Choose food&rdquo; and ends with &ldquo;Pay for meal.&rdquo;
Include steps for &ldquo;Add food to cart&rdquo; and &ldquo;Confirm cart.&rdquo;</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>graph TD
ChooseFood[Choose Food] &ndash;&gt; AddToCart[Add to Cart]
AddToCart &ndash;&gt; ConfirmCart[Confirm Cart]
ConfirmCart &ndash;&gt; PayForMeal[Pay for Meal]</p>
<p>This output, when input to a mermaid diagram renderer, will create a precise flowchart reflecting the food ordering process (<a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#figure-3-2">FigureÂ 3-2</a>).<br>
å½“è¾“å…¥åˆ°ç¾äººé±¼å›¾æ¸²æŸ“å™¨æ—¶ï¼Œæ­¤è¾“å‡ºå°†åˆ›å»ºä¸€ä¸ªåæ˜ é£Ÿç‰©è®¢è´­è¿‡ç¨‹çš„ç²¾ç¡®æµç¨‹å›¾ï¼ˆå›¾ 3-2ï¼‰ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0302.png" alt="A simple mermaid diagram output from an LLM."></p>
<h6 id="figure-3-2a-streamlined-flow-diagram-created-using-mermaid-syntax-via-an-llm">Figure 3-2.Â A streamlined flow diagram created using mermaid syntax via an LLM</h6>
<p>å›¾ 3-2ã€‚é€šè¿‡ @1001 ä½¿ç”¨ mermaid è¯­æ³•åˆ›å»ºçš„ç®€åŒ–æµç¨‹å›¾#</p>
<h2 id="mock-csv-dataæ¨¡æ‹Ÿ-csv-æ•°æ®">Mock CSV DataÂ æ¨¡æ‹Ÿ CSV æ•°æ®</h2>
<p>ChatGPT can also generate mockÂ CSV data that can be utilized for testing purposes or other uses.<br>
ChatGPT è¿˜å¯ä»¥ç”Ÿæˆæ¨¡æ‹Ÿ CSV æ•°æ®ï¼Œå¯ç”¨äºæµ‹è¯•ç›®çš„æˆ–å…¶ä»–ç”¨é€”ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Generate a sample CSV data for five students with the following fields:
name, age, grade.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>name,age,grade
John,16,A
Alex,17,B
Emma,16,A
Sophia,16,B
Liam,17,A</p>
<p>The example generated is a well-formatted CSV file, which can be loaded directly into many tools that read CSV data.<br>
ç”Ÿæˆçš„ç¤ºä¾‹æ˜¯ä¸€ä¸ªæ ¼å¼è‰¯å¥½çš„ CSV æ–‡ä»¶ï¼Œå¯ä»¥ç›´æ¥åŠ è½½åˆ°è®¸å¤šè¯»å– CSV æ•°æ®çš„å·¥å…·ä¸­ã€‚</p>
<p>Remember, GPT models rely heavily on the format you specify. For instance, specifying CSV implies the AI should use commas as separators. Be clear about the desired format for optimal results.<br>
è¯·è®°ä½ï¼ŒGPT æ¨¡å‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæ‚¨æŒ‡å®šçš„æ ¼å¼ã€‚ä¾‹å¦‚ï¼ŒæŒ‡å®š CSV æ„å‘³ç€ AI åº”ä½¿ç”¨é€—å·ä½œä¸ºåˆ†éš”ç¬¦ã€‚æ˜ç¡®æ‰€éœ€çš„æ ¼å¼ä»¥è·å¾—æœ€ä½³ç»“æœã€‚</p>
<p>Youâ€™ve explored different ways of extracting structured data from language models including regular expressions, JSON, YML, and other formats. You can also now use LLMs to direct parts ofÂ your applicationâ€™s control flow.<br>
æ‚¨å·²ç»æ¢ç´¢äº†ä»è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬æ­£åˆ™è¡¨è¾¾å¼ã€JSONã€YML å’Œå…¶ä»–æ ¼å¼ï¼‰ä¸­æå–ç»“æ„åŒ–æ•°æ®çš„ä¸åŒæ–¹æ³•ã€‚ç°åœ¨ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨ LLMs æ¥æŒ‡å¯¼åº”ç”¨ç¨‹åºæ§åˆ¶æµçš„æŸäº›éƒ¨åˆ†ã€‚</p>
<h1 id="explain-it-like-im-five">Explain It like Iâ€™m Five</h1>
<p>åƒæˆ‘äº”å²ä¸€æ ·è§£é‡Šå®ƒ</p>
<p>TheÂ <em>Explain It like Iâ€™m Five</em>Â prompt focuses on explaining a given section of textÂ <em>as if you were talking to a five-year-old child.</em><br>
â€œåƒæˆ‘äº”å²ä¸€æ ·è§£é‡Šå®ƒâ€æç¤ºä¾§é‡äºè§£é‡Šç»™å®šçš„æ–‡æœ¬éƒ¨åˆ†ï¼Œå°±å¥½åƒä½ åœ¨å’Œä¸€ä¸ªäº”å²çš„å­©å­è¯´è¯ä¸€æ ·ã€‚</p>
<p>This style of prompting is veryÂ effective and is a great way to convert technical documents into simple summaries that anyone can understand.<br>
è¿™ç§æç¤ºæ–¹å¼éå¸¸æœ‰æ•ˆï¼Œæ˜¯å°†æŠ€æœ¯æ–‡æ¡£è½¬æ¢ä¸ºä»»ä½•äººéƒ½å¯ä»¥ç†è§£çš„ç®€å•æ‘˜è¦çš„å¥½æ–¹æ³•ã€‚</p>
<p>Letâ€™s apply this prompt to an abstract ofÂ <a href="https://oreil.ly/MDOsz">â€œTumor Microenvironment: Recent Advances in Various Cancer Treatmentsâ€</a>.<br>
è®©æˆ‘ä»¬å°†è¿™ä¸ªæç¤ºåº”ç”¨äºâ€œè‚¿ç˜¤å¾®ç¯å¢ƒï¼šå„ç§ç™Œç—‡æ²»ç–—çš„æœ€æ–°è¿›å±•â€çš„æ‘˜è¦ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Explain the following text as if I was 5 years old. Explain it like I&rsquo;m 5.</p>
<p>Text: This is a review regarding different types of cancer treatments.
We aimed at analyzing the tumor microenvironment and the recent trends
for the therapeutic applications and effectiveness for several kinds
of cancers. Traditionally the cancer treatment was based on the
neoplastic cells.</p>
<p>&hellip;(shortened for brevity)&hellip;</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Cancer is a sickness that makes your body&rsquo;s cells grow and divide in a way
that is not normal. Doctors have different ways to treat cancer, like
cutting it out, giving you medicine, or using special rays to kill the
cancer cells&hellip;</p>
<p>The ChatGPT output from the precedingÂ example is much more digestible than the abstract.<br>
å‰é¢ç¤ºä¾‹çš„ ChatGPT è¾“å‡ºæ¯”æ‘˜è¦æ›´å®¹æ˜“ç†è§£ã€‚</p>
<h1 id="universal-translation-through-llms">Universal Translation Through LLMs</h1>
<p>é€šè¿‡LLMsè¿›è¡Œé€šç”¨ç¿»è¯‘</p>
<p>Additionally, language models canÂ potentially act asÂ <em>universal translators</em>Â due to their proficiency in understanding and generating human-like text across multiple languages.<br>
æ­¤å¤–ï¼Œç”±äºè¯­è¨€æ¨¡å‹èƒ½å¤Ÿç†Ÿç»ƒåœ°ç†è§£å’Œç”Ÿæˆè·¨å¤šç§è¯­è¨€çš„ç±»ä¼¼äººç±»çš„æ–‡æœ¬ï¼Œå› æ­¤å¯ä»¥å……å½“é€šç”¨ç¿»è¯‘å™¨ã€‚</p>
<p>Letâ€™s investigate this with a simple example:<br>
è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼š</p>
<ol>
<li>
<p>Generate some simple text.<br>
ç”Ÿæˆä¸€äº›ç®€å•çš„æ–‡æœ¬ã€‚</p>
</li>
<li>
<p>Change the simple text and make it difficult to read.<br>
æ›´æ”¹ç®€å•çš„æ–‡æœ¬å¹¶ä½¿å…¶éš¾ä»¥é˜…è¯»ã€‚</p>
</li>
<li>
<p>Then transform the same text back into an easy to read format in Spanish (instead of English).<br>
ç„¶åå°†ç›¸åŒçš„æ–‡æœ¬è½¬æ¢å›æ˜“äºé˜…è¯»çš„è¥¿ç­ç‰™è¯­æ ¼å¼ï¼ˆè€Œä¸æ˜¯è‹±è¯­ï¼‰ã€‚</p>
</li>
<li>
<p>Revert the simplistic text back into English.<br>
å°†ç®€å•çš„æ–‡æœ¬æ¢å¤ä¸ºè‹±æ–‡ã€‚</p>
</li>
</ol>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Create some simple text.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>The sun gently rises, painting the sky with hues of orange and pink. Birds
chirp merrily as they welcome the new day. The cool breeze whispers through
the trees, carrying the scent of fresh grass and blooming flowers. Life
awakens with a sense of tranquility and promise.</p>
<p>&hellip;(shortened for brevity)&hellip;</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Now make the text incredibly complex and hard to read.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>In the ethereal realm of celestial ascendancy, wherein the resplendent
orb of the sun commences its gradual ascent, an exquisite phenomenon
unfolds,engendering a symphony of chromatic brilliance upon the
expansive canopy of the firmament. Avian denizens, endowed with
mellifluous voices, bespeak their jubilant salutations, punctuating
the aural tapestry with notes of felicity. The zephyr, tenderly
wafting amidst arboreal sentinels, carries upon its gentle currents
the redolence of verdant blades and blossoming flora.
&hellip;(shortened for brevity)&hellip;</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Now transform the text into a simple, easy to read format that&rsquo;s in Spanish
, make sure that it is very easy to understand and read. You must ensure
that it is as simple to comprehend as possible.</p>
<ul>
<li>Use simple language.</li>
<li>Make it concise.</li>
</ul>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>En un lugar mÃ¡gico y celestial, cuando el sol empieza a salir, ocurre algo
hermoso en el cielo. Los pÃ¡jaros cantan alegremente, llenando el aire con
sus dulces melodÃ­as. Una brisa suave sopla entre los Ã¡rboles, llevando
consigo el olor fresco de la naturaleza. Todo cobra vida y promete momentos
felices. Nos invita a disfrutar de las cosas simples y a encontrar la
felicidad en los pequeÃ±os detalles. En este escenario encantador,
encontramos paz y aprendemos que hasta los momentos mÃ¡s simples pueden
ser mÃ¡gicos.</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Finally translate it back into English.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>In a magical and celestial place, when the sun begins to rise, something
beautiful happens in the sky. The birds sing cheerfully, filling the air
with their sweet melodies. A gentle breeze blows among the trees, carrying
the fresh scent of nature. Everything comes alive and promises happy
moments. It invites us to enjoy the simple things and find happiness in the
little details. In this enchanting setting, we find peace and learn that
even the simplest moments can be magical.</p>
<p>As you can see, part of the meaning is lost while doing the translation, and this is to be expected. However, large parts of the original textâ€™s intent remains, demonstrating that LLMs can act asÂ <em>highly effective translators</em>Â for languages that have adequate resources on the internet. For languages that do not have a good amount of resources, it will yield bad results.<br>
æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œåœ¨è¿›è¡Œç¿»è¯‘æ—¶ï¼Œéƒ¨åˆ†å«ä¹‰ä¼šä¸¢å¤±ï¼Œè¿™æ˜¯æ„æ–™ä¹‹ä¸­çš„ã€‚ç„¶è€Œï¼ŒåŸæ–‡çš„å¤§éƒ¨åˆ†æ„å›¾ä»ç„¶å­˜åœ¨ï¼Œè¿™è¡¨æ˜LLMså¯ä»¥ä½œä¸ºäº’è”ç½‘ä¸Šæ‹¥æœ‰è¶³å¤Ÿèµ„æºçš„è¯­è¨€çš„é«˜æ•ˆç¿»è¯‘å™¨ã€‚å¯¹äºæ²¡æœ‰å¤§é‡èµ„æºçš„è¯­è¨€ï¼Œå®ƒä¼šäº§ç”Ÿä¸å¥½çš„ç»“æœã€‚</p>
<p>The same logic applies to coding languages. LLMs are very good at generating code for established programming languages such as Python and JavaScript but perform worse for newer coding languages and packages.<br>
åŒæ ·çš„é€»è¾‘ä¹Ÿé€‚ç”¨äºç¼–ç è¯­è¨€ã€‚LLMs éå¸¸æ“…é•¿ä¸ºå·²å»ºç«‹çš„ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ Python å’Œ JavaScriptï¼‰ç”Ÿæˆä»£ç ï¼Œä½†å¯¹äºè¾ƒæ–°çš„ç¼–ç è¯­è¨€å’ŒåŒ…æ¥è¯´ï¼Œæ€§èƒ½è¾ƒå·®ã€‚</p>
<p>The boundaries between different forms ofÂ information are becomingÂ <em>increasingly fluid</em>. The essence of information itself is evolving, allowing for effortless transformations of summaries into stories, poems, or other creative expressions, ultimately enriching our understanding and engagement with the content.<br>
ä¸åŒå½¢å¼ä¿¡æ¯ä¹‹é—´çš„ç•Œé™æ­£å˜å¾—è¶Šæ¥è¶Šæ¨¡ç³Šã€‚ä¿¡æ¯æœ¬èº«çš„æœ¬è´¨æ˜¯ä¸æ–­å‘å±•çš„ï¼Œå¯ä»¥æ¯«ä¸è´¹åŠ›åœ°å°†æ‘˜è¦è½¬åŒ–ä¸ºæ•…äº‹ã€è¯—æ­Œæˆ–å…¶ä»–åˆ›é€ æ€§è¡¨è¾¾ï¼Œæœ€ç»ˆä¸°å¯Œæˆ‘ä»¬å¯¹å†…å®¹çš„ç†è§£å’Œå‚ä¸ã€‚</p>
<p><em>Diffusion models</em>Â are a unique class ofÂ generative models utilized in machine learning, specifically designed to produce new images that mimic those found in the training set.<br>
æ‰©æ•£æ¨¡å‹æ˜¯æœºå™¨å­¦ä¹ ä¸­ä½¿ç”¨çš„ä¸€ç±»ç‹¬ç‰¹çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¸“é—¨è®¾è®¡ç”¨äºç”Ÿæˆæ¨¡ä»¿è®­ç»ƒé›†ä¸­å‘ç°çš„æ–°å›¾åƒã€‚</p>
<p>Moreover, when you combine language models withÂ diffusion models, it enables seamless transitions between text, video, and other modalities. This makes it even simpler for you to conveyÂ complex ideas across various formats, facilitating a more accessible and comprehensive experience.<br>
æ­¤å¤–ï¼Œå½“æ‚¨å°†è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆæ—¶ï¼Œå®ƒå¯ä»¥åœ¨æ–‡æœ¬ã€è§†é¢‘å’Œå…¶ä»–æ¨¡æ€ä¹‹é—´å®ç°æ— ç¼è½¬æ¢ã€‚è¿™ä½¿æ‚¨å¯ä»¥æ›´è½»æ¾åœ°ä»¥å„ç§æ ¼å¼ä¼ è¾¾å¤æ‚çš„æƒ³æ³•ï¼Œä»è€Œè·å¾—æ›´æ˜“äºè®¿é—®å’Œå…¨é¢çš„ä½“éªŒã€‚</p>
<h1 id="ask-for-contextè¯¢é—®èƒŒæ™¯ä¿¡æ¯">Ask for ContextÂ è¯¢é—®èƒŒæ™¯ä¿¡æ¯</h1>
<p>LLMs are not only capable of generating text but canÂ also act as simple agents with a limited amount ofÂ <em>reasoning capability.</em>Â This allows you to write a prompt asking the language model to either:<br>
LLMs ä¸ä»…èƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬ï¼Œè¿˜å¯ä»¥å……å½“æ¨ç†èƒ½åŠ›æœ‰é™çš„ç®€å•ä»£ç†ã€‚è¿™å…è®¸æ‚¨ç¼–å†™ä¸€ä¸ªæç¤ºï¼Œè¦æ±‚è¯­è¨€æ¨¡å‹æ‰§è¡Œä»¥ä¸‹ä»»ä¸€æ“ä½œï¼š</p>
<ul>
<li>
<p>Return a valid result to a question or statement<br>
å°†æœ‰æ•ˆç»“æœè¿”å›åˆ°é—®é¢˜æˆ–é™ˆè¿°</p>
</li>
<li>
<p>Ask for more context to appropriately answer the question<br>
è¯¢é—®æ›´å¤šä¸Šä¸‹æ–‡ä»¥é€‚å½“åœ°å›ç­”é—®é¢˜</p>
</li>
</ul>
<p>In this section, youâ€™ll learn about the importance ofÂ <em>asking for context</em>Â when working with LLMs such as GPT-4. We will start with an example of a prompt that doesnâ€™t provide enough context, resulting in a less useful response.<br>
åœ¨æœ¬èŠ‚ä¸­ï¼Œæ‚¨å°†äº†è§£åœ¨ä½¿ç”¨ LLMsï¼ˆä¾‹å¦‚ GPT-4ï¼‰æ—¶è¯¢é—®ä¸Šä¸‹æ–‡çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å°†ä»ä¸€ä¸ªæç¤ºç¤ºä¾‹å¼€å§‹ï¼Œè¯¥æç¤ºæ²¡æœ‰æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´å“åº”ä¸å¤ªæœ‰ç”¨ã€‚</p>
<p>Then, we will provide a better prompt that encourages the model to ask for additional context if needed. Finally, we will use the additional context provided to generate a more informed response.<br>
ç„¶åï¼Œæˆ‘ä»¬å°†æä¾›ä¸€ä¸ªæ›´å¥½çš„æç¤ºï¼Œé¼“åŠ±æ¨¡å‹åœ¨éœ€è¦æ—¶è¯·æ±‚é¢å¤–çš„ä¸Šä¸‹æ–‡ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æä¾›çš„å…¶ä»–ä¸Šä¸‹æ–‡æ¥ç”Ÿæˆæ›´æ˜æ™ºçš„å“åº”ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Should I use MongoDB or PostgreSQL for my project?</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>It&rsquo;s difficult to make a recommendation without knowing more about your
specific software requirements. Both MongoDB and PostgreSQL have their
advantages, but the best choice depends on the specific needs of your
project.</p>
<p>In the preceding prompt, the model does not have enough context to make a meaningful recommendation. Instead, you can ask ChatGPT for a list of recommended points that would help it to make an effective decision.<br>
åœ¨å‰é¢çš„æç¤ºä¸­ï¼Œæ¨¡å‹æ²¡æœ‰è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡æ¥æå‡ºæœ‰æ„ä¹‰çš„å»ºè®®ã€‚ç›¸åï¼Œæ‚¨å¯ä»¥å‘ ChatGPT ç´¢å–ä¸€ä»½æ¨èç‚¹åˆ—è¡¨ï¼Œä»¥å¸®åŠ©å®ƒåšå‡ºæœ‰æ•ˆçš„å†³å®šã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Should I use MongoDB or PostgreSQL for my project?</p>
<p>Provide a list of requirements that would help you to make an informed
and high-quality decision about which database solution I should choose?</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>To make an informed decision between MongoDB and PostgreSQL for your
project, consider the following requirements:</p>
<ol>
<li>Data Structure: Evaluate the structure of your data. If your data
is highly unstructured or schema-less, MongoDB&rsquo;s flexible document model
may be more suitable. On the other hand, if your data has a fixed schema
or requires complex relationships, PostgreSQL&rsquo;s relational model might
be a better fit.</li>
</ol>
<p>&hellip;</p>
<ol start="10">
<li>Cost: Assess the cost implications of each database solution. MongoDB
offers both a community edition (free) and a commercial edition with
additional features and support.PostgreSQL is open-source and free to use,
with optional commercial support available from various vendors.</li>
</ol>
<p>After prompting ChatGPT about how to make an informed decision, now youâ€™re aware of what to include within your prompt to help the language model with deciding.<br>
åœ¨æç¤º ChatGPT å¦‚ä½•åšå‡ºæ˜æ™ºçš„å†³å®šåï¼Œç°åœ¨æ‚¨å·²ç»çŸ¥é“åœ¨æç¤ºä¸­åº”åŒ…å«å“ªäº›å†…å®¹ä»¥å¸®åŠ©è¯­è¨€æ¨¡å‹åšå‡ºå†³å®šã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Should I use MongoDB or PostgreSQL for my project?</p>
<p>Context:</p>
<ul>
<li>Data Structure: Highly unstructured data with varying document formats
and no fixed schema. Scalability: Project requires the ability to handle
massive amounts of data and distribute it across multiple servers.</li>
<li>ACID Compliance: Strong data consistency and integrity are crucial for
the project&rsquo;s financial transactions.</li>
<li>Querying Capabilities: Complex geospatial queries and aggregations on
location-based data.
Community and Ecosystem: Active community with numerous third-party
libraries and tools specifically tailored for the project&rsquo;s industry.</li>
<li>Data Integrity and Constraints: Enforcing strict foreign key
relationships and complex data validation rules.</li>
<li>Transactions and Concurrency: Requires handling high levels of
concurrent read and write operations while maintaining data consistency.</li>
<li>Maturity and Stability: Prioritizing a well-established and extensively
tested database for long-term stability.</li>
<li>Development and Operational Ease: Agile development approach with a need
for rapid prototyping and iterative schema design.</li>
<li>Cost: Budget constraints necessitate using an open-source solution with
no additional licensing or support costs.</li>
</ul>
<p>If you need more context, please specify what would help you to make
a better decision.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Based on these considerations, both MongoDB and PostgreSQL have their
strengths, but given the emphasis on strong data consistency, complex
querying, and data integrity constraints, PostgreSQL seems to be a more
suitable choice for your project.</p>
<p>In this final example, the model uses the additional context provided to give a well-informed recommendation for using PostgreSQL. By asking for context when necessary, LLMs like ChatGPT and GPT-4 can deliver more valuable and accurate responses.<br>
åœ¨æœ€åä¸€ä¸ªç¤ºä¾‹ä¸­ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨æä¾›çš„å…¶ä»–ä¸Šä¸‹æ–‡æ¥æä¾›ä½¿ç”¨ PostgreSQL çš„æ˜æ™ºå»ºè®®ã€‚é€šè¿‡åœ¨å¿…è¦æ—¶è¯¢é—®ä¸Šä¸‹æ–‡ï¼ŒLLMs åƒ ChatGPT å’Œ GPT-4 ä¸€æ ·å¯ä»¥æä¾›æ›´æœ‰ä»·å€¼å’Œå‡†ç¡®çš„å“åº”ã€‚</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#figure-3-3">FigureÂ 3-3</a>Â demonstrates howÂ <em>asking for context</em>Â changes the decision-making process of LLMs. Upon receiving user input, the model first assesses whether the context given is sufficient. If not, it prompts the user to provide more detailed information, emphasizing the modelâ€™s reliance on context-rich inputs. Once adequate context is acquired, the LLM then generates an informed and relevant response.<br>
å›¾ 3-3 æ¼”ç¤ºäº†è¯·æ±‚ä¸Šä¸‹æ–‡å¦‚ä½•æ”¹å˜ LLMs çš„å†³ç­–è¿‡ç¨‹ã€‚åœ¨æ”¶åˆ°ç”¨æˆ·è¾“å…¥åï¼Œæ¨¡å‹é¦–å…ˆè¯„ä¼°ç»™å®šçš„ä¸Šä¸‹æ–‡æ˜¯å¦è¶³å¤Ÿã€‚å¦‚æœæ²¡æœ‰ï¼Œå®ƒä¼šæç¤ºç”¨æˆ·æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œå¼ºè°ƒæ¨¡å‹å¯¹ä¸Šä¸‹æ–‡ä¸°å¯Œçš„è¾“å…¥çš„ä¾èµ–ã€‚ä¸€æ—¦è·å¾—äº†è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ï¼ŒLLM å°±ä¼šç”Ÿæˆä¸€ä¸ªçŸ¥æƒ…ä¸”ç›¸å…³çš„å“åº”ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0303.png" alt="The decision process of an LLM with asking for context."></p>
<h6 id="figure-3-3the-decision-process-of-an-llm-while-asking-for-context">Figure 3-3.Â The decision process of an LLM while asking for context</h6>
<p>å›¾ 3-3ã€‚LLM åœ¨è¯¢é—®ä¸Šä¸‹æ–‡æ—¶çš„å†³ç­–è¿‡ç¨‹</p>
<h1 id="allow-the-llm-to-ask-for-more-context-by-default">ALLOW THE LLM TO ASK FOR MORE CONTEXT BY DEFAULT</h1>
<p>é»˜è®¤æƒ…å†µä¸‹ï¼Œå…è®¸ LLM è¯·æ±‚æ›´å¤šä¸Šä¸‹æ–‡</p>
<p>You can allow the LLM to ask for more context as a default by including this key phrase:Â <em>If you need more context, please specify what would help you to make a better decision.</em><br>
æ‚¨å¯ä»¥é€šè¿‡åŒ…å«ä»¥ä¸‹å…³é”®çŸ­è¯­æ¥å…è®¸ LLM è¯·æ±‚æ›´å¤šä¸Šä¸‹æ–‡ä½œä¸ºé»˜è®¤å€¼ï¼šå¦‚æœæ‚¨éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ï¼Œè¯·æŒ‡å®šå¯ä»¥å¸®åŠ©æ‚¨åšå‡ºæ›´å¥½å†³å®šçš„å†…å®¹ã€‚</p>
<p>In this section, youâ€™ve seen how LLMs can act as agents that use environmental context to make decisions. By iteratively refining the prompt based on the modelâ€™s recommendations, we eventually reach a point where the model hasÂ <em>enough context to make a well-informed decision.</em><br>
åœ¨æœ¬èŠ‚ä¸­ï¼Œæ‚¨å·²ç»äº†è§£äº† LLMs å¦‚ä½•å……å½“ä½¿ç”¨ç¯å¢ƒä¸Šä¸‹æ–‡è¿›è¡Œå†³ç­–çš„ä»£ç†ã€‚é€šè¿‡æ ¹æ®æ¨¡å‹çš„å»ºè®®è¿­ä»£ç»†åŒ–æç¤ºï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šè¾¾åˆ°ä¸€ä¸ªç‚¹ï¼Œå³æ¨¡å‹æœ‰è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡æ¥åšå‡ºæ˜æ™ºçš„å†³ç­–ã€‚</p>
<p>This process highlights the importance of providing sufficient context in your prompts and being prepared to ask for more information when necessary. By doing so, you can leverage the power of LLMs like GPT-4 to make more accurate and valuable recommendations.<br>
æ­¤è¿‡ç¨‹å¼ºè°ƒäº†åœ¨æç¤ºä¸­æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡å¹¶å‡†å¤‡å¥½åœ¨å¿…è¦æ—¶è¯¢é—®æ›´å¤šä¿¡æ¯çš„é‡è¦æ€§ã€‚é€šè¿‡è¿™æ ·åšï¼Œæ‚¨å¯ä»¥åƒ GPT-1001 ä¸€æ ·åˆ©ç”¨ @4# çš„åŠ›é‡æ¥æå‡ºæ›´å‡†ç¡®ã€æ›´æœ‰ä»·å€¼çš„å»ºè®®ã€‚</p>
<p>In agent-based systems like GPT-4, the ability to ask for more context and provide a finalized answer is crucial for making well-informed decisions.Â <a href="https://oreil.ly/l3Ihy">AutoGPT</a>, a multiagent system, has a self-evaluation step that automatically checks whether the task can be completed given the current context within the prompt. This technique uses an actorâ€“critic relationship, where the existing promptÂ context is being analyzed to see whether it could be further refined before being executed.<br>
åœ¨åƒ GPT-4 è¿™æ ·åŸºäºæ™ºèƒ½ä½“çš„ç³»ç»Ÿä¸­ï¼Œè¯¢é—®æ›´å¤šä¸Šä¸‹æ–‡å¹¶æä¾›æœ€ç»ˆç­”æ¡ˆçš„èƒ½åŠ›å¯¹äºåšå‡ºæ˜æ™ºçš„å†³ç­–è‡³å…³é‡è¦ã€‚AutoGPT æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå®ƒæœ‰ä¸€ä¸ªè‡ªæˆ‘è¯„ä¼°æ­¥éª¤ï¼Œå¯ä»¥è‡ªåŠ¨æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å¯ä»¥åœ¨æç¤ºä¸­çš„å½“å‰ä¸Šä¸‹æ–‡ä¸‹å®Œæˆã€‚è¯¥æŠ€æœ¯ä½¿ç”¨å‚ä¸è€…-æ‰¹è¯„è€…å…³ç³»ï¼Œå…¶ä¸­æ­£åœ¨åˆ†æç°æœ‰çš„æç¤ºä¸Šä¸‹æ–‡ï¼Œä»¥æŸ¥çœ‹æ˜¯å¦å¯ä»¥åœ¨æ‰§è¡Œä¹‹å‰è¿›ä¸€æ­¥å®Œå–„å®ƒã€‚</p>
<h1 id="text-style-unbundlingæ–‡æœ¬æ ·å¼æ‹†åˆ†">Text Style UnbundlingÂ æ–‡æœ¬æ ·å¼æ‹†åˆ†</h1>
<p><em>Text style unbundling</em>Â is a powerful technique inÂ prompt engineering that allows you to extract and isolate specific textual features from a given document, such as tone, length, vocabulary, and structure.<br>
æ–‡æœ¬æ ·å¼æ‹†åˆ†æ˜¯æç¤ºå·¥ç¨‹ä¸­çš„ä¸€é¡¹å¼ºå¤§æŠ€æœ¯ï¼Œå®ƒå…è®¸æ‚¨ä»ç»™å®šæ–‡æ¡£ä¸­æå–å’Œéš”ç¦»ç‰¹å®šçš„æ–‡æœ¬ç‰¹å¾ï¼Œä¾‹å¦‚è¯­æ°”ã€é•¿åº¦ã€è¯æ±‡å’Œç»“æ„ã€‚</p>
<p>This allows you to create new content that shares similar characteristics with the original document, ensuring consistency in style and tone across various forms of communication.<br>
è¿™å…è®¸æ‚¨åˆ›å»ºä¸åŸå§‹æ–‡æ¡£å…·æœ‰ç›¸ä¼¼ç‰¹å¾çš„æ–°å†…å®¹ï¼Œä»è€Œç¡®ä¿å„ç§å½¢å¼çš„é€šä¿¡åœ¨é£æ ¼å’Œè¯­æ°”ä¸Šçš„ä¸€è‡´æ€§ã€‚</p>
<p>This consistency can be crucial for businesses and organizations that need to communicate with a unified voice across different channels and platforms. The benefits of this technique include:<br>
å¯¹äºéœ€è¦è·¨ä¸åŒæ¸ é“å’Œå¹³å°ä½¿ç”¨ç»Ÿä¸€è¯­éŸ³è¿›è¡Œé€šä¿¡çš„ä¼ä¸šå’Œç»„ç»‡æ¥è¯´ï¼Œè¿™ç§ä¸€è‡´æ€§è‡³å…³é‡è¦ã€‚è¿™ç§æŠ€æœ¯çš„ä¼˜ç‚¹åŒ…æ‹¬ï¼š</p>
<p>Improved brand consistency<br>
æé«˜å“ç‰Œä¸€è‡´æ€§</p>
<p>By ensuring that all content follows a similar style, organizations can strengthen their brand identity and maintain a cohesive image.<br>
é€šè¿‡ç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½éµå¾ªç›¸ä¼¼çš„é£æ ¼ï¼Œç»„ç»‡å¯ä»¥åŠ å¼ºå…¶å“ç‰Œå½¢è±¡å¹¶ä¿æŒæœ‰å‡èšåŠ›çš„å½¢è±¡ã€‚</p>
<p>Streamlined content creation<br>
ç®€åŒ–çš„å†…å®¹åˆ›å»º</p>
<p>By providing a clear set of guidelines, writers and content creators can more easily produce materials that align with a desired style.<br>
é€šè¿‡æä¾›ä¸€å¥—æ˜ç¡®çš„æŒ‡å¯¼æ–¹é’ˆï¼Œä½œå®¶å’Œå†…å®¹åˆ›ä½œè€…å¯ä»¥æ›´è½»æ¾åœ°åˆ¶ä½œå‡ºç¬¦åˆæ‰€éœ€é£æ ¼çš„ææ–™ã€‚</p>
<p>AdaptabilityÂ é€‚åº”æ€§</p>
<p>Text style unbundling allows for the easy adaptation of existing content to new formats or styles while preserving the core message and tone.<br>
æ–‡æœ¬æ ·å¼æ‹†åˆ†å…è®¸å°†ç°æœ‰å†…å®¹è½»æ¾è°ƒæ•´ä¸ºæ–°çš„æ ¼å¼æˆ–æ ·å¼ï¼ŒåŒæ—¶ä¿ç•™æ ¸å¿ƒä¿¡æ¯å’Œè¯­æ°”ã€‚</p>
<p>The process of text style unbundling involvesÂ <em>identifying the desired textual features</em>Â or creating a meta prompt (a prompt to create prompts) to extract these features and then using the extracted features to guide the generation of new content.<br>
æ–‡æœ¬æ ·å¼è§£ç»‘çš„è¿‡ç¨‹åŒ…æ‹¬è¯†åˆ«æ‰€éœ€çš„æ–‡æœ¬ç‰¹å¾æˆ–åˆ›å»ºå…ƒæç¤ºï¼ˆåˆ›å»ºæç¤ºçš„æç¤ºï¼‰æ¥æå–è¿™äº›ç‰¹å¾ï¼Œç„¶åä½¿ç”¨æå–çš„ç‰¹å¾æ¥æŒ‡å¯¼æ–°å†…å®¹çš„ç”Ÿæˆã€‚</p>
<h1 id="identifying-the-desired-textual-features">Identifying the Desired Textual Features</h1>
<p>è¯†åˆ«æ‰€éœ€çš„æ–‡æœ¬ç‰¹å¾</p>
<p>To successfully unbundle a text style, youÂ must first identify the specific features you want to extract from the input document. Common textual features to consider include:<br>
è¦æˆåŠŸè§£åŒ…æ–‡æœ¬æ ·å¼ï¼Œå¿…é¡»é¦–å…ˆç¡®å®šè¦ä»è¾“å…¥æ–‡æ¡£ä¸­æå–çš„ç‰¹å®šè¦ç´ ã€‚éœ€è¦è€ƒè™‘çš„å¸¸è§æ–‡æœ¬ç‰¹å¾åŒ…æ‹¬ï¼š</p>
<p>Tone of voiceÂ è¯­æ°”</p>
<p>The overall mood or attitude conveyed by the text, such as formal, casual, humorous, or authoritative<br>
æ–‡æœ¬ä¼ è¾¾çš„æ•´ä½“æƒ…ç»ªæˆ–æ€åº¦ï¼Œä¾‹å¦‚æ­£å¼çš„ã€éšæ„çš„ã€å¹½é»˜çš„æˆ–æƒå¨çš„</p>
<p>LengthÂ é•¿åº¦</p>
<p>The desired word count or general length of the content<br>
æ‰€éœ€çš„å­—æ•°æˆ–å†…å®¹çš„ä¸€èˆ¬é•¿åº¦</p>
<p>Vocabulary and phrasingÂ è¯æ±‡å’Œæªè¾</p>
<p>The choice of words and expressions, including industry-specific jargon, colloquialisms, and complexity<br>
è¯è¯­å’Œè¡¨è¾¾æ–¹å¼çš„é€‰æ‹©ï¼ŒåŒ…æ‹¬è¡Œä¸šç‰¹å®šçš„æœ¯è¯­ã€å£è¯­å’Œå¤æ‚æ€§</p>
<p>Additionally, you can even create a meta promptÂ that will dynamically show you these.<br>
æ­¤å¤–ï¼Œæ‚¨ç”šè‡³å¯ä»¥åˆ›å»ºä¸€ä¸ªå…ƒæç¤ºï¼ŒåŠ¨æ€æ˜¾ç¤ºè¿™äº›å†…å®¹ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Analyze the following text and identify core features that will allow
further documents to imitate the same style, consistency, and formatting
as the document below.</p>
<p>Provide all of the features as a style writing guide that will be used
for future documents.</p>
<p>Analyze the following text:</p>
<p>Artificial intelligence is rapidly transforming the world around us,
opening up new possibilities and revolutionizing industries. From
health care to entertainment, AI is making our lives better and more
convenient in countless ways. However, with great power comes great
responsibility. We must ensure that AI technologies are developed and
deployed ethically and responsibly to protect our privacy and uphold our
values.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Style Writing Guide:</p>
<p>Tone of Voice</p>
<ul>
<li>Maintain an informative and optimistic tone throughout the document.</li>
<li>Emphasize the positive impact of the subject matter while acknowledging
any potential concerns or responsibilities.</li>
</ul>
<p>Length:
&hellip;</p>
<p>Vocabulary:
&hellip;</p>
<p>Structure:
&hellip;</p>
<p>Content:</p>
<ul>
<li>Focus on the transformative nature and positive impacts of the subject
matter.</li>
<li>Highlight specific industries or areas where the subject matter has
made a difference.</li>
<li>Acknowledge any associated responsibilities or potential risks,
emphasizing the need for ethical and responsible practices.
Stress the importance of privacy protection and upholding values in
relation to the subject matter.</li>
</ul>
<h1 id="generating-new-content-with-the-extracted-features">Generating New Content with the Extracted Features</h1>
<p>ä½¿ç”¨æå–çš„ç‰¹å¾ç”Ÿæˆæ–°å†…å®¹</p>
<p>After youâ€™ve extracted the desiredÂ features from the input document, you can use this information to guide future content generation. Remember to ensure that the AI model incorporates the extracted features into its output by specifying the desired style in your prompt. For example:<br>
ä»è¾“å…¥æ–‡æ¡£ä¸­æå–æ‰€éœ€è¦ç´ åï¼Œå¯ä»¥ä½¿ç”¨æ­¤ä¿¡æ¯æ¥æŒ‡å¯¼å°†æ¥çš„å†…å®¹ç”Ÿæˆã€‚è¯·è®°ä½ï¼Œé€šè¿‡åœ¨æç¤ºä¸­æŒ‡å®šæ‰€éœ€çš„æ ·å¼ï¼Œç¡®ä¿ AI æ¨¡å‹å°†æå–çš„ç‰¹å¾åˆå¹¶åˆ°å…¶è¾“å‡ºä¸­ã€‚ä¾‹å¦‚ï¼š</p>
<ul>
<li><code>Write a new blog post on [topic] using the same tone of voice, length, vocabulary, and structure as the previously analyzed text.</code></li>
</ul>
<p>By combining this technique withÂ <em>reference text</em>Â (documents that act as grounding truth), you can produce credible, branded content that requires minimal revisions.<br>
é€šè¿‡å°†è¿™ç§æŠ€æœ¯ä¸å‚è€ƒæ–‡æœ¬ï¼ˆä½œä¸ºåŸºç¡€äº‹å®çš„æ–‡æ¡£ï¼‰ç›¸ç»“åˆï¼Œæ‚¨å¯ä»¥åˆ¶ä½œå‡ºéœ€è¦æœ€å°‘ä¿®æ”¹çš„å¯ä¿¡çš„å“ç‰Œå†…å®¹ã€‚</p>
<h1 id="extracting-specific-textual-features-with-llms">Extracting Specific Textual Features with LLMs</h1>
<p>ä½¿ç”¨ LLMs æå–ç‰¹å®šçš„æ–‡æœ¬ç‰¹å¾</p>
<p>You can easily tailor a prompt to guide an LLM in extracting particular textual features from a document. This canÂ be applied beyond just analyzing text for copywriting purposes. For instance, recognizing entities or discerning sentiment from the text can be achieved by crafting a precise instruction for the LLM.<br>
æ‚¨å¯ä»¥è½»æ¾å®šåˆ¶æç¤ºï¼Œä»¥æŒ‡å¯¼LLMä»æ–‡æ¡£ä¸­æå–ç‰¹å®šçš„æ–‡æœ¬ç‰¹å¾ã€‚è¿™å¯ä»¥åº”ç”¨äºä¸ä»…ä»…æ˜¯å‡ºäºæ–‡æ¡ˆç›®çš„åˆ†ææ–‡æœ¬ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡ä¸º LLM åˆ¶ä½œç²¾ç¡®çš„æŒ‡ä»¤æ¥å®ç°è¯†åˆ«å®ä½“æˆ–ä»æ–‡æœ¬ä¸­è¾¨åˆ«æƒ…æ„Ÿã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Analyze the following text to identify and list the entities mentioned:</p>
<p>Artificial intelligence is rapidly transforming the world around us,
opening up new possibilities and revolutionizing industries. From
health care to entertainment, AI is making our lives better and more
convenient in countless ways. However, with great power comes great
responsibility. We must ensure that AI technologies are developed
and deployed ethically and responsibly to protect our privacy and
uphold our values.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Entities:</p>
<ul>
<li>Artificial Intelligence (AI)</li>
<li>Health care</li>
<li>Entertainment</li>
<li>Privacy</li>
<li>Values</li>
</ul>
<h1 id="summarizationç»¼è¿°">SummarizationÂ ç»¼è¿°</h1>
<p>In the era of information overload, the ability to condense large amounts of text into concise, digestible summariesÂ has become an essential skill. As the volume of data we produce and consume continues to grow, the demand for effective summarization techniques has increased significantly.<br>
åœ¨ä¿¡æ¯è¿‡è½½çš„æ—¶ä»£ï¼Œå°†å¤§é‡æ–‡æœ¬æµ“ç¼©æˆç®€æ˜æ˜“æ‡‚çš„æ‘˜è¦çš„èƒ½åŠ›å·²æˆä¸ºä¸€é¡¹å¿…ä¸å¯å°‘çš„æŠ€èƒ½ã€‚éšç€æˆ‘ä»¬ç”Ÿäº§å’Œæ¶ˆè´¹çš„æ•°æ®é‡ä¸æ–­å¢é•¿ï¼Œå¯¹æœ‰æ•ˆæ‘˜è¦æŠ€æœ¯çš„éœ€æ±‚æ˜¾ç€å¢åŠ ã€‚</p>
<p>AI has made significant advancements in this domain, providing powerful tools for generating summaries that retain the most critical information while discarding less relevant details.<br>
äººå·¥æ™ºèƒ½åœ¨è¿™ä¸€é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä¸ºç”Ÿæˆæ‘˜è¦æä¾›äº†å¼ºå¤§çš„å·¥å…·ï¼Œè¿™äº›æ‘˜è¦ä¿ç•™äº†æœ€å…³é”®çš„ä¿¡æ¯ï¼ŒåŒæ—¶ä¸¢å¼ƒäº†ä¸å¤ªç›¸å…³çš„ç»†èŠ‚ã€‚</p>
<p>Letâ€™s explore how to summarize using a language model.<br>
è®©æˆ‘ä»¬æ¢è®¨ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨è¯­è¨€æ¨¡å‹è¿›è¡Œæ€»ç»“ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Summarize the following text:</p>
<p>Whereas the party of the first part, hereinafter referred to as the
&ldquo;Purchaser,&rdquo; and the party of the second part, hereinafter referred to
as the &ldquo;Vendor,&rdquo; being fully cognizant of their respective legal
obligations and responsibilities pursuant to the applicable statutory
provisions and common law doctrines, have duly entered into a legally
binding agreement, hereinafter referred to as the &ldquo;Agreement,&rdquo; for the
purpose of effectuating a transaction involving the transfer of goods,
hereinafter referred to as the &ldquo;Goods,&rdquo; between the Purchaser and the
Vendor, subject to the terms and conditions set forth herein, the Parties
do hereby solemnly acknowledge and affirm the following:
&hellip;</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>This is a legally binding agreement between the Purchaser and the Vendor
for the transfer of goods. The agreement incorporates the recitals as an
integral part, but in case of any conflict, the operative provisions of the
agreement prevail. The interpretation clause states that singular words
include the plural and vice versa, and words referring to a gender include
all genders. References to statutory provisions include any amendments or
replacements.</p>
<p>Summarization is an invaluable application of AI, enabling users to quickly extract key insights from lengthy articles, reports, or research papers. This process can help individuals make informed decisions, save time, and prioritize their reading. AI-generated summaries can also facilitate information sharing among teams, allowingÂ for more efficient collaboration and communication.<br>
æ‘˜è¦æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå®è´µåº”ç”¨ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿä»å†—é•¿çš„æ–‡ç« ã€æŠ¥å‘Šæˆ–ç ”ç©¶è®ºæ–‡ä¸­å¿«é€Ÿæå–å…³é”®è§è§£ã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥å¸®åŠ©ä¸ªäººåšå‡ºæ˜æ™ºçš„å†³å®šï¼ŒèŠ‚çœæ—¶é—´ï¼Œå¹¶ä¼˜å…ˆè€ƒè™‘ä»–ä»¬çš„é˜…è¯»ã€‚äººå·¥æ™ºèƒ½ç”Ÿæˆçš„æ‘˜è¦è¿˜å¯ä»¥ä¿ƒè¿›å›¢é˜Ÿä¹‹é—´çš„ä¿¡æ¯å…±äº«ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„åä½œå’Œæ²Ÿé€šã€‚</p>
<h1 id="summarizing-given-context-window-limitations">Summarizing Given Context Window Limitations</h1>
<p>æ€»ç»“ç»™å®šçš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶</p>
<p>For documents larger than an LLM can handleÂ in a single API request, a common approach is to chunk the document, summarize each chunk, and then combine these summaries into a final summary, as shown inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#figure-3-4">FigureÂ 3-4</a>.<br>
å¯¹äºå¤§äº LLM å¯ä»¥åœ¨å•ä¸ª API è¯·æ±‚ä¸­å¤„ç†çš„æ–‡æ¡£ï¼Œä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—ï¼Œå¯¹æ¯ä¸ªå—è¿›è¡Œæ±‡æ€»ï¼Œç„¶åå°†è¿™äº›æ‘˜è¦åˆå¹¶ä¸ºæœ€ç»ˆæ‘˜è¦ï¼Œå¦‚å›¾ 3-4 æ‰€ç¤ºã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0304.png" alt=".A summarization pipeline that uses text splitting and multiple summarization steps."></p>
<h6 id="figure-3-4a-summarization-pipeline-that-uses-text-splitting-and-multiple-summarization-steps">Figure 3-4.Â A summarization pipeline that uses text splitting and multiple summarization steps</h6>
<p>å›¾ 3-4ã€‚ä½¿ç”¨æ–‡æœ¬æ‹†åˆ†å’Œå¤šä¸ªæ‘˜è¦æ­¥éª¤çš„æ‘˜è¦ç®¡é“</p>
<p>Additionally, people may require different types of summaries for various reasons, and this is where AI summarization comes in handy. As illustrated in the preceding diagram, a large PDF document could easily be processed using AI summarization to generate distinct summaries tailored to individual needs:<br>
æ­¤å¤–ï¼Œäººä»¬å¯èƒ½å‡ºäºå„ç§åŸå› éœ€è¦ä¸åŒç±»å‹çš„æ‘˜è¦ï¼Œè¿™å°±æ˜¯ AI æ‘˜è¦æ´¾ä¸Šç”¨åœºçš„åœ°æ–¹ã€‚å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¯ä»¥ä½¿ç”¨ AI æ‘˜è¦è½»æ¾å¤„ç†å¤§å‹ PDF æ–‡æ¡£ï¼Œä»¥ç”Ÿæˆé’ˆå¯¹ä¸ªäººéœ€æ±‚é‡èº«å®šåˆ¶çš„ä¸åŒæ‘˜è¦ï¼š</p>
<p>Summary AÂ æ‘˜è¦ A</p>
<p>Provides key insights, which is perfect for users seeking a quick understanding of the documentâ€™s content, enabling them to focus on the most crucial points<br>
æä¾›å…³é”®è§è§£ï¼Œéå¸¸é€‚åˆå¯»æ±‚å¿«é€Ÿäº†è§£æ–‡æ¡£å†…å®¹çš„ç”¨æˆ·ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿä¸“æ³¨äºæœ€å…³é”®çš„ç‚¹</p>
<p>Summary BÂ æ‘˜è¦ B</p>
<p>On the other hand, offers decision-making information, allowing users to make informed decisions based on the contentâ€™s implications and recommendations<br>
å¦ä¸€æ–¹é¢ï¼Œæä¾›å†³ç­–ä¿¡æ¯ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®å†…å®¹çš„å«ä¹‰å’Œå»ºè®®åšå‡ºæ˜æ™ºçš„å†³å®š</p>
<p>Summary CÂ æ‘˜è¦ C</p>
<p>Caters to collaboration and communication, ensuring that users can efficiently share the documentâ€™s information and work together seamlessly<br>
è¿åˆåä½œå’Œæ²Ÿé€šï¼Œç¡®ä¿ç”¨æˆ·èƒ½å¤Ÿæœ‰æ•ˆåœ°å…±äº«æ–‡æ¡£ä¿¡æ¯å¹¶æ— ç¼åä½œ</p>
<p>By customizing the summariesÂ for different users, AI summarization contributes to increased information retrieval for all users, making the entire process more efficient and targeted.<br>
é€šè¿‡ä¸ºä¸åŒç”¨æˆ·è‡ªå®šä¹‰æ‘˜è¦ï¼ŒAI æ‘˜è¦æœ‰åŠ©äºå¢åŠ æ‰€æœ‰ç”¨æˆ·çš„ä¿¡æ¯æ£€ç´¢ï¼Œä½¿æ•´ä¸ªè¿‡ç¨‹æ›´åŠ é«˜æ•ˆå’Œæœ‰é’ˆå¯¹æ€§ã€‚</p>
<p>Letâ€™s assume youâ€™re only interested in finding and summarizing information about the advantages of digital marketing. Simply change your summarization prompt toÂ <code>Provide a concise, abstractive summary of the above text. Only summarize the advantages: ...</code><br>
å‡è®¾æ‚¨åªå¯¹æŸ¥æ‰¾å’Œæ€»ç»“æœ‰å…³æ•°å­—è¥é”€ä¼˜åŠ¿çš„ä¿¡æ¯æ„Ÿå…´è¶£ã€‚åªéœ€å°†æ‘˜è¦æç¤ºæ›´æ”¹ä¸ºÂ <code>Provide a concise, abstractive summary of the above text. Only summarize the advantages: ...</code></p>
<p>AI-powered summarization has emerged as an essential tool for quickly distilling vast amounts of information into concise, digestible summaries that cater to various user needs. By leveraging advanced language models like GPT-4, AI summarization techniques can efficiently extract key insights and decision-making information, and also facilitate collaboration and communication.<br>
äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ‘˜è¦å·²æˆä¸ºä¸€ç§å¿…ä¸å¯å°‘çš„å·¥å…·ï¼Œå¯ä»¥å¿«é€Ÿå°†å¤§é‡ä¿¡æ¯æç‚¼æˆç®€æ´ã€æ˜“äºç†è§£çš„æ‘˜è¦ï¼Œä»¥æ»¡è¶³å„ç§ç”¨æˆ·éœ€æ±‚ã€‚é€šè¿‡åˆ©ç”¨ GPT-4 ç­‰é«˜çº§è¯­è¨€æ¨¡å‹ï¼ŒAI æ‘˜è¦æŠ€æœ¯å¯ä»¥æœ‰æ•ˆåœ°æå–å…³é”®è§è§£å’Œå†³ç­–ä¿¡æ¯ï¼Œå¹¶ä¿ƒè¿›åä½œå’Œæ²Ÿé€šã€‚</p>
<p>As the volume of data continues to grow, the demand for effective and targeted summarization will only increase, making AI a crucial asset for individuals and organizations alike inÂ navigating the Information Age.<br>
éšç€æ•°æ®é‡çš„æŒç»­å¢é•¿ï¼Œå¯¹æœ‰æ•ˆå’Œæœ‰é’ˆå¯¹æ€§çš„æ‘˜è¦çš„éœ€æ±‚åªä¼šå¢åŠ ï¼Œè¿™ä½¿å¾—äººå·¥æ™ºèƒ½æˆä¸ºä¸ªäººå’Œç»„ç»‡åœ¨ä¿¡æ¯æ—¶ä»£é©¾é©­çš„é‡è¦èµ„äº§ã€‚</p>
<h1 id="chunking-textåˆ†å—æ–‡æœ¬">Chunking TextÂ åˆ†å—æ–‡æœ¬</h1>
<p>LLMs continue to develop and play anÂ increasingly crucial role in various applications, as the ability to process and manage large volumes of text becomes ever more important. An essential technique for handling large-scale text is known asÂ <em>chunking.</em><br>
LLMs ç»§ç»­å‘å±•ï¼Œå¹¶åœ¨å„ç§åº”ç”¨ç¨‹åºä¸­å‘æŒ¥è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ï¼Œå› ä¸ºå¤„ç†å’Œç®¡ç†å¤§é‡æ–‡æœ¬çš„èƒ½åŠ›å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚å¤„ç†å¤§å‹æ–‡æœ¬çš„ä¸€ç§åŸºæœ¬æŠ€æœ¯ç§°ä¸ºåˆ†å—ã€‚</p>
<p><em>Chunking</em>Â refers to the process of breaking down large pieces of text into smaller, more manageable units or chunks. These chunks can be based on various criteria, such as sentence, paragraph, topic, complexity, or length. By dividing text into smaller segments, AI models can more efficiently process, analyze, and generate responses.<br>
åˆ†å—æ˜¯æŒ‡å°†å¤§æ®µæ–‡æœ¬åˆ†è§£ä¸ºæ›´å°ã€æ›´æ˜“äºç®¡ç†çš„å•å…ƒæˆ–å—çš„è¿‡ç¨‹ã€‚è¿™äº›å—å¯ä»¥åŸºäºå„ç§æ¡ä»¶ï¼Œä¾‹å¦‚å¥å­ã€æ®µè½ã€ä¸»é¢˜ã€å¤æ‚æ€§æˆ–é•¿åº¦ã€‚é€šè¿‡å°†æ–‡æœ¬åˆ’åˆ†ä¸ºæ›´å°çš„ç‰‡æ®µï¼ŒAI æ¨¡å‹å¯ä»¥æ›´æœ‰æ•ˆåœ°å¤„ç†ã€åˆ†æå’Œç”Ÿæˆå“åº”ã€‚</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#figure-3-5">FigureÂ 3-5</a>Â illustrates the process of chunking a large piece of text and subsequently extracting topics from the individual chunks.<br>
å›¾ 3-5 æ¼”ç¤ºäº†å¯¹å¤§æ®µæ–‡æœ¬è¿›è¡Œåˆ†å—ï¼Œç„¶åä»å„ä¸ªå—ä¸­æå–ä¸»é¢˜çš„è¿‡ç¨‹ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0305.png" alt="Topic extraction process after chunking text."></p>
<h6 id="figure-3-5topic-extraction-with-an-llm-after-chunking-text">Figure 3-5.Â Topic extraction with an LLM after chunking text</h6>
<p>å›¾ 3-5ã€‚åœ¨åˆ†å—æ–‡æœ¬åä½¿ç”¨ LLM æå–ä¸»é¢˜</p>
<h2 id="benefits-of-chunking-text">Benefits of Chunking Text</h2>
<p>åˆ†å—æ–‡æœ¬çš„å¥½å¤„</p>
<p>There are several advantages to chunking text, which include:<br>
åˆ†å—æ–‡æœ¬æœ‰å‡ ä¸ªä¼˜ç‚¹ï¼ŒåŒ…æ‹¬ï¼š</p>
<p>Fitting within a given context length<br>
åœ¨ç»™å®šçš„ä¸Šä¸‹æ–‡é•¿åº¦å†…æ‹Ÿåˆ</p>
<p>LLMs only have a certain amount of input and output tokens, which is called aÂ <em>context length</em>. By reducing the input tokens you can make sureÂ the output wonâ€™t be cut off and the initial request wonâ€™t be rejected.<br>
LLMs åªæœ‰ä¸€å®šæ•°é‡çš„è¾“å…¥å’Œè¾“å‡ºæ ‡è®°ï¼Œè¿™ç§°ä¸ºä¸Šä¸‹æ–‡é•¿åº¦ã€‚é€šè¿‡å‡å°‘è¾“å…¥ä»¤ç‰Œï¼Œå¯ä»¥ç¡®ä¿è¾“å‡ºä¸ä¼šè¢«åˆ‡æ–­ï¼Œåˆå§‹è¯·æ±‚ä¸ä¼šè¢«æ‹’ç»ã€‚</p>
<p>Reducing costÂ é™ä½æˆæœ¬</p>
<p>Chunking helps you to only retrieve the most important points from documents, which reduces your token usage and API costs.<br>
åˆ†å—å¯å¸®åŠ©æ‚¨ä»…ä»æ–‡æ¡£ä¸­æ£€ç´¢æœ€é‡è¦çš„ç‚¹ï¼Œä»è€Œå‡å°‘ä»¤ç‰Œä½¿ç”¨å’Œ API æˆæœ¬ã€‚</p>
<p>Improved performanceÂ æ”¹è¿›çš„æ€§èƒ½</p>
<p>Chunking reduces the processing load on LLMs, allowing for faster response times and more efficient resource utilization.<br>
åˆ†å—å‡å°‘äº† LLMs ä¸Šçš„å¤„ç†è´Ÿè½½ï¼Œä»è€Œå®ç°äº†æ›´å¿«çš„å“åº”æ—¶é—´å’Œæ›´æœ‰æ•ˆçš„èµ„æºåˆ©ç”¨ç‡ã€‚</p>
<p>Increased flexibilityÂ æé«˜çµæ´»æ€§</p>
<p>Chunking allows developers to tailor AI responses based on the specific needs of a given task or application.<br>
åˆ†å—å…è®¸å¼€å‘äººå‘˜æ ¹æ®ç»™å®šä»»åŠ¡æˆ–åº”ç”¨ç¨‹åºçš„ç‰¹å®šéœ€æ±‚å®šåˆ¶ AI å“åº”ã€‚</p>
<h2 id="scenarios-for-chunking-text">Scenarios for Chunking Text</h2>
<p>å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—çš„æ–¹æ¡ˆ</p>
<p>Chunking text can be particularly beneficial in certain scenarios, while in others it may not be required. Understanding when to apply this technique can help in optimizing the performance and cost efficiency of LLMs.<br>
åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—å¯èƒ½ç‰¹åˆ«æœ‰ç”¨ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œå®ƒå¯èƒ½ä¸æ˜¯å¿…éœ€çš„ã€‚äº†è§£ä½•æ—¶åº”ç”¨æ­¤æŠ€æœ¯æœ‰åŠ©äºä¼˜åŒ– LLMs çš„æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šã€‚</p>
<h3 id="when-to-chunkä½•æ—¶åˆ†å—">When to chunkÂ ä½•æ—¶åˆ†å—</h3>
<p>Large documentsÂ å¤§å‹æ–‡æ¡£</p>
<p>When dealingÂ with extensive documents that exceed the maximum token limit of the LLM<br>
åœ¨å¤„ç†è¶…è¿‡ @1001 æœ€å¤§ä»¤ç‰Œé™åˆ¶çš„å¤§é‡æ–‡æ¡£æ—¶#</p>
<p>Complex analysisÂ å¤æ‚åˆ†æ</p>
<p>In scenarios where a detailed analysis is required and the document needs to be broken down for better comprehension and processing<br>
åœ¨éœ€è¦è¯¦ç»†åˆ†æå¹¶ä¸”éœ€è¦åˆ†è§£æ–‡æ¡£ä»¥ä¾¿æ›´å¥½åœ°ç†è§£å’Œå¤„ç†çš„æƒ…å†µä¸‹</p>
<p>Multitopic documentsÂ å¤šä¸»é¢˜æ–‡æ¡£</p>
<p>When a document covers multiple topics and itâ€™s beneficial to handle them individually<br>
å½“æ–‡æ¡£æ¶µç›–å¤šä¸ªä¸»é¢˜å¹¶ä¸”å•ç‹¬å¤„ç†å®ƒä»¬æ˜¯æœ‰ç›Šçš„</p>
<h3 id="when-not-to-chunk">When not to chunk</h3>
<p>ä½•æ—¶ä¸åˆ†å—</p>
<p>Short documentsÂ çŸ­æ–‡æ¡£</p>
<p>When the document isÂ short and well within the token limits of the LLM<br>
å½“æ–‡æ¡£å¾ˆçŸ­å¹¶ä¸”å®Œå…¨åœ¨ @1001 çš„ä»¤ç‰Œé™åˆ¶èŒƒå›´å†…æ—¶#</p>
<p>Simple analysisÂ ç®€å•åˆ†æ</p>
<p>In cases where the analysis or processing required is straightforward and doesnâ€™t benefit from chunking<br>
åœ¨æ‰€éœ€çš„åˆ†ææˆ–å¤„ç†ç®€å•ä¸”æ— æ³•ä»åˆ†å—ä¸­å—ç›Šçš„æƒ…å†µä¸‹</p>
<p>Single-topic documentsÂ å•ä¸»é¢˜æ–‡æ¡£</p>
<p>When a document is focused on a single topic and chunking doesnâ€™t add value to the processing<br>
å½“æ–‡æ¡£ä¸“æ³¨äºå•ä¸ªä¸»é¢˜å¹¶ä¸”åˆ†å—ä¸ä¼šä¸ºå¤„ç†å¢åŠ ä»·å€¼æ—¶</p>
<h2 id="poor-chunking-exampleç³Ÿç³•çš„åˆ†å—ç¤ºä¾‹">Poor Chunking ExampleÂ ç³Ÿç³•çš„åˆ†å—ç¤ºä¾‹</h2>
<p>When text is not chunked correctly, it can lead toÂ reduced LLM performance. Consider the following paragraph from a news article:<br>
å½“æ–‡æœ¬æœªæ­£ç¡®åˆ†å—æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´ LLM æ€§èƒ½é™ä½ã€‚è¯·çœ‹ä¸€ç¯‡æ–°é—»æ–‡ç« ä¸­çš„ä»¥ä¸‹æ®µè½ï¼š</p>
<p>The local council has decided to increase the budget for education by 10%
this year, a move that has been welcomed by parents and teachers alike. The
additional funds will be used to improve school infrastructure, hire more
teachers, and provide better resources for students. However, some critics
argue that the increase is not enough to address the growing demands of the
education system.</p>
<p>When the text is fragmented into isolated words, the resulting list lacks the original context:<br>
å½“æ–‡æœ¬è¢«åˆ†å‰²æˆå­¤ç«‹çš„å•è¯æ—¶ï¼Œç”Ÿæˆçš„åˆ—è¡¨ç¼ºå°‘åŸå§‹ä¸Šä¸‹æ–‡ï¼š</p>
<p>[&ldquo;The&rdquo;, &ldquo;local&rdquo;, &ldquo;council&rdquo;, &ldquo;has&rdquo;, &ldquo;decided&rdquo;, &ldquo;to&rdquo;, &ldquo;increase&rdquo;, &ldquo;the&rdquo;,
&ldquo;budget&rdquo;, &hellip;]</p>
<p>The main issues with this poor chunking example include:<br>
è¿™ä¸ªç³Ÿç³•çš„åˆ†å—ç¤ºä¾‹çš„ä¸»è¦é—®é¢˜åŒ…æ‹¬ï¼š</p>
<p>Loss of contextÂ å¤±å»ä¸Šä¸‹æ–‡</p>
<p>By splitting the text into individual words, the original meaning and relationships between the words are lost. This makes it difficult for AI models to understand and respond effectively.<br>
é€šè¿‡å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•ä¸ªå•è¯ï¼Œå•è¯ä¹‹é—´çš„åŸå§‹å«ä¹‰å’Œå…³ç³»ä¼šä¸¢å¤±ã€‚è¿™ä½¿å¾— AI æ¨¡å‹éš¾ä»¥æœ‰æ•ˆç†è§£å’Œå“åº”ã€‚</p>
<p>Increased processing load<br>
å¢åŠ å¤„ç†è´Ÿè·</p>
<p>Processing individual words requires more computational resources, making it less efficient than processing larger chunks of text.<br>
å¤„ç†å•ä¸ªå•è¯éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºï¼Œå› æ­¤å…¶æ•ˆç‡ä½äºå¤„ç†è¾ƒå¤§çš„æ–‡æœ¬å—ã€‚</p>
<p>As a result of the poor chunking in this example, an LLM may face several challenges:<br>
ç”±äºæ­¤ç¤ºä¾‹ä¸­çš„åˆ†å—è¾ƒå·®ï¼ŒLLM å¯èƒ½ä¼šé¢ä¸´ä»¥ä¸‹å‡ ä¸ªæŒ‘æˆ˜ï¼š</p>
<ul>
<li>
<p>Difficulty understanding the main ideas or themes of the text<br>
éš¾ä»¥ç†è§£æ–‡æœ¬çš„ä¸»è¦æ€æƒ³æˆ–ä¸»é¢˜</p>
</li>
<li>
<p>Struggling to generate accurate summaries or translations<br>
éš¾ä»¥ç”Ÿæˆå‡†ç¡®çš„æ‘˜è¦æˆ–ç¿»è¯‘</p>
</li>
<li>
<p>Inability to effectively perform tasks such as sentiment analysis or textÂ <code>classification</code><br>
æ— æ³•æœ‰æ•ˆæ‰§è¡Œæƒ…ç»ªåˆ†ææˆ–æ–‡æœ¬ @0 ç­‰ä»»åŠ¡#</p>
</li>
</ul>
<p>By understanding the pitfalls of poor chunking, you can apply prompt engineering principles to improve the process and achieve better results with AI language models.<br>
é€šè¿‡äº†è§£ä¸è‰¯åˆ†å—çš„é™·é˜±ï¼Œæ‚¨å¯ä»¥åº”ç”¨æç¤ºå·¥ç¨‹åŸç†æ¥æ”¹è¿›æµç¨‹å¹¶ä½¿ç”¨ AI è¯­è¨€æ¨¡å‹è·å¾—æ›´å¥½çš„ç»“æœã€‚</p>
<p>Letâ€™s explore an improved chunking example using the same news article paragraph from the previous section; youâ€™ll now chunk the text by sentence:<br>
è®©æˆ‘ä»¬ä½¿ç”¨ä¸Šä¸€èŠ‚ä¸­çš„ç›¸åŒæ–°é—»æ–‡ç« æ®µè½æ¥æ¢ç´¢ä¸€ä¸ªæ”¹è¿›çš„åˆ†å—ç¤ºä¾‹;ç°åœ¨ï¼Œæ‚¨å°†æŒ‰å¥å­å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—ï¼š</p>
<p>[&ldquo;&ldquo;&ldquo;The local council has decided to increase the budget for education
by 10% this year, a move that has been welcomed by parents and teachers alike.
&ldquo;&rdquo;&rdquo;,</p>
<p>&ldquo;&ldquo;&ldquo;The additional funds will be used to improve school infrastructure,
hire more teachers, and provide better resources for students.&rdquo;&rdquo;&rdquo;,</p>
<p>&ldquo;&ldquo;&ldquo;&ldquo;However, some critics argue that the increase is not enough to
address the growing demands of the education system.&rdquo;&rdquo;&rdquo;]</p>
<h1 id="divide-labor-and-evaluate-quality">DIVIDE LABOR AND EVALUATE QUALITY</h1>
<p>åˆ†å·¥è€ƒæ ¸è´¨é‡</p>
<p>Define the granularity at which theÂ text should be chunked, such as by sentence, paragraph, or topic. Adjust parameters like the number of tokens or model temperature to optimize the chunking process.<br>
å®šä¹‰åº”å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—çš„ç²’åº¦ï¼Œä¾‹å¦‚æŒ‰å¥å­ã€æ®µè½æˆ–ä¸»é¢˜ã€‚è°ƒæ•´ä»¤ç‰Œæ•°é‡æˆ–æ¨¡å‹æ¸©åº¦ç­‰å‚æ•°ï¼Œä»¥ä¼˜åŒ–åˆ†å—è¿‡ç¨‹ã€‚</p>
<p>By chunking the text in this manner, you couldÂ insert whole sentences into an LLM prompt with the most relevant sentences.<br>
é€šè¿‡ä»¥è¿™ç§æ–¹å¼åˆ†å—æ–‡æœ¬ï¼Œæ‚¨å¯ä»¥å°†æ•´ä¸ªå¥å­æ’å…¥åˆ°åŒ…å«æœ€ç›¸å…³å¥å­çš„ LLM æç¤ºä¸­ã€‚</p>
<h1 id="chunking-strategiesåˆ†å—ç­–ç•¥">Chunking StrategiesÂ åˆ†å—ç­–ç•¥</h1>
<p>There are many different chunking strategies, including:<br>
æœ‰è®¸å¤šä¸åŒçš„åˆ†å—ç­–ç•¥ï¼ŒåŒ…æ‹¬ï¼š</p>
<p>Splitting by sentenceÂ æŒ‰å¥å­æ‹†åˆ†</p>
<p>Preserves theÂ context and structure of the original content, making it easier for LLMs to understand and process the information. Sentence-based chunking is particularly useful for tasks like summarization, translation, and sentiment analysis.<br>
ä¿ç•™åŸå§‹å†…å®¹çš„ä¸Šä¸‹æ–‡å’Œç»“æ„ï¼Œä½¿ LLMs æ›´å®¹æ˜“ç†è§£å’Œå¤„ç†ä¿¡æ¯ã€‚åŸºäºå¥å­çš„åˆ†å—å¯¹äºæ‘˜è¦ã€ç¿»è¯‘å’Œæƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ç‰¹åˆ«æœ‰ç”¨ã€‚</p>
<p>Splitting by paragraphÂ æŒ‰æ®µè½æ‹†åˆ†</p>
<p>This approach is especially effective when dealing with longer content, as it allows the LLM to focus on one cohesive unit at a time. Paragraph-based chunking is ideal for applications like document analysis, topic modeling, and information extraction.<br>
è¿™ç§æ–¹æ³•åœ¨å¤„ç†è¾ƒé•¿çš„å†…å®¹æ—¶ç‰¹åˆ«æœ‰æ•ˆï¼Œå› ä¸ºå®ƒå…è®¸ LLM ä¸€æ¬¡ä¸“æ³¨äºä¸€ä¸ªæœ‰å‡èšåŠ›çš„å•å…ƒã€‚åŸºäºæ®µè½çš„åˆ†å—éå¸¸é€‚åˆæ–‡æ¡£åˆ†æã€ä¸»é¢˜å»ºæ¨¡å’Œä¿¡æ¯æå–ç­‰åº”ç”¨ç¨‹åºã€‚</p>
<p>Splitting by topic or section<br>
æŒ‰ä¸»é¢˜æˆ–éƒ¨åˆ†æ‹†åˆ†</p>
<p>This method can help AI models better identify and understand the main themes and ideas within the content. Topic-based chunking is well suited for tasks like text classification, content recommendations, and clustering.<br>
è¿™ç§æ–¹æ³•å¯ä»¥å¸®åŠ© AI æ¨¡å‹æ›´å¥½åœ°è¯†åˆ«å’Œç†è§£å†…å®¹ä¸­çš„ä¸»è¦ä¸»é¢˜å’Œæ€æƒ³ã€‚åŸºäºä¸»é¢˜çš„åˆ†å—éå¸¸é€‚åˆæ–‡æœ¬åˆ†ç±»ã€å†…å®¹æ¨èå’Œèšç±»ç­‰ä»»åŠ¡ã€‚</p>
<p>Splitting by complexityÂ æŒ‰å¤æ‚åº¦æ‹†åˆ†</p>
<p>For certain applications, it might be helpful to split text based on its complexity, such as the reading level or technicality of the content. By grouping similar complexity levels together, LLMs can more effectively process and analyze the text. This approach is useful for tasks like readability analysis, content adaptation, and personalized learning.<br>
å¯¹äºæŸäº›åº”ç”¨ç¨‹åºï¼Œæ ¹æ®æ–‡æœ¬çš„å¤æ‚æ€§ï¼ˆä¾‹å¦‚å†…å®¹çš„é˜…è¯»çº§åˆ«æˆ–æŠ€æœ¯æ€§ï¼‰æ‹†åˆ†æ–‡æœ¬å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚é€šè¿‡å°†ç›¸ä¼¼çš„å¤æ‚åº¦çº§åˆ«ç»„åˆåœ¨ä¸€èµ·ï¼ŒLLMs å¯ä»¥æ›´æœ‰æ•ˆåœ°å¤„ç†å’Œåˆ†ææ–‡æœ¬ã€‚è¿™ç§æ–¹æ³•å¯¹äºå¯è¯»æ€§åˆ†æã€å†…å®¹é€‚åº”å’Œä¸ªæ€§åŒ–å­¦ä¹ ç­‰ä»»åŠ¡å¾ˆæœ‰ç”¨ã€‚</p>
<p>Splitting by lengthÂ æŒ‰é•¿åº¦æ‹†åˆ†</p>
<p>This technique is particularly helpful when working with very long or complex documents, as it allows LLMs to process the content more efficiently. Length-based chunking is suitable for applications like large-scale text analysis, search engine indexing, and text preprocessing.<br>
è¿™ç§æŠ€æœ¯åœ¨å¤„ç†å¾ˆé•¿æˆ–å¾ˆå¤æ‚çš„æ–‡æ¡£æ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå…è®¸ LLMs æ›´æœ‰æ•ˆåœ°å¤„ç†å†…å®¹ã€‚åŸºäºé•¿åº¦çš„åˆ†å—é€‚ç”¨äºå¤§è§„æ¨¡æ–‡æœ¬åˆ†æã€æœç´¢å¼•æ“ç´¢å¼•å’Œæ–‡æœ¬é¢„å¤„ç†ç­‰åº”ç”¨ã€‚</p>
<p>Splitting by tokens using a tokenizer<br>
ä½¿ç”¨åˆ†è¯å™¨æŒ‰ä»¤ç‰Œæ‹†åˆ†</p>
<p>Utilizing a tokenizer is a crucial step in many natural language processing tasks, as it enables the process of splitting text into individual tokens. Tokenizers divide text into smaller units, such as words, phrases, or symbols, which can then be analyzed and processed by AI models more effectively. Youâ€™ll shortly be using a package calledÂ <code>tiktoken</code>, which is a bytes-pair encoding tokenizer (BPE) for chunking.<br>
ä½¿ç”¨åˆ†è¯å™¨æ˜¯è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„å…³é”®æ­¥éª¤ï¼Œå› ä¸ºå®ƒå¯ä»¥å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•ä¸ªä»¤ç‰Œçš„è¿‡ç¨‹ã€‚åˆ†è¯å™¨å°†æ–‡æœ¬åˆ’åˆ†ä¸ºæ›´å°çš„å•å…ƒï¼Œä¾‹å¦‚å•è¯ã€çŸ­è¯­æˆ–ç¬¦å·ï¼Œç„¶å AI æ¨¡å‹å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ†æå’Œå¤„ç†è¿™äº›å•å…ƒã€‚æ‚¨å¾ˆå¿«å°±ä¼šä½¿ç”¨ä¸€ä¸ªåä¸ºÂ <code>tiktoken</code>Â çš„åŒ…ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåˆ†å—çš„å­—èŠ‚å¯¹ç¼–ç åˆ†è¯å™¨ ï¼ˆBPEï¼‰ã€‚</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#table-3-1">TableÂ 3-1</a>Â provides a high-level overview of the different chunking strategies; itâ€™s worth considering what matters to you most when performing chunking.<br>
è¡¨ 3-1 æä¾›äº†ä¸åŒåˆ†å—ç­–ç•¥çš„é«˜çº§æ¦‚è¿°;åœ¨æ‰§è¡Œåˆ†å—æ—¶ï¼Œå€¼å¾—è€ƒè™‘ä»€ä¹ˆå¯¹æ‚¨æ¥è¯´æœ€é‡è¦ã€‚</p>
<p>Are you more interested in preserving semantic context, or would naively splitting by length suffice?<br>
æ‚¨æ˜¯å¯¹ä¿ç•™è¯­ä¹‰ä¸Šä¸‹æ–‡æ›´æ„Ÿå…´è¶£ï¼Œè¿˜æ˜¯å¤©çœŸåœ°æŒ‰é•¿åº¦æ‹†åˆ†å°±è¶³å¤Ÿäº†ï¼Ÿ</p>
<p>Table 3-1.Â Six chunking strategies highlighting their advantages and disadvantages<br>
è¡¨ 3-1.å…­ç§åˆ†å—ç­–ç•¥çªå‡ºå…¶ä¼˜ç¼ºç‚¹</p>
<table>
  <thead>
      <tr>
          <th>Splitting strategyÂ æ‹†åˆ†ç­–ç•¥</th>
          <th>Advantages</th>
          <th>Disadvantages</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Splitting by sentenceÂ æŒ‰å¥å­æ‹†åˆ†</td>
          <td>Preserves context, suitable for various tasks  <!-- raw HTML omitted -->ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œé€‚ç”¨äºå„ç§ä»»åŠ¡</td>
          <td>May not be efficient for very long content  <!-- raw HTML omitted -->å¯¹äºå¾ˆé•¿çš„å†…å®¹å¯èƒ½æ•ˆç‡ä¸é«˜</td>
      </tr>
      <tr>
          <td>Splitting by paragraphÂ æŒ‰æ®µè½æ‹†åˆ†</td>
          <td>Handles longer content, focuses on cohesive units  <!-- raw HTML omitted -->å¤„ç†è¾ƒé•¿çš„å†…å®¹ï¼Œä¸“æ³¨äºæœ‰å‡èšåŠ›çš„å•å…ƒ</td>
          <td>Less granularity, may miss subtle connections  <!-- raw HTML omitted -->ç²’åº¦è¾ƒå°ï¼Œå¯èƒ½ä¼šé—æ¼ç»†å¾®çš„è¿æ¥</td>
      </tr>
      <tr>
          <td>Splitting by topicÂ æŒ‰ä¸»é¢˜æ‹†åˆ†</td>
          <td>Identifies main themes, better for classification  <!-- raw HTML omitted -->ç¡®å®šä¸»è¦ä¸»é¢˜ï¼Œæ›´å¥½åœ°åˆ†ç±»</td>
          <td>Requires topic identification, may miss fine details  <!-- raw HTML omitted -->éœ€è¦ä¸»é¢˜è¯†åˆ«ï¼Œå¯èƒ½ä¼šé—æ¼ç»†èŠ‚</td>
      </tr>
      <tr>
          <td>Splitting by complexityÂ æŒ‰å¤æ‚åº¦æ‹†åˆ†</td>
          <td>Groups similar complexity levels, adaptive  <!-- raw HTML omitted -->å¯¹ç›¸ä¼¼çš„å¤æ‚åº¦çº§åˆ«è¿›è¡Œåˆ†ç»„ï¼Œè‡ªé€‚åº”</td>
          <td>Requires complexity measurement, not suitable for all tasks  <!-- raw HTML omitted -->éœ€è¦å¤æ‚åº¦æµ‹é‡ï¼Œå¹¶ä¸é€‚åˆæ‰€æœ‰ä»»åŠ¡</td>
      </tr>
      <tr>
          <td>Splitting by lengthÂ æŒ‰é•¿åº¦æ‹†åˆ†</td>
          <td>Manages very long content, efficient processing  <!-- raw HTML omitted -->ç®¡ç†å¾ˆé•¿çš„å†…å®¹ï¼Œé«˜æ•ˆå¤„ç†</td>
          <td>Loss of context, may require more preprocessing steps  <!-- raw HTML omitted -->ä¸¢å¤±ä¸Šä¸‹æ–‡ï¼Œå¯èƒ½éœ€è¦æ›´å¤šçš„é¢„å¤„ç†æ­¥éª¤</td>
      </tr>
      <tr>
          <td>Using a tokenizer: Splitting by tokens  <!-- raw HTML omitted -->ä½¿ç”¨åˆ†è¯å™¨ï¼šæŒ‰ä»¤ç‰Œæ‹†åˆ†</td>
          <td>Accurate token counts, which helps in avoiding LLM prompt token limits  <!-- raw HTML omitted -->å‡†ç¡®çš„ä»¤ç‰Œè®¡æ•°ï¼Œæœ‰åŠ©äºé¿å… LLM æç¤ºä»¤ç‰Œé™åˆ¶</td>
          <td>Requires tokenization, may increase computational complexity  <!-- raw HTML omitted -->éœ€è¦æ ‡è®°åŒ–ï¼Œå¯èƒ½ä¼šå¢åŠ è®¡ç®—å¤æ‚æ€§</td>
      </tr>
  </tbody>
</table>
<p>By choosing the appropriate chunkingÂ strategy for your specific use case, you can optimize the performance and accuracy of AI language models.<br>
é€šè¿‡ä¸ºæ‚¨çš„ç‰¹å®šç”¨ä¾‹é€‰æ‹©é€‚å½“çš„åˆ†å—ç­–ç•¥ï¼Œæ‚¨å¯ä»¥ä¼˜åŒ– AI è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚</p>
<h1 id="sentence-detection-using-spacy">Sentence Detection Using SpaCy</h1>
<p>ä½¿ç”¨ SpaCy è¿›è¡Œå¥å­æ£€æµ‹</p>
<p><em>Sentence detection</em>, also known as sentenceÂ boundary disambiguation, is the process used in NLP that involves identifying the start and end of sentences within a given text. It can be particularly useful for tasks that require preserving the context and structure of the original content. By splitting the text into sentences, LLMs can better understand and process the information for tasks such as summarization, translation, and sentiment analysis.<br>
å¥å­æ£€æµ‹ï¼Œä¹Ÿç§°ä¸ºå¥å­è¾¹ç•Œæ¶ˆæ­§ï¼Œæ˜¯ NLP ä¸­ä½¿ç”¨çš„è¿‡ç¨‹ï¼Œæ¶‰åŠè¯†åˆ«ç»™å®šæ–‡æœ¬ä¸­å¥å­çš„å¼€å¤´å’Œç»“å°¾ã€‚å¯¹äºéœ€è¦ä¿ç•™åŸå§‹å†…å®¹çš„ä¸Šä¸‹æ–‡å’Œç»“æ„çš„ä»»åŠ¡ï¼Œå®ƒç‰¹åˆ«æœ‰ç”¨ã€‚é€šè¿‡å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå¥å­ï¼ŒLLMs å¯ä»¥æ›´å¥½åœ°ç†è§£å’Œå¤„ç†æ‘˜è¦ã€ç¿»è¯‘å’Œæƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡çš„ä¿¡æ¯ã€‚</p>
<p>Splitting by sentence is possible using NLP libraries such asÂ <a href="https://spacy.io/">spaCy</a>. Ensure that you have spaCy installed in your Python environment. You can install it withÂ <code>pip install spacy</code>. Download theÂ <code>en_core_web_sm</code>Â model using the commandÂ <code>python -m spacy download en_core_web_sm</code>.<br>
ä½¿ç”¨ spaCy ç­‰ NLP åº“å¯ä»¥æŒ‰å¥å­æ‹†åˆ†ã€‚ç¡®ä¿æ‚¨åœ¨ Python ç¯å¢ƒä¸­å®‰è£…äº† spaCyã€‚ä½ å¯ä»¥ç”¨Â <code>pip install spacy</code>Â å®‰è£…å®ƒã€‚ä½¿ç”¨å‘½ä»¤Â <code>python -m spacy download en_core_web_sm</code>Â ä¸‹è½½Â <code>en_core_web_sm</code>Â æ¨¡å‹ã€‚</p>
<p>InÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#ex-3-3">ExampleÂ 3-3</a>, the code demonstrates sentence detection using the spaCy library in Python.<br>
åœ¨ç¤ºä¾‹ 3-3 ä¸­ï¼Œä»£ç æ¼”ç¤ºäº†ä½¿ç”¨ Python ä¸­çš„ spaCy åº“è¿›è¡Œå¥å­æ£€æµ‹ã€‚</p>
<h5 id="example-3-3sentence-detection-with-spacy">Example 3-3.Â <a href="https://oreil.ly/GKDnc">Sentence detection with spaCy</a></h5>
<p>ä¾‹ 3-3.ä½¿ç”¨ spaCy è¿›è¡Œå¥å­æ£€æµ‹</p>
<pre tabindex="0"><code>import
</code></pre><p>Output:Â è¾“å‡ºï¼š</p>
<p>This is a sentence.
This is another sentence.</p>
<p>First, youâ€™ll import the spaCy library and load the English modelÂ <code>(en_core_web_sm)</code>Â to initialize anÂ <code>nlp</code>Â object. Define an input text with two sentences; the text is then processed withÂ <code>doc = nlp(text)</code>, creating aÂ <code>doc</code>Â object as a result. Finally, the code iterates through the detected sentences using theÂ <code>doc.sents</code>Â attributeÂ and prints each sentence.<br>
é¦–å…ˆï¼Œæ‚¨å°†å¯¼å…¥ spaCy åº“å¹¶åŠ è½½è‹±æ–‡æ¨¡å‹Â <code>(en_core_web_sm)</code>Â ä»¥åˆå§‹åŒ–Â <code>nlp</code>Â å¯¹è±¡ã€‚å®šä¹‰åŒ…å«ä¸¤ä¸ªå¥å­çš„è¾“å…¥æ–‡æœ¬;ç„¶åç”¨Â <code>doc = nlp(text)</code>Â å¤„ç†æ–‡æœ¬ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªÂ <code>doc</code>Â å¯¹è±¡ã€‚æœ€åï¼Œä»£ç ä½¿ç”¨Â <code>doc.sents</code>Â å±æ€§å¾ªç¯è®¿é—®æ£€æµ‹åˆ°çš„å¥å­å¹¶æ‰“å°æ¯ä¸ªå¥å­ã€‚</p>
<h1 id="building-a-simple-chunking-algorithm-in-python">Building a Simple Chunking Algorithm in Python</h1>
<p>åœ¨ Python ä¸­æ„å»ºç®€å•çš„åˆ†å—ç®—æ³•</p>
<p>After exploring many chunkingÂ strategies, itâ€™s important to build your intuition by writing a simple chunking algorithm from scatch.<br>
åœ¨æ¢ç´¢äº†è®¸å¤šåˆ†å—ç­–ç•¥ä¹‹åï¼Œé€šè¿‡ä» scatch ç¼–å†™ä¸€ä¸ªç®€å•çš„åˆ†å—ç®—æ³•æ¥å»ºç«‹ä½ çš„ç›´è§‰æ˜¯å¾ˆé‡è¦çš„ã€‚</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#ex-3-4">ExampleÂ 3-4</a>Â shows how to chunk text based on the length of characters from the blog post â€œHubspot - What Is Digital Marketing?â€ This file can be found in the Github repository atÂ <em><a href="https://oreil.ly/30rlQ">content/chapter_3/hubspot_blog_post.txt</a></em>.<br>
ç¤ºä¾‹ 3-4 æ˜¾ç¤ºäº†å¦‚ä½•æ ¹æ®åšå®¢æ–‡ç« â€œHubspot - ä»€ä¹ˆæ˜¯æ•°å­—è¥é”€â€ä¸­çš„å­—ç¬¦é•¿åº¦å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—ã€‚æ­¤æ–‡ä»¶å¯åœ¨ Github å­˜å‚¨åº“çš„ content/chapter_3/hubspot_blog_post.txt ä¸­æ‰¾åˆ°ã€‚</p>
<p>To correctly read theÂ <em>hubspot_blog_post.txt</em>Â file, make sure your current working directory is set to theÂ <a href="https://oreil.ly/OHurh"><em>content/chapter_3</em></a>Â GitHub directory. This applies for both running the Python code or launching the Jupyter Notebook server.<br>
è‹¥è¦æ­£ç¡®è¯»å– hubspot_blog_post.txt æ–‡ä»¶ï¼Œè¯·ç¡®ä¿å½“å‰å·¥ä½œç›®å½•è®¾ç½®ä¸º content/chapter_3 GitHub ç›®å½•ã€‚è¿™é€‚ç”¨äºè¿è¡Œ Python ä»£ç æˆ–å¯åŠ¨ Jupyter Notebook æœåŠ¡å™¨ã€‚</p>
<h5 id="example-3-4character-chunking">Example 3-4.Â <a href="https://oreil.ly/n3sNy">Character chunking</a></h5>
<p>ä¾‹ 3-4.å­—ç¬¦åˆ†å—</p>
<pre tabindex="0"><code>with
</code></pre><p>Output:Â è¾“å‡ºï¼š</p>
<h2 id="products-or-services-related-to-what-yo">search engine optimization strategy for many local businesses is an optimized
Google My Business profile to appear in local search results when people look for
products or services related to what yo</h2>
<p>u offer.</p>
<h2 id="atlanta-bookstore">For Keeps Bookstore, a local bookstore in Atlanta, GA, has optimized its
Google My Business profile for local SEO so it appears in queries for
â€œatlanta bookstore.â€</h2>
<p>&hellip;(shortened for brevity)&hellip;</p>
<p>First, you open the text fileÂ <em>hubspot_blog_post.txt</em>Â with theÂ <code>open</code>Â function and read its contents into the variable text. Then using a list comprehension you create a list of chunks, where eachÂ <code>chunk</code>Â is a 200 character substring of text.<br>
é¦–å…ˆï¼Œä½¿ç”¨Â <code>open</code>Â å‡½æ•°æ‰“å¼€æ–‡æœ¬æ–‡ä»¶hubspot_blog_post.txtï¼Œå¹¶å°†å…¶å†…å®¹è¯»å…¥å˜é‡æ–‡æœ¬ä¸­ã€‚ç„¶åä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼åˆ›å»ºä¸€ä¸ªå—åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªÂ <code>chunk</code>Â éƒ½æ˜¯ä¸€ä¸ª 200 ä¸ªå­—ç¬¦çš„æ–‡æœ¬å­å­—ç¬¦ä¸²ã€‚</p>
<p>Then you use theÂ <code>range</code>Â function to generate indices for each 200 character substring, and theÂ <code>i:i+200</code>Â slice notation to extract the substring from text.<br>
ç„¶åï¼Œä½¿ç”¨Â <code>range</code>Â å‡½æ•°ä¸ºæ¯ä¸ª 200 ä¸ªå­—ç¬¦çš„å­å­—ç¬¦ä¸²ç”Ÿæˆç´¢å¼•ï¼Œå¹¶ä½¿ç”¨Â <code>i:i+200</code>Â åˆ‡ç‰‡è¡¨ç¤ºæ³•ä»æ–‡æœ¬ä¸­æå–å­å­—ç¬¦ä¸²ã€‚</p>
<p>Finally, you loop through each chunk in theÂ <code>chunks</code>Â list andÂ <code>print</code>Â it to the console.<br>
æœ€åï¼Œå°†Â <code>chunks</code>Â åˆ—è¡¨ä¸­çš„æ¯ä¸ªå—å¾ªç¯ï¼Œå¹¶å°†å…¶Â <code>print</code>Â å¾ªç¯åˆ°æ§åˆ¶å°ã€‚</p>
<p>As you can see, because the chunking implementation is relatively simple and only based on length, there are gaps within the sentences and even words.<br>
æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå› ä¸ºåˆ†å—çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œè€Œä¸”åªåŸºäºé•¿åº¦ï¼Œæ‰€ä»¥å¥å­ç”šè‡³å•è¯ä¹‹é—´éƒ½å­˜åœ¨é—´éš™ã€‚</p>
<p>For these reasons we believe that good NLP chunking has the following properties:<br>
ç”±äºè¿™äº›åŸå› ï¼Œæˆ‘ä»¬è®¤ä¸ºå¥½çš„ NLP åˆ†å—å…·æœ‰ä»¥ä¸‹å±æ€§ï¼š</p>
<ul>
<li>
<p>Preserves entire words, ideally sentences and contextual points made by speakers<br>
ä¿ç•™æ•´ä¸ªå•è¯ï¼Œæœ€å¥½æ˜¯è¯´è¯è€…çš„å¥å­å’Œä¸Šä¸‹æ–‡è¦ç‚¹</p>
</li>
<li>
<p>Handles for when sentences span across several pages, for example, page 1 into page 2<br>
å½“å¥å­è·¨è¶Šå¤šä¸ªé¡µé¢æ—¶çš„å¥æŸ„ï¼Œä¾‹å¦‚ï¼Œç¬¬ 1 é¡µåˆ°ç¬¬ 2 é¡µ</p>
</li>
<li>
<p>Provides an adequate token count for eachÂ <code>chunk</code>Â so that the total number of input tokens will appropriatelyÂ fit into a given token context window for any LLM<br>
ä¸ºæ¯ä¸ªÂ <code>chunk</code>Â æä¾›è¶³å¤Ÿçš„ä»¤ç‰Œè®¡æ•°ï¼Œä»¥ä¾¿è¾“å…¥ä»¤ç‰Œçš„æ€»æ•°å°†é€‚å½“åœ°é€‚åˆä»»ä½• @1001 çš„ç»™å®šä»¤ç‰Œä¸Šä¸‹æ–‡çª—å£#</p>
</li>
</ul>
<h1 id="sliding-window-chunkingæ»‘åŠ¨çª—å£åˆ†å—">Sliding Window ChunkingÂ æ»‘åŠ¨çª—å£åˆ†å—</h1>
<p><em>Sliding window chunking</em>Â is a technique usedÂ for dividing text data into overlapping chunks, orÂ <em>windows</em>, based on a specified number of characters.<br>
æ»‘åŠ¨çª—å£åˆ†å—æ˜¯ä¸€ç§ç”¨äºæ ¹æ®æŒ‡å®šæ•°é‡çš„å­—ç¬¦å°†æ–‡æœ¬æ•°æ®åˆ’åˆ†ä¸ºé‡å å—æˆ–çª—å£çš„æŠ€æœ¯ã€‚</p>
<p>But what exactly is a sliding window?<br>
ä½†ç©¶ç«Ÿä»€ä¹ˆæ˜¯æ¨æ‹‰çª—ï¼Ÿ</p>
<p>Imagine viewing a long piece of text through a small window. This window is only capable of displaying a fixed number of characters at a time. As you slide this window from the beginning to the end of the text, you seeÂ <em>overlapping chunks of text</em>. This mechanism forms the essence of the sliding window approach.<br>
æƒ³è±¡ä¸€ä¸‹ï¼Œé€šè¿‡ä¸€ä¸ªå°çª—å£æŸ¥çœ‹ä¸€é•¿æ®µæ–‡æœ¬ã€‚æ­¤çª—å£ä¸€æ¬¡åªèƒ½æ˜¾ç¤ºå›ºå®šæ•°é‡çš„å­—ç¬¦ã€‚å½“æ‚¨ä»æ–‡æœ¬çš„å¼€å¤´æ»‘åŠ¨æ­¤çª—å£åˆ°æ–‡æœ¬çš„ç»“å°¾æ—¶ï¼Œæ‚¨ä¼šçœ‹åˆ°é‡å çš„æ–‡æœ¬å—ã€‚è¿™ç§æœºåˆ¶æ„æˆäº†æ»‘åŠ¨çª—å£æ–¹æ³•çš„æœ¬è´¨ã€‚</p>
<p>Each window size is defined by aÂ <em>fixed number of characters</em>, and theÂ <em>step size</em>Â determines how far the window moves with each slide.<br>
æ¯ä¸ªçª—å£å¤§å°ç”±å›ºå®šæ•°é‡çš„å­—ç¬¦å®šä¹‰ï¼Œæ­¥é•¿å†³å®šäº†çª—å£éšæ¯å¼ å¹»ç¯ç‰‡ç§»åŠ¨çš„è·ç¦»ã€‚</p>
<p>InÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#figure-3-6">FigureÂ 3-6</a>, with a window size of 5 characters and a step size of 1, the first chunk would contain the first 5 characters of the text. The window then slides 1 character to the right to create the second chunk, which contains characters 2 through 6.<br>
åœ¨å›¾ 3-6 ä¸­ï¼Œçª—å£å¤§å°ä¸º 5 ä¸ªå­—ç¬¦ï¼Œæ­¥é•¿ä¸º 1ï¼Œç¬¬ä¸€ä¸ªå—å°†åŒ…å«æ–‡æœ¬çš„å‰ 5 ä¸ªå­—ç¬¦ã€‚ç„¶åï¼Œçª—å£å‘å³æ»‘åŠ¨ 1 ä¸ªå­—ç¬¦ä»¥åˆ›å»ºç¬¬äºŒä¸ªå—ï¼Œå…¶ä¸­åŒ…å«å­—ç¬¦ 2 åˆ° 6ã€‚</p>
<p>This process repeats until the end of the text is reached, ensuring each chunk overlaps with the previous and next ones to retain some shared context.<br>
æ­¤è¿‡ç¨‹é‡å¤è¿›è¡Œï¼Œç›´åˆ°åˆ°è¾¾æ–‡æœ¬çš„æœ«å°¾ï¼Œç¡®ä¿æ¯ä¸ªå—éƒ½ä¸ä¸Šä¸€ä¸ªå’Œä¸‹ä¸€ä¸ªå—é‡å ï¼Œä»¥ä¿ç•™ä¸€äº›å…±äº«çš„ä¸Šä¸‹æ–‡ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0306.png" alt="pega 0306"></p>
<h6 id="figure-3-6a-sliding-window-with-a-window-size-of-5-and-a-step-size-of-1">Figure 3-6.Â A sliding window, with a window size of 5 and a step size of 1</h6>
<p>å›¾ 3-6ã€‚æ»‘åŠ¨çª—å£ï¼Œçª—å£å¤§å°ä¸º 5ï¼Œæ­¥é•¿ä¸º 1</p>
<p>Due to the step size being 1, there is a lot of duplicate information between chunks, and at the same time the risk of losing information between chunks is dramatically reduced.<br>
ç”±äºæ­¥é•¿ä¸º 1ï¼Œå› æ­¤å—ä¹‹é—´å­˜åœ¨å¤§é‡é‡å¤ä¿¡æ¯ï¼ŒåŒæ—¶å—ä¹‹é—´ä¸¢å¤±ä¿¡æ¯çš„é£é™©å¤§å¤§é™ä½ã€‚</p>
<p>This is in stark contrast toÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#figure-3-7">FigureÂ 3-7</a>, which has a window size of 4 and a step size of 2. Youâ€™ll notice that because of the 100% increase in step size, the amount of information shared between the chunks is greatly reduced.<br>
è¿™ä¸å›¾ 3-7 å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œå›¾ 3-7 çš„çª—å£å¤§å°ä¸º 4ï¼Œæ­¥é•¿ä¸º 2ã€‚æ‚¨ä¼šæ³¨æ„åˆ°ï¼Œç”±äºæ­¥é•¿å¢åŠ äº† 100%ï¼Œå—ä¹‹é—´å…±äº«çš„ä¿¡æ¯é‡å¤§å¤§å‡å°‘ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0307.png" alt="pega 0307"></p>
<h6 id="figure-3-7a-sliding-window-with-a-window-size-of-4-and-a-step-size-of-2">Figure 3-7.Â A sliding window, with a window size of 4 and a step size of 2</h6>
<p>å›¾ 3-7ã€‚æ»‘åŠ¨çª—å£ï¼Œçª—å£å¤§å°ä¸º 4ï¼Œæ­¥é•¿ä¸º 2</p>
<p>You will likely need a larger overlap if accuracy and preserving semanatic context are more important than minimizing token inputs or the number of requests made to an LLM.<br>
å¦‚æœå‡†ç¡®æ€§å’Œä¿ç•™è¯­ä¹‰ä¸Šä¸‹æ–‡æ¯”æœ€å°åŒ–ä»¤ç‰Œè¾“å…¥æˆ–å¯¹ LLM å‘å‡ºçš„è¯·æ±‚æ•°é‡æ›´é‡è¦ï¼Œåˆ™å¯èƒ½éœ€è¦æ›´å¤§çš„é‡å ã€‚</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#ex-3-5">ExampleÂ 3-5</a>Â shows how you can implement a sliding window using Pythonâ€™sÂ <code>len()</code>Â function. TheÂ <code>len()</code>Â function provides us with the total number of characters in a given text string, which subsequently aids in defining the parameters of our sliding windows.<br>
ç¤ºä¾‹ 3-5 å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Python çš„Â <code>len()</code>Â å‡½æ•°å®ç°æ»‘åŠ¨çª—å£ã€‚Â <code>len()</code>Â å‡½æ•°ä¸ºæˆ‘ä»¬æä¾›äº†ç»™å®šæ–‡æœ¬å­—ç¬¦ä¸²ä¸­çš„å­—ç¬¦æ€»æ•°ï¼Œè¿™éšåæœ‰åŠ©äºå®šä¹‰æ»‘åŠ¨çª—å£çš„å‚æ•°ã€‚</p>
<h5 id="example-3-5sliding-window">Example 3-5.Â <a href="https://oreil.ly/aCkDo">Sliding window</a></h5>
<p>ä¾‹ 3-5.æ¨æ‹‰çª—</p>
<pre tabindex="0"><code>def
</code></pre><p>This code outputs:Â æ­¤ä»£ç è¾“å‡ºï¼š</p>
<p>Chunk 1: This is an example o
Chunk 2: is an example of sli
Chunk 3:  example of sliding
Chunk 4: ple of sliding windo
Chunk 5: f sliding window tex
Chunk 6: ding window text chu
Chunk 7: window text chunking</p>
<p>In the context of prompt engineering, the sliding window approach offers several benefits over fixed chunking methods. It allows LLMs to retain a higher degree of context, as there is an overlap between the chunks and offers an alternative approach to preserving contextÂ compared to sentence detection.<br>
åœ¨æç¤ºå·¥ç¨‹çš„èƒŒæ™¯ä¸‹ï¼Œä¸å›ºå®šåˆ†å—æ–¹æ³•ç›¸æ¯”ï¼Œæ»‘åŠ¨çª—å£æ–¹æ³•å…·æœ‰å¤šç§ä¼˜åŠ¿ã€‚å®ƒå…è®¸ LLMs ä¿ç•™æ›´é«˜ç¨‹åº¦çš„ä¸Šä¸‹æ–‡ï¼Œå› ä¸ºå—ä¹‹é—´å­˜åœ¨é‡å ï¼Œå¹¶ä¸”ä¸å¥å­æ£€æµ‹ç›¸æ¯”ï¼Œæä¾›äº†ä¸€ç§ä¿ç•™ä¸Šä¸‹æ–‡çš„æ›¿ä»£æ–¹æ³•ã€‚</p>
<h1 id="text-chunking-packagesæ–‡æœ¬åˆ†å—åŒ…">Text Chunking PackagesÂ æ–‡æœ¬åˆ†å—åŒ…</h1>
<p>When working with LLMs such as GPT-4, alwaysÂ remain wary of the maximum context length:<br>
ä½¿ç”¨ LLMsï¼ˆä¾‹å¦‚ GPT-4ï¼‰æ—¶ï¼Œè¯·å§‹ç»ˆè­¦æƒ•æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼š</p>
<ul>
<li><code>maximum_context_length = input_tokens + output_tokens</code></li>
</ul>
<p>There are various tokenizers available to break your text down into manageable units, the most popular ones being NLTK, spaCy, and tiktoken.<br>
æœ‰å„ç§æ ‡è®°å™¨å¯ç”¨äºå°†æ‚¨çš„æ–‡æœ¬åˆ†è§£ä¸ºå¯ç®¡ç†çš„å•å…ƒï¼Œæœ€å—æ¬¢è¿çš„æ˜¯ NLTKã€spaCy å’Œ tiktokenã€‚</p>
<p>BothÂ <a href="https://oreil.ly/wTmI7">NLTK</a>Â andÂ <a href="https://oreil.ly/c4MvQ">spaCy</a>Â provide comprehensive support for text processing, but youâ€™ll be focusing on tiktoken.<br>
NLTK å’Œ spaCy éƒ½ä¸ºæ–‡æœ¬å¤„ç†æä¾›å…¨é¢çš„æ”¯æŒï¼Œä½†æ‚¨å°†ä¸“æ³¨äº tiktokenã€‚</p>
<h1 id="text-chunking-with-tiktoken">Text Chunking with Tiktoken</h1>
<p>ä½¿ç”¨ Tiktoken è¿›è¡Œæ–‡æœ¬åˆ†å—</p>
<p><a href="https://oreil.ly/oSpVe">Tiktoken</a>Â is a fastÂ <em>byte pair encoding (BPE)</em>Â tokenizer that breaks down text intoÂ subword units and is designed for use with OpenAIâ€™s models. Tiktoken offers faster performance than comparable open source tokenizers.<br>
Tiktoken æ˜¯ä¸€ç§å¿«é€Ÿå­—èŠ‚å¯¹ç¼–ç  ï¼ˆBPEï¼‰ åˆ†è¯å™¨ï¼Œå¯å°†æ–‡æœ¬åˆ†è§£ä¸ºå­å­—å•å…ƒï¼Œä¸“ä¸ºä¸ OpenAI çš„æ¨¡å‹ä¸€èµ·ä½¿ç”¨è€Œè®¾è®¡ã€‚Tiktoken æä¾›æ¯”åŒç±»å¼€æºæ ‡è®°å™¨æ›´å¿«çš„æ€§èƒ½ã€‚</p>
<p>As a developer working with GPT-4 applications, using tiktoken offers you several key advantages:<br>
ä½œä¸ºä½¿ç”¨ GPT-4 åº”ç”¨ç¨‹åºçš„å¼€å‘äººå‘˜ï¼Œä½¿ç”¨ tiktoken ä¸ºæ‚¨æä¾›äº†å‡ ä¸ªå…³é”®ä¼˜åŠ¿ï¼š</p>
<p>Accurate token breakdown<br>
å‡†ç¡®çš„ä»¤ç‰Œç»†åˆ†</p>
<p>Itâ€™s crucial to divide text into tokens because GPT models interpret text as individual tokens. Identifying the number of tokens in your text helps you figure out whether the text is too lengthy for a model to process.<br>
å°†æ–‡æœ¬åˆ’åˆ†ä¸ºæ ‡è®°è‡³å…³é‡è¦ï¼Œå› ä¸º GPT æ¨¡å‹å°†æ–‡æœ¬è§£é‡Šä¸ºå•ä¸ªæ ‡è®°ã€‚è¯†åˆ«æ–‡æœ¬ä¸­çš„æ ‡è®°æ•°æœ‰åŠ©äºç¡®å®šæ–‡æœ¬æ˜¯å¦å¤ªé•¿è€Œæ— æ³•å¤„ç†æ¨¡å‹ã€‚</p>
<p>Effective resource utilization<br>
æœ‰æ•ˆåˆ©ç”¨èµ„æº</p>
<p>Having the correct token count enables you to manage resources efficiently, particularly when using the OpenAI API. Being aware of the exact number of tokens helps you regulate and optimize API usage, maintaining a balance between costs andÂ resource usage.<br>
æ‹¥æœ‰æ­£ç¡®çš„ä»¤ç‰Œè®¡æ•°ä½¿æ‚¨èƒ½å¤Ÿæœ‰æ•ˆåœ°ç®¡ç†èµ„æºï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨ OpenAI API æ—¶ã€‚äº†è§£ä»¤ç‰Œçš„ç¡®åˆ‡æ•°é‡æœ‰åŠ©äºè°ƒèŠ‚å’Œä¼˜åŒ– API ä½¿ç”¨ï¼Œä»è€Œåœ¨æˆæœ¬å’Œèµ„æºä½¿ç”¨ä¹‹é—´ä¿æŒå¹³è¡¡ã€‚</p>
<h1 id="encodingsç¼–ç ">EncodingsÂ ç¼–ç </h1>
<p>Encodings define the method ofÂ converting text into tokens, with different models utilizing different encodings. Tiktoken supports three encodings commonly used by OpenAI models:<br>
ç¼–ç å®šä¹‰äº†å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°çš„æ–¹æ³•ï¼Œä¸åŒçš„æ¨¡å‹ä½¿ç”¨ä¸åŒçš„ç¼–ç ã€‚Tiktoken æ”¯æŒ OpenAI æ¨¡å‹å¸¸ç”¨çš„ä¸‰ç§ç¼–ç ï¼š</p>
<table>
  <thead>
      <tr>
          <th>Encoding nameÂ ç¼–ç åç§°</th>
          <th>OpenAI modelsÂ OpenAI æ¨¡å‹</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>cl100k_base</td>
          <td>GPT-4, GPT-3.5-turbo, text-embedding-ada-002  <!-- raw HTML omitted -->GPT-4ã€GPT-3.5-turboã€æ–‡æœ¬åµŒå…¥-ada-002</td>
      </tr>
      <tr>
          <td>p50k_base</td>
          <td>Codex models, text-davinci-002, text-davinci-003  <!-- raw HTML omitted -->æ³•å…¸æ¨¡å‹ï¼Œtext-davinci-002ï¼Œtext-davinci-003</td>
      </tr>
      <tr>
          <td>r50k_base (or gpt2)Â r50k_baseï¼ˆæˆ– GPT2ï¼‰</td>
          <td>GPT-3 models like davinci  <!-- raw HTML omitted -->GPT-3 æ¨¡å‹ï¼Œå¦‚è¾¾èŠ¬å¥‡</td>
      </tr>
  </tbody>
</table>
<h2 id="understanding-the-tokenization-of-strings">Understanding the Tokenization of Strings</h2>
<p>äº†è§£å­—ç¬¦ä¸²çš„æ ‡è®°åŒ–</p>
<p>In English, tokens can vary in length, ranging from aÂ single character likeÂ <em>t</em>, to an entire word such asÂ <em>great</em>. This is due to the adaptable nature of tokenization, which can accommodate even tokens shorter than a character in complex script languages or tokens longer than a word in languages without spaces or where phrases function as single units.<br>
åœ¨è‹±è¯­ä¸­ï¼Œæ ‡è®°çš„é•¿åº¦å¯ä»¥æœ‰æ‰€ä¸åŒï¼Œä»å•ä¸ªå­—ç¬¦ï¼ˆå¦‚ tï¼‰åˆ°æ•´ä¸ªå•è¯ï¼ˆå¦‚ greatï¼‰ä¸ç­‰ã€‚è¿™æ˜¯ç”±äºæ ‡è®°åŒ–çš„é€‚åº”æ€§ï¼Œå®ƒç”šè‡³å¯ä»¥å®¹çº³å¤æ‚è„šæœ¬è¯­è¨€ä¸­æ¯”å­—ç¬¦çŸ­çš„æ ‡è®°ï¼Œæˆ–è€…åœ¨æ²¡æœ‰ç©ºæ ¼æˆ–çŸ­è¯­ä½œä¸ºå•ä¸ªå•å…ƒä½¿ç”¨çš„è¯­è¨€ä¸­æ¯”å•è¯é•¿çš„æ ‡è®°ã€‚</p>
<p>It is not uncommon for spaces to be included within tokens, such asÂ <code>&quot;is&quot;</code>Â rather thanÂ <code>&quot;is &quot;</code>Â orÂ <code>&quot; &quot;+&quot;is&quot;</code>. This practice helps maintain the original text formatting and can capture specific linguistic characteristics.<br>
åœ¨æ ‡è®°ä¸­åŒ…å«ç©ºæ ¼çš„æƒ…å†µå¹¶ä¸å°‘è§ï¼Œä¾‹å¦‚Â <code>&quot;is&quot;</code>Â è€Œä¸æ˜¯Â <code>&quot;is &quot;</code>Â æˆ–Â <code>&quot; &quot;+&quot;is&quot;</code>Â ã€‚è¿™ç§åšæ³•æœ‰åŠ©äºä¿æŒåŸå§‹æ–‡æœ¬æ ¼å¼ï¼Œå¹¶å¯ä»¥æ•è·ç‰¹å®šçš„è¯­è¨€ç‰¹å¾ã€‚</p>
<h6 id="noteæ³¨æ„-1">NOTEÂ æ³¨æ„</h6>
<p>To easily examine the tokenization of a string, you can useÂ <a href="https://oreil.ly/K6ZQK">OpenAI Tokenizer</a>.<br>
è¦è½»æ¾æ£€æŸ¥å­—ç¬¦ä¸²çš„æ ‡è®°åŒ–ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ OpenAI Tokenizerã€‚</p>
<p>You can installÂ <a href="https://oreil.ly/HA2QD">tiktoken from PyPI</a>Â withÂ <code>pip install</code>Â <code>tiktoken</code>. In the following example, youâ€™ll see how to easily encode text into tokens and decode tokens into text:<br>
ä½ å¯ä»¥ç”¨Â <code>pip install</code>Â <code>tiktoken</code>Â ä» PyPI å®‰è£… tiktokenã€‚åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œä½ å°†äº†è§£å¦‚ä½•è½»æ¾åœ°å°†æ–‡æœ¬ç¼–ç ä¸ºä»¤ç‰Œï¼Œä»¥åŠå¦‚ä½•å°†ä»¤ç‰Œè§£ç ä¸ºæ–‡æœ¬ï¼š</p>
<pre tabindex="0"><code># 1. Import the package:
</code></pre><p>Additionally letâ€™s write a function that will tokenizeÂ the text and then count the number of tokens given aÂ <code>text_string</code>Â andÂ <code>encoding_name</code>.<br>
æ­¤å¤–ï¼Œè®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥æ ‡è®°æ–‡æœ¬ï¼Œç„¶åè®¡ç®—ç»™å®šÂ <code>text_string</code>Â å’ŒÂ <code>encoding_name</code>Â çš„æ ‡è®°æ•°ã€‚</p>
<pre tabindex="0"><code>def
</code></pre><p>This code outputsÂ <code>8</code>.<br>
æ­¤ä»£ç è¾“å‡ºÂ <code>8</code>Â ã€‚</p>
<h1 id="estimating-token-usage-for-chat-api-calls">Estimating Token Usage for Chat API Calls</h1>
<p>ä¼°è®¡èŠå¤© API è°ƒç”¨çš„ä»¤ç‰Œä½¿ç”¨æƒ…å†µ</p>
<p>ChatGPT models, such as GPT-3.5-turbo and GPT-4, utilizeÂ tokens similarly to previous completion models. However, the message-based structure makes token counting for conversations more challenging:<br>
ChatGPT æ¨¡å‹ï¼Œä¾‹å¦‚ GPT-3.5-turbo å’Œ GPT-4ï¼Œä½¿ç”¨ä¸ä»¥å‰çš„å®Œæˆæ¨¡å‹ç±»ä¼¼çš„ä»£å¸ã€‚ä½†æ˜¯ï¼ŒåŸºäºæ¶ˆæ¯çš„ç»“æ„ä½¿å¯¹è¯çš„ä»¤ç‰Œè®¡æ•°æ›´å…·æŒ‘æˆ˜æ€§ï¼š</p>
<pre tabindex="0"><code>def
</code></pre><p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#ex-3-6">ExampleÂ 3-6</a>Â highlights the specific structure required to make a request against any of the chat models, which are currently GPT-3x and GPT-4.<br>
ç¤ºä¾‹ 3-6 é‡ç‚¹ä»‹ç»äº†é’ˆå¯¹ä»»ä½•èŠå¤©æ¨¡å‹ï¼ˆå½“å‰ä¸º GPT-3x å’Œ GPT-4ï¼‰å‘å‡ºè¯·æ±‚æ‰€éœ€çš„ç‰¹å®šç»“æ„ã€‚</p>
<p>Normally, chat history isÂ structured with aÂ <code>system</code>Â message first, and then succeeded by alternating exchanges between theÂ <code>user</code>Â and theÂ <code>assistant</code>.<br>
é€šå¸¸ï¼ŒèŠå¤©è®°å½•é¦–å…ˆä½¿ç”¨Â <code>system</code>Â æ¶ˆæ¯æ„å»ºï¼Œç„¶åé€šè¿‡Â <code>user</code>Â å’ŒÂ <code>assistant</code>Â ä¹‹é—´çš„äº¤æ›¿äº¤æ¢æ¥æˆåŠŸã€‚</p>
<h5 id="example-3-6a-payload-for-the-chat-completions-api-on-openai">Example 3-6.Â A payload for the Chat Completions API on OpenAI</h5>
<p>ä¾‹ 3-6.OpenAI ä¸ŠèŠå¤©å®Œæˆ API çš„æœ‰æ•ˆè´Ÿè½½</p>
<pre tabindex="0"><code>example_messages
</code></pre><p><code>&quot;role&quot;: &quot;system&quot;</code>Â describes a system messageÂ thatâ€™s useful forÂ <em>providing prompt instructions</em>. It offers a means to tweak the assistantâ€™s character or provide explicit directives regarding its interactive approach. Itâ€™s crucial to understand, though, that the system command isnâ€™t a prerequisite, and the modelâ€™s default demeanor without a system command could closely resemble the behavior of â€œYou are a helpful assistant.â€<br>
<code>&quot;role&quot;: &quot;system&quot;</code>Â æè¿°å¯ç”¨äºæä¾›æç¤ºè¯´æ˜çš„ç³»ç»Ÿæ¶ˆæ¯ã€‚å®ƒæä¾›äº†ä¸€ç§è°ƒæ•´åŠ©æ‰‹è§’è‰²æˆ–æä¾›æœ‰å…³å…¶äº¤äº’æ–¹æ³•çš„æ˜ç¡®æŒ‡ä»¤çš„æ–¹æ³•ã€‚ä½†æ˜¯ï¼Œé‡è¦çš„æ˜¯è¦äº†è§£ç³»ç»Ÿå‘½ä»¤ä¸æ˜¯å…ˆå†³æ¡ä»¶ï¼Œå¹¶ä¸”æ²¡æœ‰ç³»ç»Ÿå‘½ä»¤çš„æ¨¡å‹çš„é»˜è®¤ä¸¾æ­¢å¯èƒ½ä¸â€œä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹â€çš„è¡Œä¸ºéå¸¸ç›¸ä¼¼ã€‚</p>
<p>The roles that you can have areÂ <code>[&quot;system&quot;, &quot;user&quot;, &quot;assistant&quot;]</code>.<br>
æ‚¨å¯ä»¥æ‹¥æœ‰çš„è§’è‰²æ˜¯Â <code>[&quot;system&quot;, &quot;user&quot;, &quot;assistant&quot;]</code>Â ã€‚</p>
<p><code>&quot;content&quot;: &quot;Some content&quot;</code>Â is where you place the prompt or responses from a language model, depending upon the messageâ€™s role. ItÂ can be eitherÂ <code>&quot;assistant&quot;</code>,Â <code>&quot;system&quot;</code>, orÂ <code>&quot;user&quot;</code>.<br>
<code>&quot;content&quot;: &quot;Some content&quot;</code>Â æ˜¯æ”¾ç½®æç¤ºæˆ–æ¥è‡ªè¯­è¨€æ¨¡å‹çš„å“åº”çš„ä½ç½®ï¼Œå…·ä½“å–å†³äºæ¶ˆæ¯çš„è§’è‰²ã€‚å®ƒå¯ä»¥æ˜¯Â <code>&quot;assistant&quot;</code>Â ï¼ŒÂ <code>&quot;system&quot;</code>Â æˆ–Â <code>&quot;user&quot;</code>Â ã€‚</p>
<h1 id="sentiment-analysisæƒ…ç»ªåˆ†æ">Sentiment AnalysisÂ æƒ…ç»ªåˆ†æ</h1>
<p><em>Sentiment analysis</em>Â is a widely used NLPÂ technique that helps in identifying, extracting, and understanding the emotions, opinions, or sentiments expressed in a piece of text. By leveraging the power of LLMs like GPT-4, sentiment analysis has become an essential tool for businesses, researchers, and developers across various industries.<br>
æƒ…æ„Ÿåˆ†ææ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„ NLP æŠ€æœ¯ï¼Œæœ‰åŠ©äºè¯†åˆ«ã€æå–å’Œç†è§£ä¸€æ®µæ–‡æœ¬ä¸­è¡¨è¾¾çš„æƒ…ç»ªã€è§‚ç‚¹æˆ–æƒ…æ„Ÿã€‚é€šè¿‡åˆ©ç”¨ GPT-4 ç­‰ LLMs çš„å¼ºå¤§åŠŸèƒ½ï¼Œæƒ…æ„Ÿåˆ†æå·²æˆä¸ºå„è¡Œå„ä¸šçš„ä¼ä¸šã€ç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜çš„é‡è¦å·¥å…·ã€‚</p>
<p>The primary goal of sentiment analysis is to determine the attitude or emotional tone conveyed in a text, whether itâ€™s positive, negative, or neutral. This information can provide valuable insights into consumer opinions about products or services, help monitor brand reputation, and even assist in predicting market trends.<br>
æƒ…æ„Ÿåˆ†æçš„ä¸»è¦ç›®æ ‡æ˜¯ç¡®å®šæ–‡æœ¬ä¸­ä¼ è¾¾çš„æ€åº¦æˆ–æƒ…æ„ŸåŸºè°ƒï¼Œæ— è®ºæ˜¯ç§¯æçš„ã€æ¶ˆæçš„è¿˜æ˜¯ä¸­ç«‹çš„ã€‚è¿™äº›ä¿¡æ¯å¯ä»¥ä¸ºæ¶ˆè´¹è€…å¯¹äº§å“æˆ–æœåŠ¡çš„æ„è§æä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œå¸®åŠ©ç›‘æ§å“ç‰Œå£°èª‰ï¼Œç”šè‡³å¸®åŠ©é¢„æµ‹å¸‚åœºè¶‹åŠ¿ã€‚</p>
<p>The following are several prompt engineering techniques for creating effective sentiment analysis prompts:<br>
ä»¥ä¸‹æ˜¯ç”¨äºåˆ›å»ºæœ‰æ•ˆæƒ…ç»ªåˆ†ææç¤ºçš„å‡ ç§æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼š</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Is this text positive or negative?</p>
<p>I absolutely love the design of this phone, but the battery life is quite
disappointing.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>The text has a mixed tone, as it contains both positive and negative
aspects. The positive part is &ldquo;I absolutely love the design of this phone,&rdquo;
while the negative part is &ldquo;the battery life is quite disappointing.&rdquo;</p>
<p>Although GPT-4 identifies a â€œmixed tone,â€ the outcome is a result of several shortcomings in the prompt:<br>
å°½ç®¡ GPT-4 è¯†åˆ«å‡ºâ€œæ··åˆè¯­æ°”â€ï¼Œä½†ç»“æœæ˜¯æç¤ºä¸­å‡ ä¸ªç¼ºç‚¹çš„ç»“æœï¼š</p>
<p>Lack of clarityÂ ç¼ºä¹æ˜ç¡®æ€§</p>
<p>The prompt does not clearly define the desired output format.<br>
æç¤ºç¬¦æœªæ˜ç¡®å®šä¹‰æ‰€éœ€çš„è¾“å‡ºæ ¼å¼ã€‚</p>
<p>Insufficient examplesÂ ç¤ºä¾‹ä¸è¶³</p>
<p>The prompt does not include any examples of positive, negative, or neutral sentiments, which could help guide the LLM in understanding the distinctions between them.<br>
è¯¥æç¤ºä¸åŒ…æ‹¬ä»»ä½•ç§¯æã€æ¶ˆææˆ–ä¸­ç«‹æƒ…ç»ªçš„ç¤ºä¾‹ï¼Œè¿™å¯èƒ½æœ‰åŠ©äºæŒ‡å¯¼ LLM ç†è§£å®ƒä»¬ä¹‹é—´çš„åŒºåˆ«ã€‚</p>
<p>No guidance on handling mixed sentiments<br>
æ²¡æœ‰å…³äºå¤„ç†æ··åˆæƒ…ç»ªçš„æŒ‡å¯¼</p>
<p>The prompt does not specify how to handle cases where the text contains a mix of positive and negative sentiments.<br>
æç¤ºæ²¡æœ‰æŒ‡å®šå¦‚ä½•å¤„ç†æ–‡æœ¬åŒ…å«ç§¯æå’Œæ¶ˆææƒ…ç»ªæ··åˆçš„æƒ…å†µã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Using the following examples as a guide:
positive: &lsquo;I absolutely love the design of this phone!&rsquo;
negative: &lsquo;The battery life is quite disappointing.&rsquo;
neutral: &lsquo;I liked the product, but it has short battery life.&rsquo;</p>
<p>Only return either a single word of:</p>
<ul>
<li>positive</li>
<li>negative</li>
<li>neutral</li>
</ul>
<p>Please classify the sentiment of the following text as positive, negative,
or neutral: I absolutely love the design of this phone, but the battery
life is quite disappointing.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>neutral</p>
<p>This prompt is much better because it:<br>
è¿™ä¸ªæç¤ºè¦å¥½å¾—å¤šï¼Œå› ä¸ºå®ƒï¼š</p>
<p>Provides clear instructions<br>
æä¾›æ¸…æ™°çš„è¯´æ˜</p>
<p>The prompt clearly states the task, which is to classify the sentiment of the given text into one of three categories: positive, negative, or neutral.<br>
æç¤ºæ¸…æ¥šåœ°è¯´æ˜äº†ä»»åŠ¡ï¼Œå³å°†ç»™å®šæ–‡æœ¬çš„æƒ…ç»ªåˆ†ä¸ºä¸‰ç±»ä¹‹ä¸€ï¼šç§¯æã€æ¶ˆææˆ–ä¸­ç«‹ã€‚</p>
<p>Offers examplesÂ æä¾›ç¤ºä¾‹</p>
<p>The prompt provides examples for each of the sentiment categories, which helps in understanding the context and desired output.<br>
è¯¥æç¤ºä¸ºæ¯ä¸ªæƒ…ç»ªç±»åˆ«æä¾›äº†ç¤ºä¾‹ï¼Œè¿™æœ‰åŠ©äºç†è§£ä¸Šä¸‹æ–‡å’Œæ‰€éœ€çš„è¾“å‡ºã€‚</p>
<p>Defines the output format<br>
å®šä¹‰è¾“å‡ºæ ¼å¼</p>
<p>The prompt specifies that the output should be a single word, ensuring that the response isÂ concise and easy to understand.<br>
æç¤ºæŒ‡å®šè¾“å‡ºåº”ä¸ºå•ä¸ªå•è¯ï¼Œç¡®ä¿å“åº”ç®€æ´æ˜“æ‡‚ã€‚</p>
<h2 id="techniques-for-improving-sentiment-analysis">Techniques for Improving Sentiment Analysis</h2>
<p>æ”¹è¿›æƒ…ç»ªåˆ†æçš„æŠ€æœ¯</p>
<p>To enhance sentiment analysis accuracy, preprocessingÂ the input text is a vital step. This involves the following:<br>
ä¸ºäº†æé«˜æƒ…æ„Ÿåˆ†æçš„å‡†ç¡®æ€§ï¼Œå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦çš„æ­¥éª¤ã€‚è¿™æ¶‰åŠä»¥ä¸‹å†…å®¹ï¼š</p>
<p>Special characters removal<br>
ç‰¹æ®Šå­—ç¬¦åˆ é™¤</p>
<p>Exceptional charactersÂ such as emojis, hashtags, and punctuation may skew the rule-based sentiment algorithmâ€™s judgment. Besides, these characters might not be recognized by machine learning and deep learning models, resulting in misclassification.<br>
è¡¨æƒ…ç¬¦å·ã€ä¸»é¢˜æ ‡ç­¾å’Œæ ‡ç‚¹ç¬¦å·ç­‰ç‰¹æ®Šå­—ç¬¦å¯èƒ½ä¼šæ‰­æ›²åŸºäºè§„åˆ™çš„æƒ…æ„Ÿç®—æ³•çš„åˆ¤æ–­ã€‚æ­¤å¤–ï¼Œæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹å¯èƒ½æ— æ³•è¯†åˆ«è¿™äº›å­—ç¬¦ï¼Œä»è€Œå¯¼è‡´åˆ†ç±»é”™è¯¯ã€‚</p>
<p>Lowercase conversionÂ å°å†™è½¬æ¢</p>
<p>Converting allÂ the characters to lowercase aids in creating uniformity. For instance, words likeÂ <em>Happy</em>Â andÂ <em>happy</em>Â are treated as different words by models, which can cause duplication and inaccuracies.<br>
å°†æ‰€æœ‰å­—ç¬¦è½¬æ¢ä¸ºå°å†™æœ‰åŠ©äºåˆ›å»ºç»Ÿä¸€æ€§ã€‚ä¾‹å¦‚ï¼Œåƒâ€œå¿«ä¹â€å’Œâ€œå¿«ä¹â€è¿™æ ·çš„è¯è¢«æ¨¡å‹è§†ä¸ºä¸åŒçš„è¯ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´é‡å¤å’Œä¸å‡†ç¡®ã€‚</p>
<p>Spelling correctionÂ æ‹¼å†™æ›´æ­£</p>
<p>Spelling errorsÂ can cause misinterpretation and misclassification. Creating a spell-check pipeline can significantly reduce such errors and improve results.<br>
æ‹¼å†™é”™è¯¯å¯èƒ½ä¼šå¯¼è‡´è¯¯è§£å’Œé”™è¯¯åˆ†ç±»ã€‚åˆ›å»ºæ‹¼å†™æ£€æŸ¥ç®¡é“å¯ä»¥æ˜¾è‘—å‡å°‘æ­¤ç±»é”™è¯¯å¹¶æ”¹å–„ç»“æœã€‚</p>
<p>For industry- or domain-specific text, embedding domain-specific content in the prompt helps in navigating the LLMâ€™s sense of the textâ€™s framework and sentiment. It enhances accuracy in the classification and provides a heightened understanding of particular jargon and expressions.<br>
å¯¹äºç‰¹å®šäºè¡Œä¸šæˆ–é¢†åŸŸçš„æ–‡æœ¬ï¼Œåœ¨æç¤ºä¸­åµŒå…¥ç‰¹å®šäºé¢†åŸŸçš„å†…å®¹æœ‰åŠ©äºå¯¼èˆª LLM å¯¹æ–‡æœ¬æ¡†æ¶å’Œæƒ…ç»ªçš„ç†è§£ã€‚å®ƒæé«˜äº†åˆ†ç±»çš„å‡†ç¡®æ€§ï¼Œå¹¶æä¾›äº†å¯¹ç‰¹å®šè¡Œè¯å’Œè¡¨è¾¾æ–¹å¼çš„æ›´é«˜ç†è§£ã€‚</p>
<h2 id="limitations-and-challenges-in-sentiment-analysis">Limitations and Challenges in Sentiment Analysis</h2>
<p>æƒ…æ„Ÿåˆ†æçš„å±€é™æ€§å’ŒæŒ‘æˆ˜</p>
<p>Despite the advancements in LLMs and the application of prompt engineering techniques, sentiment analysis still faces some limitations and challenges:<br>
å°½ç®¡ LLMs å–å¾—äº†è¿›æ­¥ï¼Œå¹¶åº”ç”¨äº†æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œä½†æƒ…æ„Ÿåˆ†æä»ç„¶é¢ä¸´ä¸€äº›é™åˆ¶å’ŒæŒ‘æˆ˜ï¼š</p>
<p>Handling sarcasm and irony<br>
å¤„ç†è®½åˆºå’Œè®½åˆº</p>
<p>DetectingÂ sarcasm and irony in text can be difficult for LLMs, as it often requires understanding the context and subtle cues that humans can easily recognize. Misinterpreting sarcastic or ironic statements may lead to inaccurate sentiment classification.<br>
å¯¹äºLLMsæ¥è¯´ï¼Œæ£€æµ‹æ–‡æœ¬ä¸­çš„è®½åˆºå’Œè®½åˆºå¯èƒ½å¾ˆå›°éš¾ï¼Œå› ä¸ºå®ƒé€šå¸¸éœ€è¦äº†è§£äººç±»å¯ä»¥è½»æ¾è¯†åˆ«çš„ä¸Šä¸‹æ–‡å’Œå¾®å¦™çš„çº¿ç´¢ã€‚è¯¯è§£è®½åˆºæˆ–è®½åˆºçš„é™ˆè¿°å¯èƒ½ä¼šå¯¼è‡´ä¸å‡†ç¡®çš„æƒ…ç»ªåˆ†ç±»ã€‚</p>
<p>Identifying context-specific sentiment<br>
è¯†åˆ«ç‰¹å®šäºä¸Šä¸‹æ–‡çš„æƒ…ç»ª</p>
<p>SentimentÂ analysis can be challenging when dealing with context-specific sentiments, such as those related to domain-specific jargon or cultural expressions. LLMs may struggle to accurately classify sentiments inÂ these cases without proper guidance or domain-specific examples.<br>
åœ¨å¤„ç†ç‰¹å®šäºä¸Šä¸‹æ–‡çš„æƒ…ç»ªæ—¶ï¼Œä¾‹å¦‚ä¸ç‰¹å®šé¢†åŸŸæœ¯è¯­æˆ–æ–‡åŒ–è¡¨è¾¾ç›¸å…³çš„æƒ…ç»ªï¼Œæƒ…ç»ªåˆ†æå¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚LLMs åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œå¦‚æœæ²¡æœ‰é€‚å½“çš„æŒ‡å¯¼æˆ–ç‰¹å®šé¢†åŸŸçš„ç¤ºä¾‹ï¼Œå¯èƒ½å¾ˆéš¾å‡†ç¡®åœ°å¯¹æƒ…ç»ªè¿›è¡Œåˆ†ç±»ã€‚</p>
<h1 id="least-to-mostä»æœ€å°‘åˆ°æœ€å¤š">Least to MostÂ ä»æœ€å°‘åˆ°æœ€å¤š</h1>
<p>TheÂ <em>least to most</em>Â technique in prompt engineering is a powerful method for sequentially generating or extractingÂ increasingly detailed knowledge on a given topic. This method is particularly effective when dealing with complex subjects or when a high level of detail is necessary.<br>
æç¤ºå·¥ç¨‹ä¸­çš„æœ€å°åˆ°å¤§å¤šæ•°æŠ€æœ¯æ˜¯ä¸€ç§å¼ºå¤§çš„æ–¹æ³•ï¼Œç”¨äºæŒ‰é¡ºåºç”Ÿæˆæˆ–æå–æœ‰å…³ç»™å®šä¸»é¢˜çš„è¶Šæ¥è¶Šè¯¦ç»†çš„çŸ¥è¯†ã€‚è¿™ç§æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„ä¸»é¢˜æˆ–éœ€è¦é«˜åº¦è¯¦ç»†æ—¶ç‰¹åˆ«æœ‰æ•ˆã€‚</p>
<p>Least to most uses aÂ <em>chain</em>Â of prompts where each new prompt is based on the last answer. This step-by-step approach helps gather more detailed information each time, making it easier to dive deeper into any topic.<br>
ä»æœ€å°‘åˆ°å¤§å¤šæ•°ä½¿ç”¨ä¸€è¿ä¸²æç¤ºï¼Œå…¶ä¸­æ¯ä¸ªæ–°æç¤ºéƒ½åŸºäºæœ€åä¸€ä¸ªç­”æ¡ˆã€‚è¿™ç§å¾ªåºæ¸è¿›çš„æ–¹æ³•æœ‰åŠ©äºæ¯æ¬¡æ”¶é›†æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œä»è€Œæ›´å®¹æ˜“æ›´æ·±å…¥åœ°ç ”ç©¶ä»»ä½•ä¸»é¢˜ã€‚</p>
<p>This technique can also be applied to code generation, as demonstrated in a FlaskÂ <code>Hello World</code>Â app example.<br>
æ­¤æŠ€æœ¯ä¹Ÿå¯ä»¥åº”ç”¨äºä»£ç ç”Ÿæˆï¼Œå¦‚ FlaskÂ <code>Hello World</code>Â åº”ç”¨ç¤ºä¾‹ä¸­æ‰€ç¤ºã€‚</p>
<h2 id="planning-the-architecture">Planning the Architecture</h2>
<p>è§„åˆ’ä½“ç³»ç»“æ„</p>
<p>Before diving into the architecture, letâ€™s brieflyÂ understand what Flask is.Â <a href="https://oreil.ly/7N-bs">Flask</a>Â is a lightweight web application framework in Python, widely used for creating web applications quickly and with minimal code. (Flask is only used for demonstration purposes here and isnâ€™t included within theÂ <a href="https://oreil.ly/TRK0i"><em>requirements.txt</em>Â file</a>Â for the book.<br>
åœ¨æ·±å…¥ç ”ç©¶æ¶æ„ä¹‹å‰ï¼Œè®©æˆ‘ä»¬ç®€è¦äº†è§£ä¸€ä¸‹ Flask æ˜¯ä»€ä¹ˆã€‚Flask æ˜¯ Python ä¸­çš„è½»é‡çº§ Web åº”ç”¨ç¨‹åºæ¡†æ¶ï¼Œå¹¿æ³›ç”¨äºä»¥æœ€å°‘çš„ä»£ç å¿«é€Ÿåˆ›å»º Web åº”ç”¨ç¨‹åºã€‚ï¼ˆæ­¤å¤„ä»…ç”¨äºæ¼”ç¤ºç›®çš„ï¼ŒæœªåŒ…å«åœ¨æœ¬ä¹¦çš„requirements.txtæ–‡ä»¶ä¸­ã€‚</p>
<p>Now, letâ€™s ask an LLM to outline the basic architecture for a simple Flask â€œHello Worldâ€ application.<br>
ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¯· LLM æ¦‚è¿°ä¸€ä¸ªç®€å•çš„ Flask â€œHello Worldâ€ åº”ç”¨ç¨‹åºçš„åŸºæœ¬æ¶æ„ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Outline the basic architecture for a simple Flask &ldquo;Hello World&rdquo; application.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Sure! Here&rsquo;s an outline for a basic Flask &ldquo;Hello World&rdquo; application:</p>
<p>Import Flask module: Import the Flask module
to use its (((&ldquo;Flask module code generation&rdquo;)))functionality.</p>
<p>Create an instance of the Flask application: Create an instance of the
Flask class to represent your web application.
Define a route and view function: &hellip;
Implement the view function: &hellip;
Run the application: Start the Flask development server to run the application.</p>
<h2 id="coding-individual-functions">Coding Individual Functions</h2>
<p>å¯¹å•ä¸ªå‡½æ•°è¿›è¡Œç¼–ç </p>
<p>Before coding, letâ€™s clarifyÂ what a Flask route is: itâ€™s a function linked to a URL pattern that determines what users seeÂ on a specific web page. Next, weâ€™ll provide the code snippet for a simple â€œHello Worldâ€ route in Flask.<br>
åœ¨ç¼–ç ä¹‹å‰ï¼Œè®©æˆ‘ä»¬æ¾„æ¸…ä¸€ä¸‹ Flask è·¯ç”±æ˜¯ä»€ä¹ˆï¼šå®ƒæ˜¯ä¸€ä¸ªé“¾æ¥åˆ° URL æ¨¡å¼çš„å‡½æ•°ï¼Œç”¨äºç¡®å®šç”¨æˆ·åœ¨ç‰¹å®šç½‘é¡µä¸Šçœ‹åˆ°çš„å†…å®¹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åœ¨ Flask ä¸­æä¾›ç®€å•çš„â€œHello Worldâ€è·¯ç”±çš„ä»£ç ç‰‡æ®µã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Provide a code snippet for a simple &ldquo;Hello World&rdquo; route in Flask.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Certainly! Here&rsquo;s a simple code snippet for a &ldquo;Hello World&rdquo; route using Flask:</p>
<p>from flask import Flask</p>
<p>app = Flask(<strong>name</strong>)</p>
<p>@app.route(&rsquo;/&rsquo;)
def hello_world():
return &lsquo;Hello, World!&rsquo;</p>
<p>if <strong>name</strong> == &lsquo;<strong>main</strong>&rsquo;:
app.run()</p>
<p>In this code, we import the Flask module and create a Flask application instance namedÂ <code>app</code>. We then define a route using theÂ <code>@app.route</code>Â decorator, specifying the root URL pathÂ <code>'/'</code>. TheÂ <code>hello_world</code>Â function is associated with this route, and it returns the stringÂ <code>'Hello, World!'</code>Â as the response.<br>
åœ¨æ­¤ä»£ç ä¸­ï¼Œæˆ‘ä»¬å¯¼å…¥ Flask æ¨¡å—å¹¶åˆ›å»ºä¸€ä¸ªåä¸ºÂ <code>app</code>Â çš„ Flask åº”ç”¨ç¨‹åºå®ä¾‹ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨Â <code>@app.route</code>Â è£…é¥°å™¨å®šä¹‰è·¯ç”±ï¼ŒæŒ‡å®šæ ¹ URL è·¯å¾„Â <code>'/'</code>Â ã€‚Â <code>hello_world</code>Â å‡½æ•°ä¸æ­¤è·¯ç”±ç›¸å…³è”ï¼Œå®ƒè¿”å›å­—ç¬¦ä¸²Â <code>'Hello, World!'</code>Â ä½œä¸ºå“åº”ã€‚</p>
<p>The Flask application will start by callingÂ <code>app.run()</code>Â when your Python script is executed directly.<br>
å½“ç›´æ¥æ‰§è¡Œ Python è„šæœ¬æ—¶ï¼ŒFlask åº”ç”¨ç¨‹åºå°†é€šè¿‡è°ƒç”¨Â <code>app.run()</code>Â å¯åŠ¨ã€‚</p>
<h2 id="adding-testsæ·»åŠ æµ‹è¯•">Adding TestsÂ æ·»åŠ æµ‹è¯•</h2>
<p>Finally, letâ€™s ask it to generate aÂ test case for the â€œHello Worldâ€ route in the Flask application.<br>
æœ€åï¼Œè®©æˆ‘ä»¬è¦æ±‚å®ƒåœ¨ Flask åº”ç”¨ç¨‹åºä¸­ä¸ºâ€œHello Worldâ€è·¯ç”±ç”Ÿæˆä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Provide an example of a test case for the &ldquo;Hello World&rdquo; route in a Flask
application.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Sure! Here&rsquo;s an example of a test case for the &ldquo;Hello World&rdquo; route in a
Flask application:</p>
<p>import unittest
from flask import Flask
from flask.testing import FlaskClient</p>
<h1 id="create-the-flask-application">Create the Flask application</h1>
<p>app = Flask(<strong>name</strong>)</p>
<h1 id="create-the-route">Create the route</h1>
<p>@app.route(&rsquo;/&rsquo;)
def hello_world():
return &lsquo;Hello, World!&rsquo;</p>
<p>class FlaskAppTestCase(unittest.TestCase):
def setUp(self):
# Create a test client
self.app = app.test_client()</p>
<pre><code>def test_hello_world_route(self):
    # Send a GET request to the route
    response = self.app.get('/')

    # Check the response status code
    self.assertEqual(response.status_code, 200)

    # Check the response data
    self.assertEqual(response.data, b'Hello, World!')
</code></pre>
<p>if <strong>name</strong> == &lsquo;<strong>main</strong>&rsquo;:
unittest.main()</p>
<p>By using the least to most prompt technique, you can gradually build up the knowledge and understandingÂ required for the task at hand.<br>
é€šè¿‡ä½¿ç”¨æœ€å°‘åˆ°æœ€åŠæ—¶çš„æŠ€æœ¯ï¼Œæ‚¨å¯ä»¥é€æ¸å»ºç«‹æ‰‹å¤´ä»»åŠ¡æ‰€éœ€çš„çŸ¥è¯†å’Œç†è§£ã€‚</p>
<h2 id="benefits-of-the-least-to-most-technique">Benefits of the Least to Most Technique</h2>
<p>ä»æœ€å°‘åˆ°æœ€å¤šæŠ€æœ¯çš„å¥½å¤„</p>
<p>This method is particularly useful forÂ complex tasks, as it allows an LLM to generate relevant knowledge that will subsequently be used as context for future tasks.<br>
æ­¤æ–¹æ³•å¯¹äºå¤æ‚ä»»åŠ¡ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå…è®¸ LLM ç”Ÿæˆç›¸å…³çŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†éšåå°†ç”¨ä½œæœªæ¥ä»»åŠ¡çš„ä¸Šä¸‹æ–‡ã€‚</p>
<p>Letâ€™s dive deeper into the benefits of using this approach in various other scenarios:<br>
è®©æˆ‘ä»¬æ›´æ·±å…¥åœ°äº†è§£åœ¨å…¶ä»–å„ç§æ–¹æ¡ˆä¸­ä½¿ç”¨æ­¤æ–¹æ³•çš„å¥½å¤„ï¼š</p>
<p>Progressive explorationÂ æ¸è¿›å¼æ¢ç´¢</p>
<p>Breaking a complex problem into smaller tasks allows an LLM to provide more detailed and accurate information at each step. This approach is especially helpful when working with a new subject matter or a multifaceted problem.<br>
å°†å¤æ‚çš„é—®é¢˜åˆ†è§£ä¸ºæ›´å°çš„ä»»åŠ¡å…è®¸ LLM åœ¨æ¯ä¸ªæ­¥éª¤ä¸­æä¾›æ›´è¯¦ç»†å’Œå‡†ç¡®çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•åœ¨å¤„ç†æ–°ä¸»é¢˜æˆ–å¤šæ–¹é¢é—®é¢˜æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚</p>
<p>FlexibilityÂ çµæ´»æ€§</p>
<p>The least to most technique offers flexibility in addressing different aspects of a problem. It enables you to pivot, explore alternative solutions, or dive deeper into specific areas as needed.<br>
ä»æœ€å°‘åˆ°æœ€å¤šçš„æŠ€æœ¯åœ¨è§£å†³é—®é¢˜çš„ä¸åŒæ–¹é¢æä¾›äº†çµæ´»æ€§ã€‚å®ƒä½¿æ‚¨èƒ½å¤Ÿæ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´ã€æ¢ç´¢æ›¿ä»£è§£å†³æ–¹æ¡ˆæˆ–æ·±å…¥ç ”ç©¶ç‰¹å®šé¢†åŸŸã€‚</p>
<p>Improved comprehensionÂ æé«˜ç†è§£åŠ›</p>
<p>By breaking down a task into smaller steps, an LLM can deliver information in a more digestible format, making it easier for you to understand and follow.<br>
é€šè¿‡å°†ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°çš„æ­¥éª¤ï¼ŒLLM å¯ä»¥ä»¥æ›´æ˜“äºç†è§£çš„æ ¼å¼ä¼ é€’ä¿¡æ¯ï¼Œè®©æ‚¨æ›´å®¹æ˜“ç†è§£å’Œéµå¾ªã€‚</p>
<p>Collaborative learningÂ åä½œå­¦ä¹ </p>
<p>This technique promotes collaboration between you and an LLM, as it encourages an iterative process of refining the output and adjusting your responses to achieve the desired outcome.<br>
è¿™ç§æŠ€æœ¯ä¿ƒè¿›äº†ä½ å’ŒLLMä¹‹é—´çš„åä½œï¼Œå› ä¸ºå®ƒé¼“åŠ±ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹æ¥å®Œå–„è¾“å‡ºå’Œè°ƒæ•´ä½ çš„å“åº”ï¼Œä»¥å®ç°é¢„æœŸçš„ç»“æœã€‚</p>
<h2 id="challenges-with-the-least-to-most-technique">Challenges with the Least to Most Technique</h2>
<p>ä»æœ€å°‘åˆ°æœ€å¤šçš„æŠ€æœ¯æŒ‘æˆ˜</p>
<p>Overreliance on previously generatedÂ knowledge<br>
è¿‡åº¦ä¾èµ–å…ˆå‰ç”Ÿæˆçš„çŸ¥è¯†</p>
<p>Using previous chat history to store the state may lead to larger tasks forgetting their initial tasks/outputs due to limitations in context length.<br>
ç”±äºä¸Šä¸‹æ–‡é•¿åº¦çš„é™åˆ¶ï¼Œä½¿ç”¨ä»¥å‰çš„èŠå¤©å†å²è®°å½•æ¥å­˜å‚¨çŠ¶æ€å¯èƒ½ä¼šå¯¼è‡´è¾ƒå¤§çš„ä»»åŠ¡å¿˜è®°å…¶åˆå§‹ä»»åŠ¡/è¾“å‡ºã€‚</p>
<p>Dependence on prior prompts<br>
å¯¹å…ˆå‰æç¤ºçš„ä¾èµ–æ€§</p>
<p>Since each prompt is built upon preceding outputs, it is imperative to ensure that the quality and responses of previous prompts provide ample information forÂ the next step.<br>
ç”±äºæ¯ä¸ªæç¤ºéƒ½æ˜¯åŸºäºå‰é¢çš„è¾“å‡ºæ„å»ºçš„ï¼Œå› æ­¤å¿…é¡»ç¡®ä¿å…ˆå‰æç¤ºçš„è´¨é‡å’Œå“åº”ä¸ºä¸‹ä¸€æ­¥æä¾›å……è¶³çš„ä¿¡æ¯ã€‚</p>
<h1 id="evaluate-qualityè¯„ä¼°è´¨é‡">EVALUATE QUALITYÂ è¯„ä¼°è´¨é‡</h1>
<p>In the process of designing prompts, makeÂ sure to evaluate the quality of each prior LLM response. The performance of the next task depends upon the quality of information provided by the preceding one.<br>
åœ¨è®¾è®¡æç¤ºçš„è¿‡ç¨‹ä¸­ï¼Œä¸€å®šè¦è¯„ä¼°æ¯ä¸ªå…ˆå‰çš„LLMå“åº”çš„è´¨é‡ã€‚ä¸‹ä¸€ä¸ªä»»åŠ¡çš„æ‰§è¡Œå–å†³äºå‰ä¸€ä¸ªä»»åŠ¡æä¾›çš„ä¿¡æ¯çš„è´¨é‡ã€‚</p>
<h1 id="role-promptingè§’è‰²æç¤º">Role PromptingÂ è§’è‰²æç¤º</h1>
<p><em>Role prompting</em>Â is a technique in whichÂ the AI is given a specific role or character to assume while generating a response. This helps guide the AIâ€™s response style and content to better align with the userâ€™s needs or desired outcome.<br>
è§’è‰²æç¤ºæ˜¯ä¸€ç§æŠ€æœ¯ï¼Œåœ¨è¿™ç§æŠ€æœ¯ä¸­ï¼Œäººå·¥æ™ºèƒ½åœ¨ç”Ÿæˆå“åº”æ—¶è¢«èµ‹äºˆç‰¹å®šçš„è§’è‰²æˆ–è§’è‰²ã€‚è¿™æœ‰åŠ©äºæŒ‡å¯¼ AI çš„å“åº”é£æ ¼å’Œå†…å®¹æ›´å¥½åœ°ä¸ç”¨æˆ·çš„éœ€æ±‚æˆ–æœŸæœ›çš„ç»“æœä¿æŒä¸€è‡´ã€‚</p>
<h6 id="noteæ³¨æ„-2">NOTEÂ æ³¨æ„</h6>
<p><a href="https://oreil.ly/8pf40">Awesome ChatGPT prompts</a>Â provides an extensive list of role prompts that you can use.<br>
å¾ˆæ£’çš„ ChatGPT æç¤ºæä¾›äº†æ‚¨å¯ä»¥ä½¿ç”¨çš„å¤§é‡è§’è‰²æç¤ºåˆ—è¡¨ã€‚</p>
<p>Letâ€™s ask ChatGPT to take on the role of a tech reviewer for MongoDB.<br>
è®©æˆ‘ä»¬è®© ChatGPT æ‰®æ¼” MongoDB çš„æŠ€æœ¯è¯„è®ºå‘˜çš„è§’è‰²ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>I want you to act as a tech reviewer. I will give you the name of a piece
of technology and you will provide me with an in-depth review - including
pros, cons, features, and comparisons to other technologies on the market.
My first suggestion request is: MongoDB.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>MongoDB is a popular NoSQL, document-oriented database management system
designed to handle high-volume data storage and retrieval.</p>
<p>Features:</p>
<ol>
<li>Document-Oriented Storage: MongoDB stores data in a flexible, JSON-like
format called BSON, making it easy to store complex data structures like
arrays and nested objects.
&hellip;</li>
</ol>
<p>Pros:</p>
<ol>
<li>High Performance: MongoDB&rsquo;s indexing and caching capabilities deliver
fast read and write operations.
&hellip;</li>
</ol>
<p>Cons:</p>
<ol>
<li>Limited Support for Transactions: While MongoDB supports multidocument
ACID transactions, it may not be the best choice for applications that
require complex transactional operations.
&hellip;</li>
</ol>
<p>Comparisons to other technologies:</p>
<ol>
<li>Couchbase, another popular NoSQL database, offers similar features to
MongoDB, including flexible schema and JSON-based storage. However,
Couchbase is more focused on distributed caching and may be more
suitable for applications requiring low-latency data access.</li>
</ol>
<p>Conclusion:</p>
<p>MongoDB is a powerful and versatile NoSQL database solution suitable for
various applications, particularly those requiring high performance and
scalability. Its flexible data model, rich query language, and strong
consistency make it a popular choice among developers.</p>
<p>The prompt is an excellent example of role prompting, as it clearly defines the role the AI should assume (a tech reviewer) and sets expectations for the type of responseÂ desired (an in-depth review of MongoDB).<br>
æç¤ºæ˜¯è§’è‰²æç¤ºçš„ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼Œå› ä¸ºå®ƒæ¸…æ¥šåœ°å®šä¹‰äº† AI åº”è¯¥æ‰¿æ‹…çš„è§’è‰²ï¼ˆæŠ€æœ¯å®¡æŸ¥è€…ï¼‰ï¼Œå¹¶ä¸ºæ‰€éœ€çš„å“åº”ç±»å‹è®¾å®šäº†æœŸæœ›ï¼ˆå¯¹ MongoDB çš„æ·±å…¥å®¡æŸ¥ï¼‰ã€‚</p>
<h1 id="give-directionæŒ‡æ˜æ–¹å‘">GIVE DIRECTIONÂ æŒ‡æ˜æ–¹å‘</h1>
<p>When crafting prompts, considerÂ assigning a specific role to the AI. This sets the proper context for the response, creating a more focused and relevant output.<br>
åœ¨åˆ¶ä½œæç¤ºæ—¶ï¼Œè¯·è€ƒè™‘ä¸º AI åˆ†é…ç‰¹å®šè§’è‰²ã€‚è¿™ä¸ºå“åº”è®¾ç½®äº†é€‚å½“çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œåˆ›å»ºäº†æ›´æœ‰é’ˆå¯¹æ€§å’Œç›¸å…³æ€§çš„è¾“å‡ºã€‚</p>
<h1 id="benefits-of-role-prompting">Benefits of Role Prompting</h1>
<p>è§’è‰²æç¤ºçš„å¥½å¤„</p>
<p>Role prompting helps narrow downÂ the AIâ€™s responses, ensuring more focused, contextually appropriate, and tailored results. It can also enhance creativity by pushing the AI to think and respond from unique perspectives.<br>
è§’è‰²æç¤ºæœ‰åŠ©äºç¼©å° AI çš„å“åº”èŒƒå›´ï¼Œç¡®ä¿æ›´é›†ä¸­ã€æ›´ç¬¦åˆä¸Šä¸‹æ–‡å’Œé‡èº«å®šåˆ¶çš„ç»“æœã€‚å®ƒè¿˜å¯ä»¥é€šè¿‡æ¨åŠ¨äººå·¥æ™ºèƒ½ä»ç‹¬ç‰¹çš„è§’åº¦æ€è€ƒå’Œå“åº”æ¥å¢å¼ºåˆ›é€ åŠ›ã€‚</p>
<h1 id="challenges-of-role-prompting">Challenges of Role Prompting</h1>
<p>è§’è‰²æç¤ºçš„æŒ‘æˆ˜</p>
<p>Role prompting can pose certainÂ challenges. There might be potential risks for bias or stereotyping based on the role assigned. Assigning stereotyped roles can lead to generating biased responses, which could harm usability or offend individuals.Â Additionally,Â maintaining consistency in the role throughout an extended interaction can be difficult. The model might drift off-topic or respond with information irrelevant to the assigned role.<br>
è§’è‰²æç¤ºå¯èƒ½ä¼šå¸¦æ¥æŸäº›æŒ‘æˆ˜ã€‚æ ¹æ®åˆ†é…çš„è§’è‰²ï¼Œå¯èƒ½å­˜åœ¨åè§æˆ–åˆ»æ¿å°è±¡çš„æ½œåœ¨é£é™©ã€‚åˆ†é…åˆ»æ¿çš„è§’è‰²å¯èƒ½ä¼šå¯¼è‡´äº§ç”Ÿæœ‰åè§çš„ååº”ï¼Œè¿™å¯èƒ½ä¼šæŸå®³å¯ç”¨æ€§æˆ–å†’çŠ¯ä¸ªäººã€‚æ­¤å¤–ï¼Œåœ¨æ•´ä¸ªæ‰©å±•äº¤äº’è¿‡ç¨‹ä¸­ä¿æŒè§’è‰²çš„ä¸€è‡´æ€§å¯èƒ½å¾ˆå›°éš¾ã€‚æ¨¡å‹å¯èƒ½ä¼šåç¦»ä¸»é¢˜ï¼Œæˆ–è€…ä½¿ç”¨ä¸åˆ†é…çš„è§’è‰²æ— å…³çš„ä¿¡æ¯è¿›è¡Œå“åº”ã€‚</p>
<h1 id="evaluate-qualityè¯„ä¼°è´¨é‡-1">EVALUATE QUALITYÂ è¯„ä¼°è´¨é‡</h1>
<p>Consistently check the qualityÂ of the LLMâ€™s responses, especially when role prompting is in play. Monitor if the AI is sticking to the role assigned or if it is veering off-topic.<br>
å§‹ç»ˆå¦‚ä¸€åœ°æ£€æŸ¥ LLM å“åº”çš„è´¨é‡ï¼Œå°¤å…¶æ˜¯åœ¨è§’è‰²æç¤ºèµ·ä½œç”¨æ—¶ã€‚ç›‘æ§ AI æ˜¯å¦åšæŒåˆ†é…çš„è§’è‰²ï¼Œæˆ–è€…æ˜¯å¦åç¦»ä¸»é¢˜ã€‚</p>
<h1 id="when-to-use-role-prompting">When to Use Role Prompting</h1>
<p>ä½•æ—¶ä½¿ç”¨è§’è‰²æç¤º</p>
<p>Role prompting is particularly usefulÂ when you want to:<br>
è§’è‰²æç¤ºåœ¨ä»¥ä¸‹æƒ…å†µä¸‹ç‰¹åˆ«æœ‰ç”¨ï¼š</p>
<p>Elicit specific expertise<br>
è·å–ç‰¹å®šä¸“ä¸šçŸ¥è¯†</p>
<p>If you need a response that requires domain knowledge or specialized expertise, role prompting can help guide the LLM to generate more informed and accurate responses.<br>
å¦‚æœæ‚¨éœ€è¦éœ€è¦é¢†åŸŸçŸ¥è¯†æˆ–ä¸“ä¸šçŸ¥è¯†çš„å“åº”ï¼Œè§’è‰²æç¤ºå¯ä»¥å¸®åŠ©æŒ‡å¯¼ LLM ç”Ÿæˆæ›´æ˜æ™ºã€æ›´å‡†ç¡®çš„å“åº”ã€‚</p>
<p>Tailor response styleÂ é‡èº«å®šåˆ¶çš„å“åº”æ–¹å¼</p>
<p>Assigning a role can help an LLM generate responses that match a specific tone, style, or perspective, such as a formal, casual, or humorous response.<br>
åˆ†é…è§’è‰²å¯ä»¥å¸®åŠ© LLM ç”Ÿæˆä¸ç‰¹å®šè¯­æ°”ã€é£æ ¼æˆ–è§‚ç‚¹ç›¸åŒ¹é…çš„å“åº”ï¼Œä¾‹å¦‚æ­£å¼ã€éšæ„æˆ–å¹½é»˜çš„å“åº”ã€‚</p>
<p>Encourage creative responses<br>
é¼“åŠ±åˆ›é€ æ€§çš„å›åº”</p>
<p>Role prompting can be used to create fictional scenarios or generate imaginative answers by assigning roles like a storyteller, a character from a novel, or a historical figure.<br>
è§’è‰²æç¤ºå¯ç”¨äºåˆ›å»ºè™šæ„åœºæ™¯æˆ–é€šè¿‡åˆ†é…è®²æ•…äº‹çš„äººã€å°è¯´ä¸­çš„äººç‰©æˆ–å†å²äººç‰©ç­‰è§’è‰²æ¥ç”Ÿæˆå¯Œæœ‰æƒ³è±¡åŠ›çš„ç­”æ¡ˆã€‚</p>
<ul>
<li>
<p><em>Explore diverse perspectives</em>: If you want to explore different viewpoints on a topic, role prompting can help by asking the AI to assume various roles or personas, allowing for a more comprehensive understanding of the subject.<br>
æ¢ç´¢ä¸åŒçš„è§‚ç‚¹ï¼šå¦‚æœæ‚¨æƒ³æ¢ç´¢æŸä¸ªä¸»é¢˜çš„ä¸åŒè§‚ç‚¹ï¼Œè§’è‰²æç¤ºå¯ä»¥é€šè¿‡è¦æ±‚ AI æ‰®æ¼”å„ç§è§’è‰²æˆ–è§’è‰²æ¥æä¾›å¸®åŠ©ï¼Œä»è€Œæ›´å…¨é¢åœ°äº†è§£è¯¥ä¸»é¢˜ã€‚</p>
</li>
<li>
<p><em>Enhance user engagement</em>: Role prompting can make interactions more engaging and entertaining by enabling an LLM to take on characters or personas that resonate with the user.<br>
å¢å¼ºç”¨æˆ·å‚ä¸åº¦ï¼šè§’è‰²æç¤ºå¯ä»¥ä½¿ LLM æ‰®æ¼”ä¸ç”¨æˆ·äº§ç”Ÿå…±é¸£çš„è§’è‰²æˆ–è§’è‰²ï¼Œä»è€Œä½¿äº¤äº’æ›´å…·å¸å¼•åŠ›å’Œå¨±ä¹æ€§ã€‚</p>
</li>
</ul>
<p>If youâ€™re using OpenAI, then the best place toÂ add a role is within theÂ <code>System Message</code>Â for chat models.<br>
å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ OpenAIï¼Œé‚£ä¹ˆæ·»åŠ è§’è‰²çš„æœ€ä½³ä½ç½®æ˜¯åœ¨èŠå¤©æ¨¡å‹çš„Â <code>System Message</code>Â ä¸­ã€‚</p>
<h1 id="gpt-prompting-tacticsgpt-æç¤ºç­–ç•¥">GPT Prompting TacticsÂ GPT æç¤ºç­–ç•¥</h1>
<p>So far youâ€™ve already covered several prompting tactics, including asking for context, text style bundling, least to most, and role prompting.<br>
åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‚¨å·²ç»ä»‹ç»äº†å‡ ç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬è¯¢é—®ä¸Šä¸‹æ–‡ã€æ–‡æœ¬æ ·å¼æ†ç»‘ã€ä»å°‘åˆ°å¤šå’Œè§’è‰²æç¤ºã€‚</p>
<p>Letâ€™s cover several more tactics, fromÂ managing potential hallucinations with appropriate reference text, to providing an LLM with criticalÂ <em>thinking time</em>, to understanding the concept ofÂ <em>task decomposition</em>â€”we have plenty for you to explore.<br>
è®©æˆ‘ä»¬ä»‹ç»æ›´å¤šçš„ç­–ç•¥ï¼Œä»ä½¿ç”¨é€‚å½“çš„å‚è€ƒæ–‡æœ¬ç®¡ç†æ½œåœ¨çš„å¹»è§‰ï¼Œåˆ°æä¾›å…·æœ‰æ‰¹åˆ¤æ€§æ€ç»´æ—¶é—´çš„LLMï¼Œå†åˆ°ç†è§£ä»»åŠ¡åˆ†è§£çš„æ¦‚å¿µâ€”â€”æˆ‘ä»¬æœ‰å¾ˆå¤šå¯ä¾›æ‚¨æ¢ç´¢çš„åœ°æ–¹ã€‚</p>
<p>These methodologies have been designed to significantly boost the precision of your AIâ€™s output and are recommended byÂ <a href="https://oreil.ly/QZE8n">OpenAI</a>. Also, each tactic utilizes one or more of the prompt engineering principles discussed inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#five_principles_01">ChapterÂ 1</a>.<br>
è¿™äº›æ–¹æ³•æ—¨åœ¨æ˜¾ç€æé«˜ AI è¾“å‡ºçš„ç²¾åº¦ï¼Œå¹¶è¢« OpenAI æ¨èã€‚æ­¤å¤–ï¼Œæ¯ç§ç­–ç•¥éƒ½åˆ©ç”¨äº†ç¬¬ 1 ç« ä¸­è®¨è®ºçš„ä¸€ä¸ªæˆ–å¤šä¸ªæç¤ºå·¥ç¨‹åŸåˆ™ã€‚</p>
<h2 id="avoiding-hallucinations-with-reference">Avoiding Hallucinations with Reference</h2>
<p>å‚è€ƒé¿å…å¹»è§‰</p>
<p>The first method for avoiding text-basedÂ hallucinations is to instruct the model toÂ <em>only answer using reference text.</em><br>
é¿å…åŸºäºæ–‡æœ¬çš„å¹»è§‰çš„ç¬¬ä¸€ç§æ–¹æ³•æ˜¯æŒ‡ç¤ºæ¨¡å‹ä»…ä½¿ç”¨å‚è€ƒæ–‡æœ¬è¿›è¡Œå›ç­”ã€‚</p>
<p>By supplying an AI model with accurate and relevant information about a given query, the model can be directed to use this information to generate its response.<br>
é€šè¿‡å‘ AI æ¨¡å‹æä¾›æœ‰å…³ç»™å®šæŸ¥è¯¢çš„å‡†ç¡®ä¸”ç›¸å…³çš„ä¿¡æ¯ï¼Œå¯ä»¥æŒ‡ç¤ºæ¨¡å‹ä½¿ç”¨æ­¤ä¿¡æ¯æ¥ç”Ÿæˆå…¶å“åº”ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Refer to the articles enclosed within triple quotes to respond to queries.</p>
<p>You must follow the following principles:</p>
<ul>
<li>In cases where the answer isn&rsquo;t found within these articles, simply
return &ldquo;I could not find an answer&rdquo;.</li>
</ul>
<p>&quot;&rdquo;&rdquo;
B2B clients tend to have longer decision-making processes, and thus longer
sales funnels. Relationship-building strategies work better for these
clients, whereas B2C customers tend to respond better to short-term offers
and messages.
&quot;&rdquo;&quot;</p>
<p>Example responses:</p>
<ul>
<li>I could not find an answer.</li>
<li>Yes, B2B clients tend to have longer decision-making processes, and thus
longer sales funnels.</li>
</ul>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Yes, B2B clients tend to have longer decision-making processes, which leads
to longer sales cycles.</p>
<p>If you were to ask the same reference text this question:<br>
å¦‚æœä½ è¦é—®åŒæ ·çš„å‚è€ƒæ–‡æœ¬ï¼Œè¿™ä¸ªé—®é¢˜ï¼š</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>&hellip;The rest of the prompt&hellip;</p>
<p>Question: Are B2C sales more cost-effective?</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>I could not find an answer.</p>
<h1 id="give-direction-and-specify-format">GIVE DIRECTION AND SPECIFY FORMAT</h1>
<p>ç»™å‡ºæ–¹å‘å¹¶æŒ‡å®šæ ¼å¼</p>
<p>The preceding prompt is excellent as itÂ both instructs the model on how to find answers and also sets a specific response format for any unanswerable questions.<br>
å‰é¢çš„æç¤ºéå¸¸å¥½ï¼Œå› ä¸ºå®ƒæ—¢æŒ‡å¯¼æ¨¡å‹å¦‚ä½•æŸ¥æ‰¾ç­”æ¡ˆï¼Œåˆä¸ºä»»ä½•æ— æ³•å›ç­”çš„é—®é¢˜è®¾ç½®ç‰¹å®šçš„å“åº”æ ¼å¼ã€‚</p>
<p>Considering the constrained context windows of GPTs, a method for dynamically retrieving information relevant to the asked query might be necessary to utilize this strategy.<br>
è€ƒè™‘åˆ° GPT çš„å—é™ä¸Šä¸‹æ–‡çª—å£ï¼Œå¯èƒ½éœ€è¦ä¸€ç§åŠ¨æ€æ£€ç´¢ä¸æ‰€è¯·æ±‚æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯çš„æ–¹æ³•æ‰èƒ½åˆ©ç”¨æ­¤ç­–ç•¥ã€‚</p>
<p>Another approach is to directÂ the model toÂ <em>incorporate references</em>Â from a given text in its response. When the input is enhanced with relevant information, the model can be guided to include citations in its responses by referring to sections of the supplied documents. This approach has the added benefit that citations in the output can beÂ <em>authenticated automatically by matching strings</em>Â within the given documents.<br>
å¦ä¸€ç§æ–¹æ³•æ˜¯æŒ‡ç¤ºæ¨¡å‹åœ¨å…¶å“åº”ä¸­åˆå¹¶æ¥è‡ªç»™å®šæ–‡æœ¬çš„å¼•ç”¨ã€‚å½“è¾“å…¥å¾—åˆ°ç›¸å…³ä¿¡æ¯çš„å¢å¼ºæ—¶ï¼Œå¯ä»¥æŒ‡å¯¼æ¨¡å‹é€šè¿‡å‚è€ƒæ‰€æä¾›æ–‡æ¡£çš„å„ä¸ªéƒ¨åˆ†åœ¨å…¶å“åº”ä¸­åŒ…å«å¼•æ–‡ã€‚è¿™ç§æ–¹æ³•è¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„å¥½å¤„ï¼Œå³å¯ä»¥é€šè¿‡åŒ¹é…ç»™å®šæ–‡æ¡£ä¸­çš„å­—ç¬¦ä¸²æ¥è‡ªåŠ¨éªŒè¯è¾“å‡ºä¸­çš„å¼•æ–‡ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>You will be provided with a document delimited by triple quotes and a
question. Your task is to answer the question using only the provided
document and to cite the passage(s) of the document used to answer the
question. If the document does not contain the information needed to answer
this question then simply write: &ldquo;Insufficient information.&rdquo; If an answer
to the question is provided, it must be annotated with a citation. Use the
following format to cite relevant passages ({&ldquo;citation&rdquo;: â€¦}).</p>
<p>&quot;&quot;&quot;
A nuclear reactor is a device that initiates, controls, and maintains a
nuclear chain reaction. The nuclear reactor uses the energy released by
nuclear fission to generate heat, which is then often used to produce steam
to drive a turbine and produce electricity.</p>
<p>-Excerpted from <a href="https://en.wikipedia.org/wiki/Nuclear_reactor">https://en.wikipedia.org/wiki/Nuclear_reactor</a>
&quot;&quot;&quot;</p>
<p>Question: What is a nuclear reactor?</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>A nuclear reactor is a device that initiates, controls, and maintains a
nuclear chain reaction.It uses the energy released by nuclear fission to
generate heat, which is then often used to produce steam to drive a turbine
and produce electricity. {&ldquo;citation&rdquo;: &ldquo;A nuclear reactor is a device that
initiates, controls, and maintains a nuclear chain reaction. The nuclear
reactor uses the energy released by nuclear fission to generate heat, which
is then often used to produce steam to drive a turbine and produce
electricity.&rdquo;}</p>
<h2 id="give-gpts-thinking-time">Give GPTs â€œThinking Timeâ€</h2>
<p>ç»™ GPT â€œæ€è€ƒæ—¶é—´â€</p>
<p>Often, by explicitly guiding an LLM toÂ <em>derive solutions from first principles</em>Â before reaching a verdict, you canÂ garner more accurate responses. Providing an LLM withÂ <em>thinking time</em>Â can often lead to better results.<br>
é€šå¸¸ï¼Œé€šè¿‡æ˜ç¡®å¼•å¯¼ LLM åœ¨åšå‡ºåˆ¤å†³ä¹‹å‰ä»ç¬¬ä¸€æ€§åŸç†ä¸­å¾—å‡ºè§£å†³æ–¹æ¡ˆï¼Œæ‚¨å¯ä»¥è·å¾—æ›´å‡†ç¡®çš„å“åº”ã€‚ä¸ºLLMæä¾›æ€è€ƒæ—¶é—´é€šå¸¸å¯ä»¥å¸¦æ¥æ›´å¥½çš„ç»“æœã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>First, try to solve the problem on your own. Then, see if your answer
matches the proposed solution. Don&rsquo;t judge the given answer until you&rsquo;ve
tried solving it yourself.</p>
<p>Problem:
Jane has 5 apples, then John takes 2 from Mary. Mary originally had 15
apples. John then gives his apples to Jane.</p>
<p>How many apples does Jane have?</p>
<p>Proposed solution:</p>
<ol>
<li>Janes originally had 5 apples.</li>
<li>John takes 2 apples from Mary, but this doesn&rsquo;t affect Jane&rsquo;s count.</li>
<li>John then gives his apples to Jane.</li>
</ol>
<p>So, Jane&rsquo;s final count of apples is her original 5 apples plus the 2 apples
that John gave her, for a total of 7 apples.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Let&rsquo;s break down the problem:</p>
<ol>
<li>Jane originally has 5 apples.</li>
<li>John takes 2 apples from Mary. However, this action doesn&rsquo;t directly
impact the number of apples Jane has at this point.</li>
<li>John gives his apples to Jane.</li>
</ol>
<p>Following these steps, we need to add the 2 apples John gave to Jane to
Jane&rsquo;s original number of apples. Therefore:</p>
<p>5 apples (Jane&rsquo;s original number) + 2 apples (given by John) = 7 apples</p>
<p>So, Jane now has 7 apples. My solution matches the proposed solution.</p>
<p>In some cases, when using GPT-4 or other generative AI models, you may not want the modelâ€™s reasoning process to be visible to the user.<br>
åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œåœ¨ä½¿ç”¨ GPT-4 æˆ–å…¶ä»–ç”Ÿæˆå¼ AI æ¨¡å‹æ—¶ï¼Œæ‚¨å¯èƒ½ä¸å¸Œæœ›ç”¨æˆ·çœ‹åˆ°æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</p>
<p>You can achieve this by asking an LLM toÂ <em>generate an inner monologue</em>. This is particularly useful in tutoring applications, where revealing the modelâ€™s reasoning might give away theÂ solution prematurely.<br>
ä½ å¯ä»¥é€šè¿‡è¦æ±‚LLMç”Ÿæˆå†…å¿ƒç‹¬ç™½æ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¿™åœ¨è¾…å¯¼åº”ç”¨ç¨‹åºä¸­ç‰¹åˆ«æœ‰ç”¨ï¼Œåœ¨è¿™äº›åº”ç”¨ç¨‹åºä¸­ï¼Œæ­ç¤ºæ¨¡å‹çš„æ¨ç†å¯èƒ½ä¼šè¿‡æ—©åœ°æ³„éœ²è§£å†³æ–¹æ¡ˆã€‚</p>
<h2 id="the-inner-monologue-tactic">The Inner Monologue Tactic</h2>
<p>å†…å¿ƒç‹¬ç™½ç­–ç•¥</p>
<p>TheÂ <em>inner monologue tactic</em>Â instructs the modelÂ to structure parts of the output that should be hidden from the user in a specific format. This makes it easy to remove these parts before presenting the final output to the user.<br>
å†…å¿ƒç‹¬ç™½ç­–ç•¥æŒ‡ç¤ºæ¨¡å‹æ„å»ºè¾“å‡ºä¸­åº”ä»¥ç‰¹å®šæ ¼å¼å¯¹ç”¨æˆ·éšè—çš„éƒ¨åˆ†ã€‚è¿™æ ·å°±å¯ä»¥åœ¨å‘ç”¨æˆ·å‘ˆç°æœ€ç»ˆè¾“å‡ºä¹‹å‰è½»æ¾ç§»é™¤è¿™äº›éƒ¨ä»¶ã€‚</p>
<p>Hereâ€™s how you can utilize this tactic to answer user queries effectively.<br>
ä»¥ä¸‹æ˜¯å¦‚ä½•åˆ©ç”¨æ­¤ç­–ç•¥æœ‰æ•ˆåœ°å›ç­”ç”¨æˆ·æŸ¥è¯¢çš„æ–¹æ³•ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Step 1: Solve the Problem</p>
<p>First, you need to work out your solution to the problem. Be sure not to
rely on the user&rsquo;s solution, as it might be incorrect. You can encapsulate
your workings within triple quotes (<code>&quot;&quot;&quot;</code>) to indicate that this section
should be hidden.</p>
<p>Step 2: Compare Solutions</p>
<p>Next, compare your solution with the user&rsquo;s solution. Evaluate whether the
user&rsquo;s solution is correct or not. Again, encapsulate this section within
triple quotes (<code>&quot;&quot;&quot;</code>).</p>
<p>Step 3: Prepare a Hint</p>
<p>If the user made a mistake, determine an appropriate hint that can help
guide them toward the correct answer, without giving it away outright.
This section should also be within triple quotes (<code>&quot;&quot;&quot;</code>).</p>
<p>Step 4: Deliver the Hint</p>
<p>If a mistake was made, present the hint from the previous step to the user.
Remember not to use triple quotes for this step. Instead of using a title
like &ldquo;Step 4&rdquo;, use a heading such as <code>Hint:</code></p>
<p>Problem Statement: A deck of cards has 52 cards: 13 hearts, 13 diamonds, 13
clubs, and 13 spades. If two cards are drawn from the deck without
replacement, what is the probability that one of the drawn cards is a heart
and the other one is a club?</p>
<p>User&rsquo;s Solution: (13/52)*(13/51) = 169/2652 = 13/204</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>&quot;&quot;&quot;
Step 1 - My Solution:</p>
<p>First, calculate the probability of drawing 1 heart and 1 club from a
deck of 52 cards without replacement.</p>
<p>In the first draw, the probability of drawing a heart is 13/52.</p>
<p>For the second draw, after removing 1 card (assuming it&rsquo;s a heart), the
total number of cards becomes 51. So, the probability of drawing a club now
is 13/51.</p>
<p>Multiplying these two probabilities gives the probability of both events
happening in sequence,which is P(Heart then Club) = (13/52)*(13/51).</p>
<p>Repeat the calculation assuming the first drawn card is a club and the
second drawn card is a heart. Remember, order matters in this case as
we&rsquo;re dealing with separate events.
&quot;&quot;&quot;
&quot;&quot;&quot;
Step 2 - Comparison with the User&rsquo;s Solution:</p>
<p>Compare your solution with the user&rsquo;s solution. The user&rsquo;s solution only
considers the sequence of drawing a heart first and then a club. They
have not accounted for the alternate sequence  (Club then Heart), and
thus, the solution is incomplete.
&quot;&quot;&quot;
&quot;&quot;&quot;
Step 3 - Preparation of Hint:</p>
<p>Given that the user hasn&rsquo;t considered both sequences (Heart then Club
and Club then Heart), a hint at the consideration of both possible
sequences will be helpful. This could nudge the user in the right
direction without revealing the solution outright.
&quot;&quot;&quot;</p>
<h1 id="step-4---delivery-of-hint">Step 4 - Delivery of Hint:</h1>
<h1 id="hint-remember-that-the-two-cards-could-be-drawn-in-any-order-consider">Hint: Remember that the two cards could be drawn in any order. Consider</h1>
<p>both possible sequences: Heart then Club and Club then Heart.</p>
<h2 id="self-eval-llm-responsesè‡ªæˆ‘è¯„ä¼°-llm-å“åº”">Self-Eval LLM ResponsesÂ è‡ªæˆ‘è¯„ä¼° LLM å“åº”</h2>
<p>Another tactic you can useÂ is toÂ <em>critque a generated LLM output</em>Â and ask whether the LLM missed any information or important facts. Youâ€™re essentially asking an LLM toÂ <em>evaluate itself</em>Â based on its previous output.<br>
æ‚¨å¯ä»¥ä½¿ç”¨çš„å¦ä¸€ç§ç­–ç•¥æ˜¯æ‰¹è¯„ç”Ÿæˆçš„ LLM è¾“å‡ºï¼Œå¹¶è¯¢é—® LLM æ˜¯å¦é—æ¼äº†ä»»ä½•ä¿¡æ¯æˆ–é‡è¦äº‹å®ã€‚æ‚¨å®é™…ä¸Šæ˜¯åœ¨è¦æ±‚ LLM æ ¹æ®å…¶å…ˆå‰çš„è¾“å‡ºæ¥è¯„ä¼°è‡ªå·±ã€‚</p>
<p>First, create a prompt to generate aÂ <code>&quot;Hello World&quot;</code>Â function.<br>
é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªæç¤ºæ¥ç”Ÿæˆä¸€ä¸ªÂ <code>&quot;Hello World&quot;</code>Â å‡½æ•°ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Generate a &ldquo;Hello World&rdquo; Python function.</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Sure, here&rsquo;s a very simple Python function that prints &ldquo;Hello, World!&rdquo; to
the console:</p>
<p>def print_hello_world():
print(&ldquo;Hello, World!&rdquo;)</p>
<p>print_hello_world()</p>
<p>When you run this function, it will print the string &ldquo;Hello, World!&rdquo; to the
console.</p>
<p>The LLM returns a simple Python function calledÂ <code>print_hello_world()</code>Â that prints the traditional â€œHello, World!â€ greeting.<br>
LLM è¿”å›ä¸€ä¸ªåä¸ºÂ <code>print_hello_world()</code>Â çš„ç®€å• Python å‡½æ•°ï¼Œè¯¥å‡½æ•°æ‰“å°ä¼ ç»Ÿçš„â€œHelloï¼Œ Worldï¼â€é—®å€™è¯­ã€‚</p>
<p>Then make a second LLM request with the previous chat history, asking for the initial output to be improved.<br>
ç„¶åä½¿ç”¨ä¹‹å‰çš„èŠå¤©è®°å½•å‘å‡ºç¬¬äºŒä¸ª LLM è¯·æ±‚ï¼Œè¦æ±‚æ”¹è¿›åˆå§‹è¾“å‡ºã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>&hellip;Previous Chat History&hellip;</p>
<p>Given the previous function, can you improve it again with anything else?
Perhaps defaults or type hints?</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>We can add type hints to make it clear that the function doesn&rsquo;t take any
arguments and doesn&rsquo;t return anything. We can also add a docstring to
describe what the function does.</p>
<p>Here&rsquo;s the improved code:</p>
<p>def print_hello_world() -&gt; None:
&quot;&quot;&quot;
Prints &ldquo;Hello, World!&rdquo; to the standard output.
&quot;&quot;&quot;
print(&ldquo;Hello, World!&rdquo;)</p>
<p>If you wanted to make the function more flexible, you could add a default
argument that allows you to print a different message:</p>
<p>def print_message(message: str = &ldquo;Hello, World!&rdquo;) -&gt; None:
&quot;&quot;&quot;
Prints the given message to the standard output. If no message is given
, it defaults to &ldquo;Hello, World!&rdquo;.</p>
<pre><code>Args:
    message (str, optional): The message to be printed.
    Defaults to &quot;Hello, World!&quot;.
&quot;&quot;&quot;
print(message)
</code></pre>
<p>Consequently GPT-4 returns an upgraded version of the function, now furnished with Python type hints and a default argument. This enriches the function with greater flexibility, allowing it to print not justÂ <code>&quot;Hello, World!&quot;</code>Â but any user-specified message.<br>
å› æ­¤ï¼ŒGPT-4 è¿”å›è¯¥å‡½æ•°çš„å‡çº§ç‰ˆæœ¬ï¼Œç°åœ¨æä¾› Python ç±»å‹æç¤ºå’Œé»˜è®¤å‚æ•°ã€‚è¿™ä»¥æ›´å¤§çš„çµæ´»æ€§ä¸°å¯Œäº†è¯¥åŠŸèƒ½ï¼Œä½¿å…¶ä¸ä»…å¯ä»¥æ‰“å°Â <code>&quot;Hello, World!&quot;</code>Â ï¼Œè¿˜å¯ä»¥æ‰“å°ä»»ä½•ç”¨æˆ·æŒ‡å®šçš„æ¶ˆæ¯ã€‚</p>
<p>These prompt-response exchangesÂ illustrate how you can easily refine generated LLM outputs until youâ€™re satisfied with the final output.<br>
è¿™äº›å¿«é€Ÿå“åº”äº¤æ¢è¯´æ˜äº†å¦‚ä½•è½»æ¾ä¼˜åŒ–ç”Ÿæˆçš„ LLM è¾“å‡ºï¼Œç›´åˆ°æ‚¨å¯¹æœ€ç»ˆè¾“å‡ºæ„Ÿåˆ°æ»¡æ„ä¸ºæ­¢ã€‚</p>
<h6 id="noteæ³¨æ„-3">NOTEÂ æ³¨æ„</h6>
<p>Itâ€™s possible to critique an LLMâ€™s response multiple times, until no further refinement is provided by the LLM.<br>
å¯ä»¥å¤šæ¬¡æ‰¹è¯„ LLM çš„å“åº”ï¼Œç›´åˆ° LLM æ²¡æœ‰æä¾›è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚</p>
<h1 id="classification-with-llms">Classification with LLMs</h1>
<p>ä½¿ç”¨ LLMs è¿›è¡Œåˆ†ç±»</p>
<p>Classifying, in the context of AI, refersÂ to the process of predicting the class or category of a given data point or sample. Itâ€™s a common task in machine learning where models are trained to assign predefined labels to unlabeled data based on learned patterns.<br>
åœ¨äººå·¥æ™ºèƒ½çš„èƒŒæ™¯ä¸‹ï¼Œåˆ†ç±»æ˜¯æŒ‡é¢„æµ‹ç»™å®šæ•°æ®ç‚¹æˆ–æ ·æœ¬çš„ç±»åˆ«æˆ–ç±»åˆ«çš„è¿‡ç¨‹ã€‚è¿™æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€é¡¹å¸¸è§ä»»åŠ¡ï¼Œå…¶ä¸­æ¨¡å‹ç»è¿‡è®­ç»ƒï¼Œæ ¹æ®å­¦ä¹ çš„æ¨¡å¼å°†é¢„å®šä¹‰çš„æ ‡ç­¾åˆ†é…ç»™æœªæ ‡è®°çš„æ•°æ®ã€‚</p>
<p>LLMs are powerful assets when it comes to classification, even with zero or only a small number of examples provided within a prompt. Why? Thatâ€™s because LLMs, like GPT-4, have been previously trained on an extensive dataset and now possess a degree of reasoning.<br>
LLMs åœ¨åˆ†ç±»æ–¹é¢æ˜¯å¼ºå¤§çš„èµ„äº§ï¼Œå³ä½¿åœ¨æç¤ºä¸­æä¾›é›¶æˆ–ä»…æä¾›å°‘é‡ç¤ºä¾‹ã€‚ä¸ºä»€ä¹ˆï¼Ÿè¿™æ˜¯å› ä¸º LLMs å’Œ GPT-4 ä¸€æ ·ï¼Œä¹‹å‰å·²ç»åœ¨å¹¿æ³›çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œç°åœ¨æ‹¥æœ‰ä¸€å®šç¨‹åº¦çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p>There are two overarching strategies in solving classification problems with LLMs:Â <em>zero-shot learning</em>Â andÂ <em>few-shot learning</em>.<br>
ä½¿ç”¨ LLMs è§£å†³åˆ†ç±»é—®é¢˜æœ‰ä¸¤ç§æ€»ä½“ç­–ç•¥ï¼šé›¶æ ·æœ¬å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ ã€‚</p>
<p>Zero-shot learningÂ é›¶æ ·æœ¬å­¦ä¹ </p>
<p>In this process, theÂ LLM classifies data with exceptional accuracy, without the aid of any prior specific examples. Itâ€™s akin to acing a project without any preparationâ€”impressive, right?<br>
åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼ŒLLM ä»¥æé«˜çš„ç²¾åº¦å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»ï¼Œæ— éœ€å€ŸåŠ©ä»»ä½•å…ˆå‰çš„ç‰¹å®šç¤ºä¾‹ã€‚è¿™å°±åƒåœ¨æ²¡æœ‰ä»»ä½•å‡†å¤‡çš„æƒ…å†µä¸‹å®Œæˆä¸€ä¸ªé¡¹ç›®â€”â€”ä»¤äººå°è±¡æ·±åˆ»ï¼Œå¯¹å§ï¼Ÿ</p>
<p>Few-shot learningÂ å°æ ·æœ¬å­¦ä¹ </p>
<p>Here, you provideÂ your LLM with a small number of examples. This strategy can significantly influence the structure of your output format and enhance the overall classification accuracy.<br>
åœ¨è¿™é‡Œï¼Œæ‚¨ä¸ºLLMæä¾›äº†å°‘é‡ç¤ºä¾‹ã€‚æ­¤ç­–ç•¥å¯ä»¥æ˜¾è‘—å½±å“è¾“å‡ºæ ¼å¼çš„ç»“æ„ï¼Œå¹¶æé«˜æ•´ä½“åˆ†ç±»å‡†ç¡®æ€§ã€‚</p>
<p>Why is this groundbreaking for you?<br>
ä¸ºä»€ä¹ˆè¿™å¯¹ä½ æ¥è¯´æ˜¯å¼€åˆ›æ€§çš„ï¼Ÿ</p>
<p>Leveraging LLMs lets you sidestep lengthy processes that traditional machine learning processes demand. Therefore, you can quickly prototype a classification model, determine a base level accuracy, and create immediate business value.<br>
åˆ©ç”¨ LLMs å¯ä»¥é¿å…ä¼ ç»Ÿæœºå™¨å­¦ä¹ è¿‡ç¨‹æ‰€éœ€çš„å†—é•¿è¿‡ç¨‹ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿåˆ›å»ºåˆ†ç±»æ¨¡å‹åŸå‹ï¼Œç¡®å®šåŸºæœ¬çº§åˆ«çš„å‡†ç¡®æ€§ï¼Œå¹¶ç«‹å³åˆ›é€ ä¸šåŠ¡ä»·å€¼ã€‚</p>
<h6 id="warningè­¦å‘Š-2">WARNINGÂ è­¦å‘Š</h6>
<p>Although an LLM can perform classification, depending upon your problem and training data you might find that using a traditional machine learning process could yield better results.<br>
å°½ç®¡ LLM å¯ä»¥æ‰§è¡Œåˆ†ç±»ï¼Œä½†æ ¹æ®æ‚¨çš„é—®é¢˜å’Œè®­ç»ƒæ•°æ®ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°ä½¿ç”¨ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ è¿‡ç¨‹å¯ä»¥äº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚</p>
<h1 id="building-a-classification-model">Building a Classification Model</h1>
<p>æ„å»ºåˆ†ç±»æ¨¡å‹</p>
<p>Letâ€™s explore a few-shot learning example to determine theÂ sentiment of text into eitherÂ <code>'Compliment'</code>,Â <code>'Complaint'</code>, orÂ <code>'Neutral'</code>.<br>
è®©æˆ‘ä»¬æ¢ç´¢ä¸€ä¸ªå‡ ä¸ªæ ·æœ¬çš„å­¦ä¹ ç¤ºä¾‹ï¼Œä»¥ç¡®å®šæ–‡æœ¬çš„æƒ…ç»ªä¸ºÂ <code>'Compliment'</code>Â ï¼ŒÂ <code>'Complaint'</code>Â æˆ–Â <code>'Neutral'</code>Â ã€‚</p>
<p>Given the statement, classify it as either &ldquo;Compliment&rdquo;, &ldquo;Complaint&rdquo;, or
&ldquo;Neutral&rdquo;:</p>
<ol>
<li>&ldquo;The sun is shining.&rdquo; - Neutral</li>
<li>&ldquo;Your support team is fantastic!&rdquo; - Compliment</li>
<li>&ldquo;I had a terrible experience with your software.&rdquo; - Complaint</li>
</ol>
<p>You must follow the following principles:</p>
<ul>
<li>Only return the single classification word. The response should be either
&ldquo;Compliment&rdquo;, &ldquo;Complaint&rdquo;, or &ldquo;Neutral&rdquo;.</li>
<li>Perform the classification on the text enclosed within &quot;&quot;&quot; delimiters.</li>
</ul>
<p>&ldquo;&ldquo;&ldquo;The user interface is intuitive.&rdquo;&rdquo;&rdquo;</p>
<p>Classification:</p>
<p>Compliment</p>
<p>Several good use cases for LLM classification include:<br>
LLM åˆ†ç±»çš„å‡ ä¸ªå¾ˆå¥½çš„ç”¨ä¾‹åŒ…æ‹¬ï¼š</p>
<p>Customer reviewsÂ å®¢æˆ·è¯„ä»·</p>
<p>Classify user reviewsÂ into categories like â€œPositive,â€ â€œNegative,â€ or â€œNeutral.â€ Dive deeper by further identifying subthemes such as â€œUsability,â€ â€œCustomer Support,â€ or â€œPrice.â€<br>
å°†ç”¨æˆ·è¯„è®ºåˆ†ä¸ºâ€œæ­£é¢â€ã€â€œè´Ÿé¢â€æˆ–â€œä¸­ç«‹â€ç­‰ç±»åˆ«ã€‚é€šè¿‡è¿›ä¸€æ­¥ç¡®å®šâ€œå¯ç”¨æ€§â€ã€â€œå®¢æˆ·æ”¯æŒâ€æˆ–â€œä»·æ ¼â€ç­‰å­ä¸»é¢˜æ¥æ›´æ·±å…¥åœ°äº†è§£ã€‚</p>
<p>Email filteringÂ ç”µå­é‚®ä»¶è¿‡æ»¤</p>
<p>Detect the intent orÂ purpose of emails and classify them as â€œInquiry,â€ â€œComplaint,â€ â€œFeedback,â€ or â€œSpam.â€ This can help businesses prioritize responses and manage communications efficiently.<br>
æ£€æµ‹ç”µå­é‚®ä»¶çš„æ„å›¾æˆ–ç›®çš„ï¼Œå¹¶å°†å…¶åˆ†ç±»ä¸ºâ€œæŸ¥è¯¢â€ã€â€œæŠ•è¯‰â€ã€â€œåé¦ˆâ€æˆ–â€œåƒåœ¾é‚®ä»¶â€ã€‚è¿™å¯ä»¥å¸®åŠ©ä¼ä¸šç¡®å®šå“åº”çš„ä¼˜å…ˆçº§å¹¶æœ‰æ•ˆåœ°ç®¡ç†é€šä¿¡ã€‚</p>
<p>Social media sentiment analysis<br>
ç¤¾äº¤åª’ä½“æƒ…ç»ªåˆ†æ</p>
<p>MonitorÂ brand mentions and sentiment across social media platforms. Classify posts or comments as â€œPraise,â€ â€œCritic,â€ â€œQuery,â€ or â€œNeutral.â€ Gain insights into public perception and adapt marketing or PR strategies accordingly.<br>
ç›‘æ§ç¤¾äº¤åª’ä½“å¹³å°ä¸Šçš„å“ç‰ŒæåŠå’Œæƒ…ç»ªã€‚å°†å¸–å­æˆ–è¯„è®ºåˆ†ç±»ä¸ºâ€œè¡¨æ‰¬â€ã€â€œæ‰¹è¯„â€ã€â€œæŸ¥è¯¢â€æˆ–â€œä¸­ç«‹â€ã€‚æ·±å…¥äº†è§£å…¬ä¼—çš„çœ‹æ³•ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´è¥é”€æˆ–å…¬å…³ç­–ç•¥ã€‚</p>
<p>News article categorization<br>
æ–°é—»æ–‡ç« åˆ†ç±»</p>
<p>Given theÂ vast amount of news generated daily, LLMs can classify articles by themes or topics such as â€œPolitics,â€ â€œTechnology,â€ â€œEnvironment,â€ or â€œEntertainment.â€<br>
é‰´äºæ¯å¤©äº§ç”Ÿçš„å¤§é‡æ–°é—»ï¼ŒLLMs å¯ä»¥æŒ‰ä¸»é¢˜æˆ–ä¸»é¢˜ï¼ˆä¾‹å¦‚â€œæ”¿æ²»â€ã€â€œæŠ€æœ¯â€ã€â€œç¯å¢ƒâ€æˆ–â€œå¨±ä¹â€ï¼‰å¯¹æ–‡ç« è¿›è¡Œåˆ†ç±»ã€‚</p>
<p>RÃ©sumÃ© screeningÂ ç®€å†ç­›é€‰</p>
<p>For HR departmentsÂ inundated with rÃ©sumÃ©s, classify them based on predefined criteria like â€œQualified,â€ â€œOverqualified,â€ â€œUnderqualified,â€ or categorize by expertise areasÂ such as â€œSoftware Development,â€ â€œMarketing,â€ or â€œSales.â€<br>
å¯¹äºå……æ–¥ç€ç®€å†çš„äººåŠ›èµ„æºéƒ¨é—¨ï¼Œè¯·æ ¹æ®â€œåˆæ ¼â€ã€â€œåˆæ ¼â€ã€â€œä¸åˆæ ¼â€ç­‰é¢„å®šä¹‰æ ‡å‡†å¯¹å…¶è¿›è¡Œåˆ†ç±»ï¼Œæˆ–æŒ‰â€œè½¯ä»¶å¼€å‘â€ã€â€œè¥é”€â€æˆ–â€œé”€å”®â€ç­‰ä¸“ä¸šé¢†åŸŸè¿›è¡Œåˆ†ç±»ã€‚</p>
<h6 id="warningè­¦å‘Š-3">WARNINGÂ è­¦å‘Š</h6>
<p>Be aware that exposing emails, rÃ©sumÃ©s, or sensitive data does run the risk of data being leaked into OpenAIâ€™s future models as training data.<br>
è¯·æ³¨æ„ï¼Œæš´éœ²ç”µå­é‚®ä»¶ã€ç®€å†æˆ–æ•æ„Ÿæ•°æ®ç¡®å®å­˜åœ¨æ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®æ³„éœ²åˆ° OpenAI æœªæ¥æ¨¡å‹ä¸­çš„é£é™©ã€‚</p>
<h1 id="majority-vote-for-classification">Majority Vote for Classification</h1>
<p>å¤šæ•°ç¥¨èµæˆåˆ†ç±»</p>
<p>Utilizing multiple LLM requests canÂ help in reducing the variance of your classification labels. This process, known asÂ <em>majority vote</em>, is somewhat like choosing the most common fruit out of a bunch. For instance, if you have 10 pieces of fruit and 6 out of them are apples, then apples are the majority. The same principle goes for choosing the majority vote in classification labels.<br>
åˆ©ç”¨å¤šä¸ª LLM è¯·æ±‚æœ‰åŠ©äºå‡å°‘åˆ†ç±»æ ‡ç­¾çš„æ–¹å·®ã€‚è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸ºå¤šæ•°æŠ•ç¥¨ï¼Œæœ‰ç‚¹åƒä»ä¸€å †æ°´æœä¸­é€‰æ‹©æœ€å¸¸è§çš„æ°´æœã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ 10 å—æ°´æœï¼Œå…¶ä¸­ 6 å—æ˜¯è‹¹æœï¼Œé‚£ä¹ˆè‹¹æœå å¤§å¤šæ•°ã€‚åŒæ ·çš„åŸåˆ™ä¹Ÿé€‚ç”¨äºåœ¨åˆ†ç±»æ ‡ç­¾ä¸­é€‰æ‹©å¤šæ•°ç¥¨ã€‚</p>
<p>By soliciting several classifications and taking theÂ <em>most frequent classification</em>, youâ€™re able to reduce the impact of potential outliers or unusual interpretations from a single model inference. However, do bear in mind that there can be significant downsides to this approach, including the increased time required and cost for multiple API calls.<br>
é€šè¿‡å¾æ±‚å¤šä¸ªåˆ†ç±»å¹¶é‡‡ç”¨æœ€é¢‘ç¹çš„åˆ†ç±»ï¼Œæ‚¨å¯ä»¥å‡å°‘å•ä¸ªæ¨¡å‹æ¨ç†ä¸­æ½œåœ¨å¼‚å¸¸å€¼æˆ–å¼‚å¸¸è§£é‡Šçš„å½±å“ã€‚ä½†æ˜¯ï¼Œè¯·è®°ä½ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½å­˜åœ¨é‡å¤§ç¼ºç‚¹ï¼ŒåŒ…æ‹¬å¢åŠ å¤šä¸ª API è°ƒç”¨æ‰€éœ€çš„æ—¶é—´å’Œæˆæœ¬ã€‚</p>
<p>Letâ€™s classify the same piece of text three times, and then take the majority vote:<br>
è®©æˆ‘ä»¬å¯¹åŒä¸€æ®µæ–‡æœ¬è¿›è¡Œä¸‰æ¬¡åˆ†ç±»ï¼Œç„¶åè¿›è¡Œå¤šæ•°æŠ•ç¥¨ï¼š</p>
<pre tabindex="0"><code>from
</code></pre><p>Calling theÂ <code>most_frequent_classification(responses)</code>Â function should pinpointÂ <code>'Neutral'</code>Â as the dominant sentiment. Youâ€™ve now learned how to use the OpenAI package for majority voteÂ classification.<br>
è°ƒç”¨Â <code>most_frequent_classification(responses)</code>Â å‡½æ•°åº”å°†Â <code>'Neutral'</code>Â ç¡®å®šä¸ºä¸»å¯¼æƒ…ç»ªã€‚æ‚¨ç°åœ¨å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨ OpenAI è½¯ä»¶åŒ…è¿›è¡Œå¤šæ•°æŠ•ç¥¨åˆ†ç±»ã€‚</p>
<h1 id="criteria-evaluationæ ‡å‡†è¯„ä¼°">Criteria EvaluationÂ æ ‡å‡†è¯„ä¼°</h1>
<p>InÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#five_principles_01">ChapterÂ 1</a>, a human-based evaluation system was used with a simple thumbs-up/thumbs-down rating systemÂ to identify how often a response met our expectations. Rating manually can be expensive and tedious, requiring a qualified human to judge quality or identify errors. While this work can be outsourced to low-cost raters on services such asÂ <a href="https://www.mturk.com/">Mechanical Turk</a>, designing such a task in a way that gets valid results can itself be time-consuming and error prone. One increasingly common approach is to use a more sophisticated LLM to evaluate the responses of a smaller model.<br>
åœ¨ç¬¬ 1 ç« ä¸­ï¼Œä½¿ç”¨åŸºäºäººç±»çš„è¯„ä¼°ç³»ç»Ÿå’Œç®€å•çš„ç«–èµ·å¤§æ‹‡æŒ‡/ç«–èµ·å¤§æ‹‡æŒ‡çš„è¯„çº§ç³»ç»Ÿæ¥ç¡®å®šå“åº”æ»¡è¶³æˆ‘ä»¬æœŸæœ›çš„é¢‘ç‡ã€‚æ‰‹åŠ¨è¯„çº§å¯èƒ½æ—¢æ˜‚è´µåˆä¹å‘³ï¼Œéœ€è¦åˆæ ¼çš„äººå‘˜æ¥åˆ¤æ–­è´¨é‡æˆ–è¯†åˆ«é”™è¯¯ã€‚è™½ç„¶è¿™é¡¹å·¥ä½œå¯ä»¥å¤–åŒ…ç»™ Mechanical Turk ç­‰æœåŠ¡çš„ä½æˆæœ¬è¯„ä¼°å‘˜ï¼Œä½†ä»¥è·å¾—æœ‰æ•ˆç»“æœçš„æ–¹å¼è®¾è®¡è¿™æ ·çš„ä»»åŠ¡æœ¬èº«å¯èƒ½å¾ˆè€—æ—¶ä¸”å®¹æ˜“å‡ºé”™ã€‚ä¸€ç§è¶Šæ¥è¶Šå¸¸è§çš„æ–¹æ³•æ˜¯ä½¿ç”¨æ›´å¤æ‚çš„ LLM æ¥è¯„ä¼°è¾ƒå°æ¨¡å‹çš„å“åº”ã€‚</p>
<p>The evidence is mixed on whether LLMs canÂ act as effective evaluators, with some studiesÂ <a href="https://oreil.ly/nfc3f">claiming LLMs are human-level evaluators</a>Â and othersÂ <a href="https://oreil.ly/ykkzY">identifying inconsistencies in how LLMs evaluate</a>. In our experience, GPT-4 is a useful evaluator with consistent results across a diverse set of tasks. In particular, GPT-4 is effective and reliable in evaluating the responses from smaller, less sophisticated models like GPT-3.5-turbo. In the example that follows, we generate concise and verbose examples of answers to a question using GPT-3.5-turbo, ready for rating with GPT-4.<br>
å…³äºLLMsæ˜¯å¦å¯ä»¥ä½œä¸ºæœ‰æ•ˆçš„è¯„ä¼°è€…ï¼Œè¯æ®ä¸ä¸€ï¼Œä¸€äº›ç ”ç©¶å£°ç§°LLMsæ˜¯äººç±»æ°´å¹³çš„è¯„ä¼°è€…ï¼Œè€Œå¦ä¸€äº›ç ”ç©¶åˆ™æŒ‡å‡ºäº†LLMsè¯„ä¼°æ–¹å¼çš„ä¸ä¸€è‡´ã€‚æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼ŒGPT-4 æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„è¯„ä¼°å™¨ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­å…·æœ‰ä¸€è‡´çš„ç»“æœã€‚ç‰¹åˆ«æ˜¯ï¼ŒGPT-4 åœ¨è¯„ä¼° GPT-3.5-turbo ç­‰è¾ƒå°ã€ä¸å¤ªå¤æ‚çš„æ¨¡å‹çš„å“åº”æ–¹é¢æ˜¯æœ‰æ•ˆå’Œå¯é çš„ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ GPT-3.5-turbo ç”Ÿæˆäº†ç®€æ˜æ‰¼è¦çš„é—®é¢˜ç­”æ¡ˆç¤ºä¾‹ï¼Œå‡†å¤‡ä½¿ç”¨ GPT-4 è¿›è¡Œè¯„åˆ†ã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<pre tabindex="0"><code>from
</code></pre><p>Output:Â è¾“å‡ºï¼š</p>
<p>Style: concise,  Rating: 1
Style: verbose,  Rating: 0
Style: concise,  Rating: 1
Style: verbose,  Rating: 0
Style: concise,  Rating: 1
Style: verbose,  Rating: 0
Style: concise,  Rating: 1
Style: verbose,  Rating: 0
Style: concise,  Rating: 1
Style: verbose,  Rating: 0</p>
<p>This script is a Python program thatÂ interacts with the OpenAI API to generate and evaluate responses based on their conciseness. Hereâ€™s a step-by-step explanation:<br>
è¯¥è„šæœ¬æ˜¯ä¸€ä¸ª Python ç¨‹åºï¼Œå®ƒä¸ OpenAI API äº¤äº’ï¼Œä»¥æ ¹æ®å…¶ç®€æ´æ€§ç”Ÿæˆå’Œè¯„ä¼°å“åº”ã€‚ä»¥ä¸‹æ˜¯åˆ†æ­¥è¯´æ˜ï¼š</p>
<ol>
<li>
<p><code>responses = []</code>Â creates an empty list namedÂ <code>responses</code>Â to store the responses generated by the OpenAI API.<br>
<code>responses = []</code>Â åˆ›å»ºä¸€ä¸ªåä¸ºÂ <code>responses</code>Â çš„ç©ºåˆ—è¡¨æ¥å­˜å‚¨ OpenAI API ç”Ÿæˆçš„å“åº”ã€‚</p>
</li>
<li>
<p>TheÂ <code>for</code>Â loop runs 10 times, generating a response for each iteration.<br>
<code>for</code>Â å¾ªç¯è¿è¡Œ 10 æ¬¡ï¼Œæ¯æ¬¡è¿­ä»£éƒ½ä¼šç”Ÿæˆä¸€ä¸ªå“åº”ã€‚</p>
</li>
<li>
<p>Inside the loop,Â <code>style</code>Â is determined based on the current iteration number (<code>i</code>). It alternates between â€œconciseâ€ and â€œverboseâ€ for even and odd iterations, respectively.<br>
åœ¨å¾ªç¯ä¸­ï¼ŒÂ <code>style</code>Â æ˜¯æ ¹æ®å½“å‰è¿­ä»£æ¬¡æ•° ï¼ˆÂ <code>i</code>Â ï¼‰ ç¡®å®šçš„ã€‚å®ƒåˆ†åˆ«åœ¨å¶æ•°å’Œå¥‡æ•°è¿­ä»£çš„â€œç®€æ´â€å’Œâ€œå†—é•¿â€ä¹‹é—´äº¤æ›¿ã€‚</p>
</li>
<li>
<p>Depending on theÂ <code>style</code>, aÂ <code>prompt</code>Â string is formatted to ask, â€œWhat is the meaning of life?â€ in either a concise or verbose manner.<br>
æ ¹æ®Â <code>style</code>Â ï¼ŒÂ <code>prompt</code>Â å­—ç¬¦ä¸²çš„æ ¼å¼ä¸ºä»¥ç®€æ´æˆ–å†—é•¿çš„æ–¹å¼è¯¢é—®â€œç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ</p>
</li>
<li>
<p><code>response = client.chat.completions.create(...)</code>Â makes a request to the OpenAI API to generate a response based on theÂ <code>prompt</code>. The model used here is specified as â€œgpt-3.5-turbo.â€<br>
<code>response = client.chat.completions.create(...)</code>Â å‘ OpenAI API å‘å‡ºè¯·æ±‚ï¼Œä»¥æ ¹æ®Â <code>prompt</code>Â ç”Ÿæˆå“åº”ã€‚æ­¤å¤„ä½¿ç”¨çš„å‹å·æŒ‡å®šä¸ºâ€œgpt-3.5-turboâ€ã€‚</p>
</li>
<li>
<p>The generated response is then stripped of any leading or trailing whitespace and added to theÂ <code>responses</code>Â list.<br>
ç„¶åï¼Œç”Ÿæˆçš„å“åº”å°†å»é™¤ä»»ä½•å‰å¯¼æˆ–å°¾éšç©ºæ ¼ï¼Œå¹¶æ·»åŠ åˆ°Â <code>responses</code>Â åˆ—è¡¨ä¸­ã€‚</p>
</li>
<li>
<p><code>system_prompt = &quot;&quot;&quot;You are assessing...&quot;&quot;&quot;</code>Â sets up a prompt used for evaluating the conciseness of the generated responses.<br>
<code>system_prompt = &quot;&quot;&quot;You are assessing...&quot;&quot;&quot;</code>Â è®¾ç½®äº†ä¸€ä¸ªæç¤ºï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆçš„å“åº”çš„ç®€æ´æ€§ã€‚</p>
</li>
<li>
<p><code>ratings = []</code>Â initializes an empty list to store the conciseness ratings.<br>
<code>ratings = []</code>Â åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨ç®€æ´åº¦è¯„çº§ã€‚</p>
</li>
<li>
<p>AnotherÂ <code>for</code>Â loop iterates over each response inÂ <code>responses</code>.<br>
å¦ä¸€ä¸ªÂ <code>for</code>Â å¾ªç¯éå†Â <code>responses</code>Â ä¸­çš„æ¯ä¸ªå“åº”ã€‚</p>
</li>
<li>
<p>For each response, the script sends it along with theÂ <code>system_prompt</code>Â to the OpenAI API, requesting a conciseness evaluation. This time, the model used is â€œgpt-4.â€<br>
å¯¹äºæ¯ä¸ªå“åº”ï¼Œè„šæœ¬ä¼šå°†å…¶ä¸Â <code>system_prompt</code>Â ä¸€èµ·å‘é€åˆ° OpenAI APIï¼Œè¯·æ±‚è¿›è¡Œç®€æ´æ€§è¯„ä¼°ã€‚è¿™ä¸€æ¬¡ï¼Œä½¿ç”¨çš„æ¨¡å‹æ˜¯â€œgpt-4â€ã€‚</p>
</li>
<li>
<p>The evaluation rating (either 1 for concise or 0 for not concise) is then stripped of whitespace and added to theÂ <code>ratings</code>Â list.<br>
ç„¶åï¼Œè¯„ä¼°è¯„çº§ï¼ˆ1 è¡¨ç¤ºç®€æ´ï¼Œ0 è¡¨ç¤ºä¸ç®€æ´ï¼‰å°†å»é™¤ç©ºæ ¼å¹¶æ·»åŠ åˆ°Â <code>ratings</code>Â åˆ—è¡¨ä¸­ã€‚</p>
</li>
<li>
<p>The finalÂ <code>for</code>Â loop iterates over theÂ <code>ratings</code>Â list. For each rating, it prints theÂ <code>style</code>Â of the response (either â€œconciseâ€ or â€œverboseâ€) and its corresponding concisenessÂ <code>rating</code>.<br>
æœ€åä¸€ä¸ªÂ <code>for</code>Â å¾ªç¯éå†Â <code>ratings</code>Â åˆ—è¡¨ã€‚å¯¹äºæ¯ä¸ªè¯„çº§ï¼Œå®ƒéƒ½ä¼šæ‰“å°å“åº”çš„Â <code>style</code>Â ï¼ˆâ€œç®€æ´â€æˆ–â€œå†—é•¿â€ï¼‰åŠå…¶ç›¸åº”çš„ç®€æ´åº¦Â <code>rating</code>Â ã€‚</p>
</li>
</ol>
<p>For simple ratings like conciseness, GPT-4 performs with near 100% accuracy; however, for more complex ratings, itâ€™s important to spend some time evaluating the evaluator. For example, by setting test cases that contain an issue, as well as test cases that do not contain an issue, you can identify the accuracy of your evaluation metric. An evaluator can itself be evaluated by counting the number of false positives (when the LLM hallucinates an issue in a test case that is known not to contain an issue), as well as the number of false negatives (when the LLM misses an issue in a test case that is known to contain an issue). In our example we generated the concise and verbose examples, so we can easily check the rating accuracy, but in more complex examples you may need human evaluators to validate the ratings.<br>
å¯¹äºç®€æ´ç­‰ç®€å•è¯„çº§ï¼ŒGPT-4 çš„å‡†ç¡®ç‡æ¥è¿‘ 100%;ä½†æ˜¯ï¼Œå¯¹äºæ›´å¤æ‚çš„è¯„çº§ï¼ŒèŠ±ä¸€äº›æ—¶é—´è¯„ä¼°è¯„ä¼°å‘˜éå¸¸é‡è¦ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡è®¾ç½®åŒ…å«é—®é¢˜çš„æµ‹è¯•ç”¨ä¾‹ä»¥åŠä¸åŒ…å«é—®é¢˜çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¯ä»¥ç¡®å®šè¯„ä¼°æŒ‡æ ‡çš„å‡†ç¡®æ€§ã€‚è¯„ä¼°å™¨æœ¬èº«å¯ä»¥é€šè¿‡è®¡ç®—è¯¯æŠ¥çš„æ•°é‡ï¼ˆå½“ LLM åœ¨å·²çŸ¥ä¸åŒ…å«é—®é¢˜çš„æµ‹è¯•ç”¨ä¾‹ä¸­å‡ºç°å¹»è§‰æ—¶ï¼‰ä»¥åŠè¯¯æŠ¥çš„æ•°é‡ï¼ˆå½“ LLM åœ¨å·²çŸ¥åŒ…å«é—®é¢˜çš„æµ‹è¯•ç”¨ä¾‹ä¸­é—æ¼é—®é¢˜æ—¶ï¼‰æ¥è¯„ä¼°è¯„ä¼°å™¨æœ¬èº«ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ç®€æ˜æ‰¼è¦çš„ç¤ºä¾‹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥è½»æ¾æ£€æŸ¥è¯„çº§å‡†ç¡®æ€§ï¼Œä½†åœ¨æ›´å¤æ‚çš„ç¤ºä¾‹ä¸­ï¼Œæ‚¨å¯èƒ½éœ€è¦äººå·¥è¯„ä¼°äººå‘˜æ¥éªŒè¯è¯„çº§ã€‚</p>
<h1 id="evaluate-qualityè¯„ä¼°è´¨é‡-2">EVALUATE QUALITYÂ è¯„ä¼°è´¨é‡</h1>
<p>Using GPT-4 to evaluate theÂ responses of less sophisticated models is an emerging standard practice, but care must be taken that the results are reliable and consistent.<br>
ä½¿ç”¨ GPT-4 è¯„ä¼°ä¸å¤ªå¤æ‚çš„æ¨¡å‹çš„å“åº”æ˜¯ä¸€ç§æ–°å…´çš„æ ‡å‡†åšæ³•ï¼Œä½†å¿…é¡»æ³¨æ„ç»“æœçš„å¯é æ€§å’Œä¸€è‡´æ€§ã€‚</p>
<p>Compared to human-based evaluation, LLM-based or synthetic evaluation typically costs an order of magnitude less and completes in a few minutes rather than taking days or weeks. Even in important or sensitive cases where a final manual review by a human is necessary, rapid iteration and A/B testing of the prompt through synthetic reviews can save significant time and improve results considerably. However, the cost of running many tests at scale can add up, and the latency or rate limits of GPT-4 can be a blocker. If at all possible, a prompt engineer should firstÂ test using programmatic techniques that donâ€™t require a call to an LLM, such as simply measuring the length of the response, which runs near instantly for close to zero cost.<br>
ä¸åŸºäºäººå·¥çš„è¯„ä¼°ç›¸æ¯”ï¼ŒåŸºäº LLM æˆ–ç»¼åˆè¯„ä¼°çš„æˆæœ¬é€šå¸¸è¦ä½ä¸€ä¸ªæ•°é‡çº§ï¼Œå¹¶ä¸”åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆï¼Œè€Œä¸æ˜¯éœ€è¦å‡ å¤©æˆ–å‡ å‘¨çš„æ—¶é—´ã€‚å³ä½¿åœ¨é‡è¦æˆ–æ•æ„Ÿçš„æƒ…å†µä¸‹ï¼Œéœ€è¦äººå·¥è¿›è¡Œæœ€ç»ˆçš„äººå·¥å®¡æŸ¥ï¼Œé€šè¿‡ç»¼åˆå®¡æŸ¥å¯¹æç¤ºè¿›è¡Œå¿«é€Ÿè¿­ä»£å’Œ A/B æµ‹è¯•ä¹Ÿå¯ä»¥èŠ‚çœå¤§é‡æ—¶é—´å¹¶æ˜¾ç€æ”¹å–„ç»“æœã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡è¿è¡Œè®¸å¤šæµ‹è¯•çš„æˆæœ¬å¯èƒ½ä¼šå¢åŠ ï¼Œè€Œ GPT-4 çš„å»¶è¿Ÿæˆ–é€Ÿç‡é™åˆ¶å¯èƒ½ä¼šæˆä¸ºéšœç¢ã€‚å¦‚æœå¯èƒ½çš„è¯ï¼Œæç¤ºå·¥ç¨‹å¸ˆåº”è¯¥é¦–å…ˆä½¿ç”¨ä¸éœ€è¦è°ƒç”¨ LLM çš„ç¼–ç¨‹æŠ€æœ¯è¿›è¡Œæµ‹è¯•ï¼Œä¾‹å¦‚ç®€å•åœ°æµ‹é‡å“åº”çš„é•¿åº¦ï¼Œè¯¥å“åº”å‡ ä¹å¯ä»¥ç«‹å³è¿è¡Œï¼Œæˆæœ¬å‡ ä¹ä¸ºé›¶ã€‚</p>
<h1 id="meta-promptingå…ƒæç¤º">Meta PromptingÂ å…ƒæç¤º</h1>
<p><em>Meta prompting</em>Â is a technique thatÂ involves the creation of text prompts that, in turn, generate other text prompts. These text prompts are then used to generate new assets in many mediums such as images, videos, and more text.<br>
å…ƒæç¤ºæ˜¯ä¸€ç§æ¶‰åŠåˆ›å»ºæ–‡æœ¬æç¤ºçš„æŠ€æœ¯ï¼Œè€Œæ–‡æœ¬æç¤ºåˆä¼šç”Ÿæˆå…¶ä»–æ–‡æœ¬æç¤ºã€‚ç„¶åï¼Œè¿™äº›æ–‡æœ¬æç¤ºç”¨äºåœ¨è®¸å¤šåª’ä½“ï¼ˆå¦‚å›¾åƒã€è§†é¢‘å’Œæ›´å¤šæ–‡æœ¬ï¼‰ä¸­ç”Ÿæˆæ–°èµ„äº§ã€‚</p>
<p>To better understand meta prompting, letâ€™s take the example of authoring a childrenâ€™s book with the assistance of GPT-4. First, you direct the LLM to generate the text for your childrenâ€™s book. Afterward, you invoke meta prompting by instructing GPT-4 to produce prompts that are suitable for image-generation models. This could mean creating situational descriptions or specific scenes based on the storyline of your book, which then can be given to AI models like Midjourney or Stable Diffusion. These image-generation models can, therefore, deliver images in harmony with your AI-crafted childrenâ€™s story.<br>
ä¸ºäº†æ›´å¥½åœ°ç†è§£å…ƒæç¤ºï¼Œè®©æˆ‘ä»¬ä»¥åœ¨ GPT-4 çš„å¸®åŠ©ä¸‹åˆ›ä½œå„¿ç«¥è¯»ç‰©ä¸ºä¾‹ã€‚é¦–å…ˆï¼Œæ‚¨æŒ‡ç¤º LLM ä¸ºæ‚¨çš„å„¿ç«¥è¯»ç‰©ç”Ÿæˆæ–‡æœ¬ã€‚ä¹‹åï¼Œæ‚¨å¯ä»¥é€šè¿‡æŒ‡ç¤º GPT-4 ç”Ÿæˆé€‚åˆå›¾åƒç”Ÿæˆæ¨¡å‹çš„æç¤ºæ¥è°ƒç”¨å…ƒæç¤ºã€‚è¿™å¯èƒ½æ„å‘³ç€æ ¹æ®ä½ çš„ä¹¦çš„æ•…äº‹æƒ…èŠ‚åˆ›å»ºæƒ…å¢ƒæè¿°æˆ–ç‰¹å®šåœºæ™¯ï¼Œç„¶åå¯ä»¥å°†å…¶æä¾›ç»™ Midjourney æˆ– Stable Diffusion ç­‰ AI æ¨¡å‹ã€‚å› æ­¤ï¼Œè¿™äº›å›¾åƒç”Ÿæˆæ¨¡å‹å¯ä»¥æä¾›ä¸æ‚¨ AI åˆ¶ä½œçš„å„¿ç«¥æ•…äº‹ç›¸åè°ƒçš„å›¾åƒã€‚</p>
<p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#figure-3-8">FigureÂ 3-8</a>Â visually describes the process of meta prompting in the context of crafting a childrenâ€™s book.<br>
å›¾ 3-8 ç›´è§‚åœ°æè¿°äº†åœ¨åˆ¶ä½œå„¿ç«¥è¯»ç‰©çš„ä¸Šä¸‹æ–‡ä¸­å…ƒæç¤ºçš„è¿‡ç¨‹ã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0308.png" alt="Creating image prompts from an LLM that will later be used by MidJourney for image creation."></p>
<h6 id="figure-3-8utilizing-an-llm-to-generate-image-prompts-for-midjourneys-image-creation-in-the-process-of-crafting-a-childrens-book">Figure 3-8.Â Utilizing an LLM to generate image prompts for MidJourneyâ€™s image creation in the process of crafting a childrenâ€™s book</h6>
<p>å›¾ 3-8ã€‚åœ¨åˆ¶ä½œå„¿ç«¥è¯»ç‰©çš„è¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨LLMä¸ºMidJourneyçš„å›¾åƒåˆ›å»ºç”Ÿæˆå›¾åƒæç¤º</p>
<p>Meta prompts offer a multitude of benefits for a variety of applications:<br>
å…ƒæç¤ºä¸ºå„ç§åº”ç”¨ç¨‹åºæä¾›äº†è®¸å¤šå¥½å¤„ï¼š</p>
<p>Image generation from product descriptions<br>
ä»äº§å“æè¿°ç”Ÿæˆå›¾åƒ</p>
<p>Meta promptsÂ can be employed to derive an image generation prompt for image models likeÂ <a href="https://www.midjourney.com/">Midjourney</a>, effectively creating a visual representation of product descriptions.<br>
å…ƒæç¤ºå¯ç”¨äºä¸º Midjourney ç­‰å›¾åƒæ¨¡å‹æ´¾ç”Ÿå›¾åƒç”Ÿæˆæç¤ºï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ›å»ºäº§å“æè¿°çš„å¯è§†åŒ–è¡¨ç¤ºã€‚</p>
<p>Generating style/feature prompts<br>
ç”Ÿæˆæ ·å¼/åŠŸèƒ½æç¤º</p>
<p>Letâ€™s consider youÂ are a copywriter needing to develop a unique style guide prompt from a couple of blog posts. Given each client has a distinctive tone and style, itâ€™s beneficial to utilize aÂ <em>meta prompt</em>Â that encapsulates all the varied features, rather than producing a single prompt output.<br>
è®©æˆ‘ä»¬å‡è®¾æ‚¨æ˜¯ä¸€åæ’°ç¨¿äººï¼Œéœ€è¦ä»å‡ ç¯‡åšå®¢æ–‡ç« ä¸­å¼€å‘ç‹¬ç‰¹çš„é£æ ¼æŒ‡å—æç¤ºã€‚é‰´äºæ¯ä¸ªå®¢æˆ·ç«¯éƒ½æœ‰ç‹¬ç‰¹çš„è¯­æ°”å’Œé£æ ¼ï¼Œä½¿ç”¨å°è£…æ‰€æœ‰ä¸åŒåŠŸèƒ½çš„å…ƒæç¤ºè€Œä¸æ˜¯ç”Ÿæˆå•ä¸ªæç¤ºè¾“å‡ºæ˜¯æœ‰ç›Šçš„ã€‚</p>
<p>Optimizing prompts to achieve specific goals<br>
ä¼˜åŒ–æç¤ºä»¥å®ç°ç‰¹å®šç›®æ ‡</p>
<p>A common approach is to ask ChatGPT or another language model to refine or improveÂ <code>Prompt A</code>Â in order to attainÂ <code>Goal 1</code>, givenÂ <code>Prompt A</code>Â andÂ <code>Goal 1</code>. This method aids in identifying any missed input features, that by adding could assist the language model in generating a more detailed and desirable response.<br>
ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯è¦æ±‚ ChatGPT æˆ–å…¶ä»–è¯­è¨€æ¨¡å‹æ”¹è¿›æˆ–æ”¹è¿›Â <code>Prompt A</code>Â ä»¥è·å¾—Â <code>Goal 1</code>Â ï¼Œç»™å®šÂ <code>Prompt A</code>Â å’ŒÂ <code>Goal 1</code>Â ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºè¯†åˆ«ä»»ä½•é—æ¼çš„è¾“å…¥ç‰¹å¾ï¼Œé€šè¿‡æ·»åŠ è¿™äº›ç‰¹å¾å¯ä»¥å¸®åŠ©è¯­è¨€æ¨¡å‹ç”Ÿæˆæ›´è¯¦ç»†å’Œç†æƒ³çš„å“åº”ã€‚</p>
<p>Letâ€™s experiment with creating a meta prompt for the copywriting example.<br>
è®©æˆ‘ä»¬å°è¯•ä¸ºæ–‡æ¡ˆç¤ºä¾‹åˆ›å»ºä¸€ä¸ªå…ƒæç¤ºã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<p>Act as a GPT Prompt Engineer, you are solely responsible for producing
highly effective large language model text prompts given a task.</p>
<p>You must follow the following principles:</p>
<ul>
<li>Return only a single prompt, never return the output of a prompt.</li>
<li>You must follow the task properly.</li>
<li>You must either refine the prompt or create a new prompt depending upon
the Task.</li>
<li>Bullet point out any important features so that another Prompt Engineer
can easily copy inputs into the prompt.</li>
</ul>
<p>Task: You must create a copywriting guide ChatGPT several blog posts.
Include relevant features within the ChatGPT prompt that will allow ChatGPT
to imitate the writer. You must decide what the most important features are
to mimic the writer&rsquo;s style etc. You will only be applying this style to
new blog posts.</p>
<p>Blog post text: You need a variety of soft skills in addition to technical
skills to succeed in the technology sector. Soft skills are used by
software professionals to collaborate with their peers&hellip;</p>
<p>Output:Â è¾“å‡ºï¼š</p>
<p>Prompt:</p>
<p>Based on the extensive text provided, your task is to create a series of
blog posts following the same style and structure of the writer. As a guide
you should follow these features:</p>
<ol>
<li>Clear, direct language: Your writing should be in clear and direct
language, breaking down complex subjects into easily understandable
concepts.</li>
<li>Use of subheadings: To provide structure to your blog posts, break them
up with relevant subheadings. This helps to guide the reader and make your
text more readable.
&hellip;</li>
<li>Inclusion of technical terminology: When discussing technical topics,
use appropriate technical language to ensure accuracy and credibility.
However, be sure to provide explanations or definitions for more complex
terms.</li>
</ol>
<p>As previously mentioned, it can be beneficial toÂ <em>critique the modelâ€™s output</em>Â and to ask for a refined prompt either via the interface or API.<br>
å¦‚å‰æ‰€è¿°ï¼Œæ‰¹è¯„æ¨¡å‹çš„è¾“å‡ºå¹¶é€šè¿‡æ¥å£æˆ– API è¯·æ±‚æ”¹è¿›æç¤ºå¯èƒ½æ˜¯æœ‰ç›Šçš„ã€‚</p>
<p>ChatGPT can perform web searches and can make use of the content of web pages, along with the previously generated copywriting style guide. Letâ€™s provide two URLs that ChatGPT will read to enhance the output.<br>
ChatGPT å¯ä»¥æ‰§è¡Œç½‘ç»œæœç´¢ï¼Œå¹¶å¯ä»¥åˆ©ç”¨ç½‘é¡µå†…å®¹ï¼Œä»¥åŠä¹‹å‰ç”Ÿæˆçš„æ–‡æ¡ˆé£æ ¼æŒ‡å—ã€‚è®©æˆ‘ä»¬æä¾›ä¸¤ä¸ª ChatGPT å°†è¯»å–çš„ URL ä»¥å¢å¼ºè¾“å‡ºã€‚</p>
<p>Input:Â è¾“å…¥ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Given the following style guide <span style="color:#f92672">and</span> these two blog posts, I want you to
</span></span><span style="display:flex;"><span>improve the copywriting style prompt so an LLM <span style="color:#f92672">or</span> copywriter can write
</span></span><span style="display:flex;"><span>effectively <span style="color:#66d9ef">for</span> my client<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>Previously Generated Copywriting guide<span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>URL <span style="color:#ae81ff">1</span>: https:<span style="color:#f92672">//</span>understandingdata<span style="color:#f92672">.</span>com<span style="color:#f92672">/</span>posts<span style="color:#f92672">/</span>how<span style="color:#f92672">-</span>to<span style="color:#f92672">-</span>clean<span style="color:#f92672">-</span><span style="color:#f92672">and</span><span style="color:#f92672">-</span>process<span style="color:#f92672">-</span>data
</span></span><span style="display:flex;"><span>URL <span style="color:#ae81ff">2</span>: https:<span style="color:#f92672">//</span>understandingdata<span style="color:#f92672">.</span>com<span style="color:#f92672">/</span>posts<span style="color:#f92672">/</span>how<span style="color:#f92672">-</span>to<span style="color:#f92672">-</span>scrape<span style="color:#f92672">-</span>twitter<span style="color:#f92672">-</span>data
</span></span></code></pre></div><p><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#figure-3-9">FigureÂ 3-9</a>Â shows the output.<br>
è¾“å‡ºå¦‚å›¾ 3-9 æ‰€ç¤ºã€‚</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0309.png" alt="pega 0309"></p>
<h6 id="figure-3-9chatgpt-refining-a-meta-prompt-by-two-url-web-pages">Figure 3-9.Â ChatGPT refining a meta prompt by two URL web pages</h6>
<p>å›¾ 3-9ã€‚ChatGPT é€šè¿‡ä¸¤ä¸ª URL ç½‘é¡µç»†åŒ–å…ƒæç¤º</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Based on the style guide <span style="color:#f92672">and</span> the two blog posts, here<span style="color:#e6db74">&#39;s an improved</span>
</span></span><span style="display:flex;"><span>copywriting style prompt:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>To create a blog post <span style="color:#66d9ef">for</span> the client, adhere to the following guidelines:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1.</span> Clarity <span style="color:#f92672">and</span> Directness: Use clear, direct language to explain complex
</span></span><span style="display:flex;"><span>topics<span style="color:#f92672">.</span> Present information <span style="color:#f92672">in</span> an easy<span style="color:#f92672">-</span>to<span style="color:#f92672">-</span>understand manner<span style="color:#f92672">.</span> Aim <span style="color:#66d9ef">for</span> your
</span></span><span style="display:flex;"><span>audience to grasp concepts without needing prior knowledge<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">8.</span> Use of Technical Terminology: Use the appropriate technical language
</span></span><span style="display:flex;"><span>when discussing complex topics<span style="color:#f92672">.</span> However, make sure to provide definitions
</span></span><span style="display:flex;"><span><span style="color:#f92672">or</span> explanations <span style="color:#66d9ef">for</span> these terms to keep the text accessible<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Model your writing after these blog post examples:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;What is an API? And How Do They Relate to Data Engineering?&#34;</span>: This blog
</span></span><span style="display:flex;"><span>post uses clear <span style="color:#f92672">and</span> direct language to explain the concept of an API<span style="color:#f92672">.</span> It
</span></span><span style="display:flex;"><span>provides detailed explanations <span style="color:#f92672">and</span> uses practical examples of commonly used
</span></span><span style="display:flex;"><span>APIs<span style="color:#f92672">.</span> It links the technical concept of APIs to business growth <span style="color:#f92672">and</span>
</span></span><span style="display:flex;"><span>productivity<span style="color:#f92672">.</span> The information <span style="color:#f92672">is</span> well<span style="color:#f92672">-</span>categorized using subheadings like
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;Types of APIâ€™s.&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;How To Easily Setup A Google Cloud Project With APIs&#34;</span>: This blog post <span style="color:#f92672">is</span> a
</span></span><span style="display:flex;"><span>practical guide on setting up a Google Cloud Project<span style="color:#f92672">.</span> It<span style="color:#e6db74">&#39;s broken down into</span>
</span></span><span style="display:flex;"><span>numbered steps, each <span style="color:#66d9ef">with</span> a clear subheading<span style="color:#f92672">.</span> The post uses detailed
</span></span><span style="display:flex;"><span>explanations <span style="color:#f92672">and</span> includes practical examples such <span style="color:#66d9ef">as</span> screenshots<span style="color:#f92672">.</span> It also
</span></span><span style="display:flex;"><span>uses clear, direct language to guide the reader through the process<span style="color:#f92672">.</span>
</span></span></code></pre></div><p>Meta prompting offers a dynamic and innovative way to harness the power of generative AI models, fostering the creation of complex, multifaceted prompts and even prompts that generate other prompts. It broadens the application scope, from text and image generation to style and feature prompts, and optimization toward specific goals. As you continue to refine and explore the potential of meta prompting, it promises to be a game changer in how you utilize, interact with, and benefitÂ from using LLMs.<br>
å…ƒæç¤ºæä¾›äº†ä¸€ç§åŠ¨æ€å’Œåˆ›æ–°çš„æ–¹å¼æ¥åˆ©ç”¨ç”Ÿæˆå¼ AI æ¨¡å‹çš„åŠ›é‡ï¼Œä¿ƒè¿›åˆ›å»ºå¤æ‚ã€å¤šæ–¹é¢çš„æç¤ºï¼Œç”šè‡³æ˜¯ç”Ÿæˆå…¶ä»–æç¤ºçš„æç¤ºã€‚å®ƒæ‹“å®½äº†åº”ç”¨èŒƒå›´ï¼Œä»æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆåˆ°æ ·å¼å’ŒåŠŸèƒ½æç¤ºï¼Œä»¥åŠé’ˆå¯¹ç‰¹å®šç›®æ ‡çš„ä¼˜åŒ–ã€‚éšç€æ‚¨ç»§ç»­å®Œå–„å’Œæ¢ç´¢å…ƒæç¤ºçš„æ½œåŠ›ï¼Œå®ƒæœ‰æœ›æ”¹å˜æ‚¨å¦‚ä½•ä½¿ç”¨ LLMsã€ä¸ä¹‹äº¤äº’å¹¶ä»ä¸­å—ç›Šçš„æ¸¸æˆè§„åˆ™ã€‚</p>
<h1 id="summaryæ€»ç»“-1">SummaryÂ æ€»ç»“</h1>
<p>After reading this chapter, you are now aware of how crucial it is to give clear directions and examples to generate desired outputs. Also, you have hands-on experience extracting structured data from a hierarchical list using regular expressions in Python, and youâ€™ve learned to utilize nested data structures like JSON and YAML to produce robust, parsable outputs.<br>
é˜…è¯»æœ¬ç« åï¼Œæ‚¨ç°åœ¨æ„è¯†åˆ°ç»™å‡ºæ˜ç¡®çš„æ–¹å‘å’Œç¤ºä¾‹ä»¥ç”Ÿæˆæ‰€éœ€çš„è¾“å‡ºæ˜¯å¤šä¹ˆé‡è¦ã€‚æ­¤å¤–ï¼Œæ‚¨è¿˜å…·æœ‰ä½¿ç”¨ Python ä¸­çš„æ­£åˆ™è¡¨è¾¾å¼ä»åˆ†å±‚åˆ—è¡¨ä¸­æå–ç»“æ„åŒ–æ•°æ®çš„å®è·µç»éªŒï¼Œå¹¶ä¸”æ‚¨å·²ç»å­¦ä¼šäº†åˆ©ç”¨åµŒå¥—æ•°æ®ç»“æ„ï¼ˆå¦‚ JSON å’Œ YAMLï¼‰æ¥ç”Ÿæˆå¯é ã€å¯è§£æçš„è¾“å‡ºã€‚</p>
<p>Youâ€™ve learned several best practices and effective prompt engineering techniques, including the famous â€œExplain it like Iâ€™m fiveâ€, role prompting, and meta prompting techniques. In the next chapter, you will learn how to use a popular LLM package called LangChain thatâ€™ll help you to create more advanced prompt engineeringÂ workflows.<br>
æ‚¨å·²ç»å­¦ä¹ äº†å‡ ç§æœ€ä½³å®è·µå’Œæœ‰æ•ˆçš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼ŒåŒ…æ‹¬è‘—åçš„â€œåƒæˆ‘äº”å²ä¸€æ ·è§£é‡Šå®ƒâ€ã€è§’è‰²æç¤ºå’Œå…ƒæç¤ºæŠ€æœ¯ã€‚åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨åä¸º LangChain çš„æµè¡Œ LLM åŒ…ï¼Œè¯¥åŒ…å°†å¸®åŠ©æ‚¨åˆ›å»ºæ›´é«˜çº§çš„æç¤ºå·¥ç¨‹å·¥ä½œæµç¨‹ã€‚</p>
<h1 id="4-advanced-techniques-for-text-generation-with-langchain"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch04.html">4. Advanced Techniques For Text Generation With LangChain</a></h1>
<h1 id="chapter-4advanced-techniques-fortext-generation-with-langchainä½¿ç”¨langchainç”Ÿæˆæ–‡æœ¬çš„é«˜çº§æŠ€æœ¯">Chapter 4.Â Advanced Techniques forÂ Text Generation with LangChainä½¿ç”¨LangChainç”Ÿæˆæ–‡æœ¬çš„é«˜çº§æŠ€æœ¯</h1>
<p>Using simple prompt engineering techniques will often work for most tasks, but occasionally youâ€™ll need to use a more powerful toolkit to solve complex generative AI problems. Such problems and tasks include:
ä½¿ç”¨ç®€å•çš„æç¤ºå·¥ç¨‹æŠ€æœ¯é€šå¸¸é€‚ç”¨äºå¤§å¤šæ•°ä»»åŠ¡ï¼Œä½†æœ‰æ—¶æ‚¨éœ€è¦ä½¿ç”¨æ›´å¼ºå¤§çš„å·¥å…·åŒ…æ¥è§£å†³å¤æ‚çš„ç”Ÿæˆå¼ AI é—®é¢˜ã€‚æ­¤ç±»é—®é¢˜å’Œä»»åŠ¡åŒ…æ‹¬ï¼š</p>
<p>Context length</p>
<p>Summarizing an entire book into a digestible synopsis.
å°†æ•´æœ¬ä¹¦æ€»ç»“æˆä¸€ä¸ªæ˜“äºç†è§£çš„æè¦ã€‚</p>
<p>Combining sequential LLM inputs/outputs
ç»„åˆé¡ºåº LLM è¾“å…¥/è¾“å‡º</p>
<p>Creating a story for a book including the characters, plot, and world building.
ä¸ºä¸€æœ¬ä¹¦åˆ›ä½œä¸€ä¸ªæ•…äº‹ï¼ŒåŒ…æ‹¬äººç‰©ã€æƒ…èŠ‚å’Œä¸–ç•Œæ„å»ºã€‚</p>
<p>Performing complex reasoning tasks
æ‰§è¡Œå¤æ‚çš„æ¨ç†ä»»åŠ¡</p>
<p>LLMs acting as an agent. For example, you could create an LLM agent to help you achieve your personal fitness goals.
LLMs å……å½“ä»£ç†ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ª LLM ä»£ç†æ¥å¸®åŠ©æ‚¨å®ç°ä¸ªäººå¥èº«ç›®æ ‡ã€‚</p>
<p>To skillfully tackle such complex generative AI challenges, becoming acquainted with LangChain, an open source framework, is highly beneficial. This tool simplifies and enhances your LLMâ€™s workflows substantially.
ä¸ºäº†å·§å¦™åœ°åº”å¯¹å¦‚æ­¤å¤æ‚çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŒ‘æˆ˜ï¼Œç†Ÿæ‚‰å¼€æºæ¡†æ¶LangChainæ˜¯éå¸¸æœ‰ç›Šçš„ã€‚è¯¥å·¥å…·å¤§å¤§ç®€åŒ–å’Œå¢å¼ºäº†LLMçš„å·¥ä½œæµç¨‹ã€‚</p>
<h1 id="introduction-to-langchain">Introduction to LangChain</h1>
<p>LangChain is a versatile frameworkÂ that enables the creation of applications utilizing LLMs and is available as both aÂ <a href="https://oreil.ly/YPid-">Python</a>Â and aÂ <a href="https://oreil.ly/5Vl0W">TypeScript</a>Â package. Its central tenet is that the most impactful and distinct applications wonâ€™t merely interface with a language model via an API, but will also:
LangChain æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½æ¡†æ¶ï¼Œæ”¯æŒä½¿ç”¨ LLMs åˆ›å»ºåº”ç”¨ç¨‹åºï¼Œå¹¶å¯ä½œä¸º Python å’Œ TypeScript åŒ…ä½¿ç”¨ã€‚å®ƒçš„æ ¸å¿ƒåŸåˆ™æ˜¯ï¼Œæœ€æœ‰å½±å“åŠ›å’Œæœ€ç‹¬ç‰¹çš„åº”ç”¨ç¨‹åºä¸ä»…ä¼šé€šè¿‡ API ä¸è¯­è¨€æ¨¡å‹äº¤äº’ï¼Œè€Œä¸”è¿˜ä¼šï¼š
Enhance data awareness</p>
<p>The framework aims to establish a seamless connection between a language model and external data sources.</p>
<p>Enhance agency</p>
<p>It strives to equip language models with the ability to engage with and influence their environment.</p>
<p>The LangChain framework illustrated inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch04.html#figure-4-1">FigureÂ 4-1</a>Â providesÂ a range of modular abstractions that are essential for working with LLMs, along with a broad selection of implementations for these abstractions.</p>
<p><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098153427/files/assets/pega_0401.png" alt="pega 0401"></p>
<h6 id="figure-4-1the-major-modules-of-the-langchain-llm-framework">Figure 4-1.Â The major modules of the LangChain LLM framework</h6>
<p>Each module is designed to be user-friendly and can beÂ efficiently utilized independently or together. There are currently six common modules within LangChain:</p>
<p>Model I/O</p>
<p>Handles input/output operations related to the model</p>
<p>Retrieval</p>
<p>Focuses on retrieving relevant text for the LLM</p>
<p>Chains</p>
<p>Also known asÂ <em>LangChain runnables</em>, chains enable the construction of sequences of LLM operations or function calls</p>
<p>Agents</p>
<p>Allows chains to make decisions on which tools to use based on high-level directives or instructions</p>
<p>Memoryè®°å¿†</p>
<p>Persists the state of an application between different runs of a chain
åœ¨é“¾çš„ä¸åŒè¿è¡Œä¹‹é—´æŒä¹…ä¿å­˜åº”ç”¨ç¨‹åºçš„çŠ¶æ€</p>
<p>Callbackså›è°ƒ</p>
<p>For running additional code on specific events, such as when every new token is generated
ç”¨äºåœ¨ç‰¹å®šäº‹ä»¶ä¸Šè¿è¡Œå…¶ä»–ä»£ç ï¼Œä¾‹å¦‚åœ¨ç”Ÿæˆæ¯ä¸ªæ–°ä»¤ç‰Œæ—¶</p>
<h2 id="environment-setup">Environment Setup</h2>
<p>You can install LangChain on your terminalÂ with either of these commands:</p>
<ul>
<li>
<p><code>pip install langchain langchain-openai</code></p>
</li>
<li>
<p><code>conda install -c conda-forge langchain langchain-openai</code></p>
</li>
</ul>
<p>If you would prefer to install the package requirements for the entire book, you can use theÂ <a href="https://oreil.ly/WKOma"><em>requirements.txt</em></a>Â file from the GitHub repository.</p>
<p>Itâ€™s recommended to install the packages within aÂ virtual environment:</p>
<p>Create a virtual environment</p>
<p><code>python -m venv venv</code></p>
<p>Activate the virtual environment</p>
<p><code>source venv/bin/activate</code></p>
<p>Install the dependencies</p>
<p><code>pip install -r requirements.txt</code></p>
<p>LangChain requires integrations with one or more model providers. For example, to use OpenAIâ€™s model APIs, youâ€™ll need to install their Python package withÂ <code>pip install openai</code>.</p>
<p>As discussed inÂ <a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch01.html#five_principles_01">ChapterÂ 1</a>, itâ€™s best practice to set an environment variable calledÂ <code>OPENAI_API_KEY</code>Â in your terminal or load it from anÂ <em>.env</em>Â file usingÂ <a href="https://oreil.ly/wvuO7"><code>python-dotenv</code></a>. However, for prototyping you can choose to skip this step by passing in your API key directly when loading a chat model in LangChain:</p>
<pre tabindex="0"><code>from langchain_openai.chat_models import ChatOpenAI
chat = ChatOpenAI(api_key=&#34;api_key&#34;)
</code></pre><h6 id="warning">WARNING</h6>
<p>Hardcoding API keys in scripts is not recommendedÂ due to security reasons. Instead, utilize environment variables or configuration files to manage your keys.
å‡ºäºå®‰å…¨åŸå› ï¼Œä¸å»ºè®®åœ¨è„šæœ¬ä¸­å¯¹ API å¯†é’¥è¿›è¡Œç¡¬ç¼–ç ã€‚ç›¸åï¼Œè¯·ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶æ¥ç®¡ç†å¯†é’¥ã€‚</p>
<p>In the constantly evolving landscape of LLMs, you can encounter the challenge of disparities across different model APIs. The lack of standardization in interfaces can induce extra layers of complexity in prompt engineering and obstruct the seamless integration of diverse models into your projects.
åœ¨ LLMs ä¸æ–­å‘å±•çš„ç¯å¢ƒä¸­ï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ°ä¸åŒæ¨¡å‹ API ä¹‹é—´å­˜åœ¨å·®å¼‚çš„æŒ‘æˆ˜ã€‚æ¥å£ç¼ºä¹æ ‡å‡†åŒ–å¯èƒ½ä¼šåœ¨æç¤ºå·¥ç¨‹ä¸­å¢åŠ é¢å¤–çš„å¤æ‚æ€§ï¼Œå¹¶é˜»ç¢å°†ä¸åŒæ¨¡å‹æ— ç¼é›†æˆåˆ°æ‚¨çš„é¡¹ç›®ä¸­ã€‚</p>
<p>This is where LangChain comes into play. As a comprehensive framework, LangChain allows you to easily consume the varying interfaces of different models.</p>
<p>LangChainâ€™s functionality ensures that you arenâ€™t required to reinvent your prompts or code every time you switch between models. Its platform-agnostic approach promotes rapid experimentation with a broad range of models, such asÂ <a href="https://www.anthropic.com/">Anthropic</a>,Â <a href="https://cloud.google.com/vertex-ai">Vertex AI</a>,Â <a href="https://openai.com/">OpenAI</a>, andÂ <a href="https://oreil.ly/bedrock">BedrockChat</a>. This not only expedites the model evaluation process but also saves critical time and resources by simplifying complex model integrations.</p>
<p>In the sections that follow, youâ€™ll be using the OpenAI package and their API in LangChain.</p>
<h1 id="chat-models">Chat Models</h1>
<p>Chat models such as GPT-4 have become theÂ primary way to interface with OpenAIâ€™s API. Instead of offering a straightforward â€œinput text, output textâ€ response, they propose an interaction method whereÂ <em>chat messages</em>Â are the input and output elements.</p>
<p>Generating LLM responses using chat models involvesÂ inputting one or more messages into the chat model. In the context of LangChain, the currently accepted message types areÂ <code>AIMessage</code>,Â <code>HumanMessage</code>, andÂ <code>SystemMessage</code>. The output from a chat model will always be anÂ <code>AIMessage</code>.</p>
<p>SystemMessage</p>
<p>Represents information that shouldÂ be instructions to the AI system. These are used to guide the AIâ€™s behavior or actions in some way.</p>
<p>HumanMessage</p>
<p>Represents information coming fromÂ a human interacting with the AI system. This could be a question, a command, or any other input from a human user that the AI needs to process and respond to.</p>
<p>AIMessage</p>
<p>Represents information coming fromÂ the AI system itself. This is typically the AIâ€™s response to aÂ <code>HumanMessage</code>Â or the result of aÂ <code>SystemMessage</code>Â instruction.</p>
<h6 id="note">NOTE</h6>
<p>Make sure to leverage theÂ <code>SystemMessage</code>Â for delivering explicit directions. OpenAI has refined GPT-4 and upcoming LLM models to pay particular attention to the guidelines given within this type of message.</p>
<p>Letâ€™s create a joke generator inÂ LangChain.</p>
<p>Input:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_openai.chat_models <span style="color:#f92672">import</span> ChatOpenAI
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.schema <span style="color:#f92672">import</span> AIMessage, HumanMessage, SystemMessage
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chat <span style="color:#f92672">=</span> ChatOpenAI(temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>messages <span style="color:#f92672">=</span> [SystemMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;&#39;Act as a senior software engineer
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">at a startup company.&#39;&#39;&#39;</span>),
</span></span><span style="display:flex;"><span>HumanMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;&#39;Please can you provide a funny joke
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">about software engineers?&#39;&#39;&#39;</span>)]
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> chat<span style="color:#f92672">.</span>invoke(input<span style="color:#f92672">=</span>messages)
</span></span><span style="display:flex;"><span>print(response<span style="color:#f92672">.</span>content)
</span></span></code></pre></div><pre tabindex="0"><code>
Output:
</code></pre><p>Sure, here&rsquo;s a lighthearted joke for you:
Why did the software engineer go broke?
Because he lost his domain in a bet and couldn&rsquo;t afford to renew it.</p>
<pre tabindex="0"><code></code></pre><p>First, youâ€™ll importÂ <code>ChatOpenAI</code>,Â <code>AIMessage</code>,Â <code>HumanMessage</code>, andÂ <code>SystemMessage</code>. Then create an instance of theÂ <code>ChatOpenAI</code>Â class with a temperature parameter of 0.5 (randomness).
é¦–å…ˆï¼Œæ‚¨å°†å¯¼å…¥ ChatOpenAI ï¼Œ AIMessage ï¼Œ HumanMessage å’Œ SystemMessage ã€‚ç„¶ååˆ›å»ºæ¸©åº¦å‚æ•°ä¸º 0.5ï¼ˆéšæœºæ€§ï¼‰çš„ ChatOpenAI ç±»çš„å®ä¾‹ã€‚</p>
<p>After creating a model, a list namedÂ <code>messages</code>Â is populated with aÂ <code>SystemMessage</code>Â object, defining the role for the LLM, and aÂ <code>HumanMessage</code>Â object, which asks for a software engineerâ€”related joke.
åˆ›å»ºæ¨¡å‹åï¼Œä¸€ä¸ªåä¸º messages çš„åˆ—è¡¨å°†å¡«å……ä¸€ä¸ª SystemMessage å¯¹è±¡ï¼Œè¯¥å¯¹è±¡å®šä¹‰äº† LLM çš„è§’è‰²ï¼Œä»¥åŠä¸€ä¸ª HumanMessage å¯¹è±¡ï¼Œè¯¥å¯¹è±¡è¦æ±‚ä¸è½¯ä»¶å·¥ç¨‹å¸ˆç›¸å…³çš„ç¬‘è¯ã€‚</p>
<p>Calling the chat model withÂ <code>.invoke(input=messages)</code>Â feeds the LLM with a list of messages, and then you retrieve the LLMâ€™s response withÂ <code>response.content</code>.
ä½¿ç”¨ .invoke(input=messages) è°ƒç”¨èŠå¤©æ¨¡å‹ä¼šå‘ LLM æä¾›æ¶ˆæ¯åˆ—è¡¨ï¼Œç„¶åä½¿ç”¨ response.content æ£€ç´¢ LLM çš„å“åº”ã€‚</p>
<p>There is a legacy method that allows you toÂ directly call theÂ <code>chat</code>Â object withÂ <code>chat(messages=messages)</code>:
æœ‰ä¸€ç§é—ç•™æ–¹æ³•å…è®¸æ‚¨ä½¿ç”¨Â <code>chat(messages=messages)</code>Â ç›´æ¥è°ƒç”¨Â <code>chat</code>Â å¯¹è±¡ï¼š</p>
<pre tabindex="0"><code>response = chat(messages=messages)
</code></pre><h1 id="streaming-chat-models-æµå¼èŠå¤©æ¨¡å‹">Streaming Chat Models# æµå¼èŠå¤©æ¨¡å‹</h1>
<p>You might have observed whileÂ using ChatGPT how words are sequentially returned to you, one character at a time. This distinct pattern of response generation is referred to asÂ <em>streaming</em>, and it plays a crucial role in enhancing the performance of chat-based applications:
æ‚¨å¯èƒ½åœ¨ä½¿ç”¨ ChatGPT æ—¶è§‚å¯Ÿåˆ°å•è¯æ˜¯å¦‚ä½•æŒ‰é¡ºåºè¿”å›ç»™æ‚¨çš„ï¼Œä¸€æ¬¡ä¸€ä¸ªå­—ç¬¦ã€‚è¿™ç§ç‹¬ç‰¹çš„å“åº”ç”Ÿæˆæ¨¡å¼ç§°ä¸ºæµå¼å¤„ç†ï¼Œå®ƒåœ¨å¢å¼ºåŸºäºèŠå¤©çš„åº”ç”¨ç¨‹åºçš„æ€§èƒ½æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼š</p>
<pre tabindex="0"><code>for chunk in chat.stream(messages):
    print(chunk.content, end=&#34;&#34;, flush=True)```

When you callÂ `chat.stream(messages)`, it yields chunks of the message one at a time. This means each segment of the chat message is individually returned. As each chunk arrives, it is then instantaneously printed to the terminal and flushed. This way,Â _streaming_Â allows for minimal latency from your LLM responses.
å½“æ‚¨è°ƒç”¨Â `chat.stream(messages)`Â æ—¶ï¼Œå®ƒä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªæ¶ˆæ¯å—ã€‚è¿™æ„å‘³ç€èŠå¤©æ¶ˆæ¯çš„æ¯ä¸ªç‰‡æ®µéƒ½ä¼šå•ç‹¬è¿”å›ã€‚å½“æ¯ä¸ªå—åˆ°è¾¾æ—¶ï¼Œå®ƒä¼šç«‹å³æ‰“å°åˆ°ç»ˆç«¯å¹¶å†²æ´—ã€‚è¿™æ ·ï¼Œæµå¼ä¼ è¾“å¯ä»¥å°† LLM å“åº”çš„å»¶è¿Ÿé™è‡³æœ€ä½ã€‚


Streaming holds several benefits from an end-user perspective. First, it dramatically reduces the waiting time for users. As soon as the text starts generating character by character, users can start interpreting the message. Thereâ€™s no need for a full message to be constructed before it is seen. This, in turn, significantly enhances user interactivity and minimizes latency.
ä»æœ€ç»ˆç”¨æˆ·çš„è§’åº¦æ¥çœ‹ï¼Œæµåª’ä½“æœ‰å‡ ä¸ªå¥½å¤„ã€‚é¦–å…ˆï¼Œå®ƒå¤§å¤§å‡å°‘äº†ç”¨æˆ·çš„ç­‰å¾…æ—¶é—´ã€‚ä¸€æ—¦æ–‡æœ¬å¼€å§‹é€ä¸ªå­—ç¬¦ç”Ÿæˆï¼Œç”¨æˆ·å°±å¯ä»¥å¼€å§‹è§£é‡Šæ¶ˆæ¯ã€‚åœ¨çœ‹åˆ°å®Œæ•´æ¶ˆæ¯ä¹‹å‰ï¼Œæ— éœ€æ„å»ºå®Œæ•´çš„æ¶ˆæ¯ã€‚è¿™åè¿‡æ¥åˆå¤§å¤§å¢å¼ºäº†ç”¨æˆ·äº¤äº’æ€§å¹¶æœ€å¤§é™åº¦åœ°å‡å°‘äº†å»¶è¿Ÿã€‚


Nevertheless, this technique comes with its own set of challenges. One significant challenge is parsing the outputs while they are being streamed. Understanding and appropriately responding to the message as it is being formed can prove to be intricate, especially when the content is complex and detailed.
ç„¶è€Œï¼Œè¿™ç§æŠ€æœ¯ä¹Ÿæœ‰å…¶è‡ªèº«çš„ä¸€ç³»åˆ—æŒ‘æˆ˜ã€‚ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜æ˜¯åœ¨æµå¼ä¼ è¾“è¾“å‡ºæ—¶è§£æè¾“å‡ºã€‚åœ¨ä¿¡æ¯å½¢æˆæ—¶ç†è§£å¹¶é€‚å½“åœ°å›åº”ä¿¡æ¯å¯èƒ½è¢«è¯æ˜æ˜¯é”™ç»¼å¤æ‚çš„ï¼Œå°¤å…¶æ˜¯å½“å†…å®¹å¤æ‚è€Œè¯¦ç»†æ—¶ã€‚


# Creating Multiple LLM Generations

There may be scenarios where you find it usefulÂ to generate multiple responses from LLMs. This is particularly true while creating dynamic content like social media posts. Rather than providing a list of messages, you provide aÂ list of message lists.
åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°ä» LLMs ç”Ÿæˆå¤šä¸ªå“åº”å¾ˆæœ‰ç”¨ã€‚åœ¨åˆ›å»ºç¤¾äº¤åª’ä½“å¸–å­ç­‰åŠ¨æ€å†…å®¹æ—¶å°¤å…¶å¦‚æ­¤ã€‚æ‚¨æä¾›çš„ä¸æ˜¯é‚®ä»¶åˆ—è¡¨ï¼Œè€Œæ˜¯é‚®ä»¶åˆ—è¡¨åˆ—è¡¨ã€‚


Input:
```python
# 2x lists of messages, which is the same as [messages, messages]
synchronous_llm_result = chat.batch([messages]*2)
print(synchronous_llm_result)
</code></pre><p>Output:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>[AIMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;&#39;Sure, here&#39;s a lighthearted joke for you:</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">Why did
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">the software engineer go broke?</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">Because he kept forgetting to Ctrl+ Z
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">his expenses!&#39;&#39;&#39;</span>),
</span></span><span style="display:flex;"><span>AIMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;&#39;Sure, here</span><span style="color:#ae81ff">\&#39;</span><span style="color:#e6db74">s a lighthearted joke for you:</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">Why do
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">software engineers prefer dark mode?</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">Because it</span><span style="color:#ae81ff">\&#39;</span><span style="color:#e6db74">s easier on their
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;byte&#34; vision!&#39;&#39;&#39;</span>)]
</span></span></code></pre></div><p>The benefit of usingÂ <code>.batch()</code>Â overÂ <code>.invoke()</code>Â is that you can parallelize the number of API requests made to OpenAI.
ä½¿ç”¨Â <code>.batch()</code>Â è€Œä¸æ˜¯Â <code>.invoke()</code>Â çš„å¥½å¤„æ˜¯ï¼Œæ‚¨å¯ä»¥å¹¶è¡ŒåŒ–å‘ OpenAI å‘å‡ºçš„ API è¯·æ±‚æ•°é‡ã€‚</p>
<p>For any runnable in LangChain, you can add aÂ <code>RunnableConfig</code>Â argument to theÂ <code>batch</code>Â function that contains many configurable parameters, includingÂ <code>max_``concurrency</code>:</p>
<blockquote>
<p>å¯¹äºLangChainä¸­çš„ä»»ä½•å¯è¿è¡Œå¯¹è±¡ï¼Œæ‚¨å¯ä»¥å‘Â <code>batch</code>Â å‡½æ•°æ·»åŠ ä¸€ä¸ªÂ <code>RunnableConfig</code>Â å‚æ•°ï¼Œè¯¥å‡½æ•°åŒ…å«è®¸å¤šå¯é…ç½®çš„å‚æ•°ï¼ŒåŒ…æ‹¬Â <code>max_</code>Â <code>concurrency</code>Â ï¼š</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.runnables.config <span style="color:#f92672">import</span> RunnableConfig
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a RunnableConfig with the desired concurrency limit:</span>
</span></span><span style="display:flex;"><span>config <span style="color:#f92672">=</span> RunnableConfig(max_concurrency<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Call the .batch() method with the inputs and config:</span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> chat<span style="color:#f92672">.</span>batch([messages, messages], config<span style="color:#f92672">=</span>config)
</span></span></code></pre></div><hr>
<pre tabindex="0"><code class="language-######" data-lang="######">
In computer science,Â _asynchronous (async) functions_Â are thoseÂ that operate independently of other processes, thereby enabling several API requests to be run concurrently without waiting for each other. In LangChain, these async functions let you make many API requests all at once, not one after the other. This is especially helpful in more complex workflows and decreases the overall latency to your users.
&gt; åœ¨è®¡ç®—æœºç§‘å­¦ä¸­ï¼Œå¼‚æ­¥ï¼ˆå¼‚æ­¥ï¼‰å‡½æ•°æ˜¯ç‹¬ç«‹äºå…¶ä»–è¿›ç¨‹è¿è¡Œçš„å‡½æ•°ï¼Œä»è€Œä½¿å¤šä¸ª API è¯·æ±‚èƒ½å¤ŸåŒæ—¶è¿è¡Œè€Œæ— éœ€ç›¸äº’ç­‰å¾…ã€‚åœ¨LangChainä¸­ï¼Œè¿™äº›å¼‚æ­¥å‡½æ•°å…è®¸ä½ ä¸€æ¬¡å‘å‡ºè®¸å¤šAPIè¯·æ±‚ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°å‘å‡ºã€‚è¿™åœ¨æ›´å¤æ‚çš„å·¥ä½œæµä¸­ç‰¹åˆ«æœ‰ç”¨ï¼Œå¹¶å‡å°‘äº†ç”¨æˆ·çš„æ•´ä½“å»¶è¿Ÿã€‚


Most of the asynchronous functions within LangChain are simply prefixed with the letterÂ `a`, such asÂ `.ainvoke()`Â andÂ `.abatch()`. If you would like to use the async API for more efficient task performance, thenÂ utilize these functions.
&gt; LangChainä¸­çš„å¤§å¤šæ•°å¼‚æ­¥å‡½æ•°éƒ½åªæ˜¯ä»¥å­—æ¯Â `a`Â ä¸ºå‰ç¼€ï¼Œä¾‹å¦‚Â `.ainvoke()`Â å’ŒÂ `.abatch()`Â ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨å¼‚æ­¥ API æ¥æé«˜ä»»åŠ¡æ€§èƒ½ï¼Œè¯·ä½¿ç”¨è¿™äº›å‡½æ•°ã€‚


# LangChain Prompt Templates

Up until this point, youâ€™ve been hardcoding theÂ strings in theÂ `ChatOpenAI`Â objects. As your LLM applications grow in size, it becomes increasingly important to utilizeÂ  prompt templates.
åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‚¨ä¸€ç›´åœ¨å¯¹Â `ChatOpenAI`Â å¯¹è±¡ä¸­çš„å­—ç¬¦ä¸²è¿›è¡Œç¡¬ç¼–ç ã€‚éšç€ LLM åº”ç”¨ç¨‹åºè§„æ¨¡çš„å¢é•¿ï¼Œä½¿ç”¨æç¤ºæ¨¡æ¿å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚


Prompt templates Â are good for generating reproducible prompts for AI language models. They consist of aÂ _template_, a text string that can take in parameters, and construct a text prompt for a language model.
æç¤ºæ¨¡æ¿é€‚ç”¨äºä¸º AI è¯­è¨€æ¨¡å‹ç”Ÿæˆå¯é‡ç°çš„æç¤ºã€‚å®ƒä»¬ç”±ä¸€ä¸ªæ¨¡æ¿ã€ä¸€ä¸ªå¯ä»¥æ¥æ”¶å‚æ•°çš„æ–‡æœ¬å­—ç¬¦ä¸²ç»„æˆï¼Œå¹¶ä¸ºè¯­è¨€æ¨¡å‹æ„é€ ä¸€ä¸ªæ–‡æœ¬æç¤ºã€‚


Without prompt templates, you would likely useÂ PythonÂ `f-string`Â formatting:
å¦‚æœæ²¡æœ‰æç¤ºæ¨¡æ¿ï¼Œæ‚¨å¯èƒ½ä¼šä½¿ç”¨ PythonÂ `f-string`Â æ ¼å¼
</code></pre><p>language = &ldquo;Python&rdquo;
prompt = f&quot;What is the best way to learn coding in {language}?&quot;
print(prompt) # What is the best way to learn coding in Python?</p>
<pre tabindex="0"><code>
But why not simply use anÂ `f-string`Â for prompt templating? Using LangChainâ€™s prompt templates instead allows you to easily:
ä½†æ˜¯ä¸ºä»€ä¹ˆä¸ç®€å•åœ°ä½¿ç”¨Â `f-string`Â è¿›è¡Œæç¤ºæ¨¡æ¿å‘¢ï¼Ÿç›¸åï¼Œä½¿ç”¨LangChainçš„æç¤ºæ¨¡æ¿å¯ä»¥è®©ä½ è½»æ¾åœ°ï¼š


- Validate your prompt inputs
éªŒè¯æç¤ºè¾“å…¥

    
- Combine multiple prompts together with composition
å°†å¤šä¸ªæç¤ºä¸ç»„åˆç»“åˆåœ¨ä¸€èµ·

    
- Define custom selectors that will inject k-shot examples into your prompt
å®šä¹‰è‡ªå®šä¹‰é€‰æ‹©å™¨ï¼Œå°† k-shot ç¤ºä¾‹æ³¨å…¥åˆ°æ‚¨çš„æç¤ºä¸­

    
- Save and load prompts fromÂ _.yml_Â andÂ _.json_Â files
ä¿å­˜å’ŒåŠ è½½.ymlå’Œ.jsonæ–‡ä»¶ä¸­çš„æç¤º

    
- Create custom prompt templates thatÂ execute additional code or instructions when created
åˆ›å»ºè‡ªå®šä¹‰æç¤ºæ¨¡æ¿ï¼Œä»¥ä¾¿åœ¨åˆ›å»ºæ—¶æ‰§è¡Œå…¶ä»–ä»£ç æˆ–æŒ‡ä»¤

    

# LangChain Expression Language (LCEL)

TheÂ `|`Â pipe operator is a key component ofÂ LangChain Expression Language (LCEL) that allows you to chain together different components orÂ _runnables_Â in a data processing pipeline.

In LCEL, theÂ `|`Â operator is similar to the Unix pipe operator. It takes the output of one component and feeds it as input to the next component in the chain. This allows you to easily connect and combine different components to create a complex chain of operations:
</code></pre><p>chain = prompt | model</p>
<pre tabindex="0"><code>
TheÂ `|`Â operator is used to chain together the prompt and model components. The output of the prompt component is passed as input to the model component. This chaining mechanism allows you to build complex chains from basic components and enables the seamless flow of data between different stages of the processing pipeline.

Additionally,Â _the order matters_, so you could technically create this chain:
</code></pre><p>bad_order_chain = model | prompt</p>
<pre tabindex="0"><code>
But it would produce an error after using theÂ `invoke`Â function, because the values returned fromÂ `model`Â are not compatible with the expected inputs for the prompt.

Letâ€™s create a business name generator using prompt templates that will return five to seven relevant business names:
</code></pre><p>from langchain_openai.chat_models import ChatOpenAI
from langchain_core.prompts import (SystemMessagePromptTemplate,
ChatPromptTemplate)</p>
<p>template = &quot;&quot;&quot;
You are a creative consultant brainstorming names for businesses.</p>
<p>You must follow the following principles:
{principles}</p>
<p>Please generate a numerical list of five catchy names for a start-up in the
{industry} industry that deals with {context}?</p>
<p>Here is an example of the format:</p>
<ol>
<li>Name1</li>
<li>Name2</li>
<li>Name3</li>
<li>Name4</li>
<li>Name5
&quot;&quot;&quot;</li>
</ol>
<p>model = ChatOpenAI()
system_prompt = SystemMessagePromptTemplate.from_template(template)
chat_prompt = ChatPromptTemplate.from_messages([system_prompt])</p>
<p>chain = chat_prompt | model</p>
<p>result = chain.invoke({
&ldquo;industry&rdquo;: &ldquo;medical&rdquo;,
&ldquo;context&rdquo;:&lsquo;&lsquo;&lsquo;creating AI solutions by automatically summarizing patient
records&rsquo;&rsquo;&rsquo;,
&ldquo;principles&rdquo;:&lsquo;&lsquo;&lsquo;1. Each name should be short and easy to
remember. 2. Each name should be easy to pronounce.
3. Each name should be unique and not already taken by another company.&rsquo;&rsquo;&rsquo;
})</p>
<p>print(result.content)</p>
<pre tabindex="0"><code>
Output:
</code></pre><ol>
<li>SummarAI</li>
<li>MediSummar</li>
<li>AutoDocs</li>
<li>RecordAI</li>
<li>SmartSummarize</li>
</ol>
<pre tabindex="0"><code>

First, youâ€™ll importÂ `ChatOpenAI`,Â `SystemMessagePromptTemplate`, andÂ `ChatPromptTemplate`. Then, youâ€™ll define a prompt template with specific guidelines underÂ `template`, instructing the LLM to generate business names.Â `ChatOpenAI()`Â initializes the chat, whileÂ `SystemMessagePromptTemplate.from_template(template)`Â andÂ `ChatPromptTemplate.from_messages([system_prompt])`Â create your prompt template.
é¦–å…ˆï¼Œæ‚¨å°†å¯¼å…¥Â `ChatOpenAI`Â ï¼ŒÂ `SystemMessagePromptTemplate`Â å’ŒÂ `ChatPromptTemplate`Â ã€‚ç„¶åï¼Œæ‚¨å°†åœ¨Â `template`Â ä¸‹å®šä¹‰ä¸€ä¸ªå…·æœ‰ç‰¹å®šå‡†åˆ™çš„æç¤ºæ¨¡æ¿ï¼ŒæŒ‡ç¤º LLM ç”Ÿæˆä¼ä¸šåç§°ã€‚Â `ChatOpenAI()`Â åˆå§‹åŒ–èŠå¤©ï¼Œè€ŒÂ `SystemMessagePromptTemplate.from_template(template)`Â å’ŒÂ `ChatPromptTemplate.from_messages([system_prompt])`Â åˆ›å»ºæç¤ºæ¨¡æ¿ã€‚


You create an LCELÂ `chain`Â by piping togetherÂ `chat_prompt`Â and theÂ `model`, which is thenÂ _invoked_. This replaces theÂ `{industries}`,Â `{context}`, andÂ `{principles}`Â placeholders in the prompt with the dictionary values within theÂ `invoke`Â function.
&gt; æ‚¨å¯ä»¥é€šè¿‡å°†Â `chat_prompt`Â å’ŒÂ `model`Â ç®¡é“è¿æ¥åœ¨ä¸€èµ·æ¥åˆ›å»ºä¸€ä¸ª LCELÂ `chain`Â ï¼Œç„¶åè°ƒç”¨è¯¥ç®¡é“ã€‚è¿™ä¼šå°†æç¤ºç¬¦ä¸­çš„Â `{industries}`Â ã€Â `{context}`Â å’ŒÂ `{principles}`Â å ä½ç¬¦æ›¿æ¢ä¸ºÂ `invoke`Â å‡½æ•°ä¸­çš„å­—å…¸å€¼ã€‚


Finally, you extract the LLMâ€™s response as a string accessing theÂ `.content`Â property on theÂ `result`Â variable.
&gt; æœ€åï¼Œå°† LLM çš„å“åº”æå–ä¸ºè®¿é—®Â `result`Â å˜é‡çš„Â `.content`Â å±æ€§çš„å­—ç¬¦ä¸²ã€‚

---
#### GIVE DIRECTION AND SPECIFY FORMAT

Carefully crafted instructions might include things like â€œYou are a creative consultant brainstorming names for businessesâ€ and â€œPlease generate a numerical list of five to seven catchyÂ names for a start-up.â€ Cues like these guide your LLM to perform the exact task you require from it.

## Using PromptTemplate with Chat Models

LangChain provides a more traditionalÂ template calledÂ `PromptTemplate`, which requiresÂ `input_variables`Â andÂ `template`Â arguments.
&gt; LangChainæä¾›äº†ä¸€ä¸ªæ›´ä¼ ç»Ÿçš„æ¨¡æ¿ï¼Œç§°ä¸ºÂ `PromptTemplate`Â ï¼Œå®ƒéœ€è¦Â `input_variables`Â å’ŒÂ `template`Â å‚æ•°ã€‚


Input:
</code></pre><p>from langchain_core.prompts import PromptTemplate
from langchain.prompts.chat import SystemMessagePromptTemplate
from langchain_openai.chat_models import ChatOpenAI
prompt=PromptTemplate(
template=&lsquo;&lsquo;&lsquo;You are a helpful assistant that translates {input_language} to
{output_language}.&rsquo;&rsquo;&rsquo;,
input_variables=[&ldquo;input_language&rdquo;, &ldquo;output_language&rdquo;],
)
system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)
chat = ChatOpenAI()
chat.invoke(system_message_prompt.format_messages(
input_language=&ldquo;English&rdquo;,output_language=&ldquo;French&rdquo;))</p>
<pre tabindex="0"><code>
Output:
</code></pre><p>AIMessage(content=&ldquo;Vous Ãªtes un assistant utile qui traduit l&rsquo;anglais en
franÃ§ais.&rdquo;, additional_kwargs={}, example=False)</p>
<pre tabindex="0"><code>
# Output Parsers

InÂ [ChapterÂ 3](https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch03.html#standard_practices_03), you used regular expressions (regex) to extract structured data from text that contained numerical lists, but itâ€™s possible to do this automatically in LangChain withÂ _output parsers_.

_Output parsers_Â are a higher-level abstraction providedÂ by LangChain for parsing structured data from LLM string responses. Currently the available output parsers are:

List parser

  Returns a list of comma-separatedÂ items.

Datetime parser

  Parses an LLM output into datetimeÂ format.

Enum parser

  Parses strings into enumÂ values.

Auto-fixing parser

Wraps another output parser, and if thatÂ output parser fails, it will call another LLM to fix any errors.

Pydantic (JSON) parser

Parses LLM responses into JSON output thatÂ conforms to a Pydantic schema.

Retry parser

Provides retrying a failed parseÂ from a previous output parser.

Structured output parser

Can be used when you want toÂ return multiple fields.

XML parser

Parses LLM responses into an XML-basedÂ format.

As youâ€™ll discover, there are two important functionsÂ for LangChain output parsers:

`.get_format_instructions()`

This function provides the necessary instructions into your prompt to output a structured format that can be parsed.

`.parse(llm_output: str)`

This function is responsible for parsing your LLM responses into a predefined format.

Generally, youâ€™ll find that the Pydantic (JSON) parser withÂ `ChatOpenAI()`Â provides the most flexibility.

The Pydantic (JSON) parser takes advantage of theÂ [Pydantic](https://oreil.ly/QIMih)Â library in Python. Pydantic is a data validation library that provides a way to validate incoming data using Python type annotations. This means that Pydantic allows you to create schemas for your data and automatically validates and parses input data according to those schemas.

Input:
    
</code></pre><p>from langchain_core.prompts.chat import (
ChatPromptTemplate,
SystemMessagePromptTemplate,
)
from langchain_openai.chat_models import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser
from pydantic.v1 import BaseModel, Field
from typing import List</p>
<p>temperature = 0.0</p>
<p>class BusinessName(BaseModel):
name: str = Field(description=&ldquo;The name of the business&rdquo;)
rating_score: float = Field(description=&lsquo;&lsquo;&lsquo;The rating score of the
business. 0 is the worst, 10 is the best.&rsquo;&rsquo;&rsquo;)</p>
<p>class BusinessNames(BaseModel):
names: List[BusinessName] = Field(description=&lsquo;&lsquo;&lsquo;A list
of busines names&rsquo;&rsquo;&rsquo;)</p>
<h1 id="set-up-a-parser--inject-instructions-into-the-prompt-template">Set up a parser + inject instructions into the prompt template:</h1>
<p>parser = PydanticOutputParser(pydantic_object=BusinessNames)</p>
<p>principles = &quot;&quot;&quot;</p>
<ul>
<li>The name must be easy to remember.</li>
<li>Use the {industry} industry and Company context to create an effective name.</li>
<li>The name must be easy to pronounce.</li>
<li>You must only return the name without any other text or characters.</li>
<li>Avoid returning full stops, \n, or any other characters.</li>
<li>The maximum length of the name must be 10 characters.
&quot;&quot;&quot;</li>
</ul>
<h1 id="chat-model-output-parser">Chat Model Output Parser:</h1>
<p>model = ChatOpenAI()
template = &ldquo;&ldquo;&ldquo;Generate five business names for a new start-up company in the
{industry} industry.
You must follow the following principles: {principles}
{format_instructions}
&quot;&rdquo;&rdquo;
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])</p>
<h1 id="creating-the-lcel-chain">Creating the LCEL chain:</h1>
<p>prompt_and_model = chat_prompt | model</p>
<p>result = prompt_and_model.invoke(
{
&ldquo;principles&rdquo;: principles,
&ldquo;industry&rdquo;: &ldquo;Data Science&rdquo;,
&ldquo;format_instructions&rdquo;: parser.get_format_instructions(),
}
)</p>
<h1 id="the-output-parser-parses-the-llm-response-into-a-pydantic-object">The output parser, parses the LLM response into a Pydantic object:</h1>
<p>print(parser.parse(result.content))</p>
<pre tabindex="0"><code>
Output:
</code></pre><p>names=[BusinessName(name=&lsquo;DataWiz&rsquo;, rating_score=8.5),
BusinessName(name=&lsquo;InsightIQ&rsquo;,
rating_score=9.2), BusinessName(name=&lsquo;AnalytiQ&rsquo;, rating_score=7.8),
BusinessName(name=&lsquo;SciData&rsquo;, rating_score=8.1),
BusinessName(name=&lsquo;InfoMax&rsquo;, rating_score=9.5)]</p>
<pre tabindex="0"><code>After youâ€™ve loaded the necessary libraries, youâ€™ll set up a ChatOpenAI model. Then create SystemMessagePromptTemplate from your template and form a ChatPromptTemplate with it. Youâ€™ll use the Pydantic models BusinessName and BusinessNames to structure your desired output, a list of unique business names. Youâ€™ll create a Pydantic parser for parsing these models and format the prompt using user-inputted variables by calling the invoke function. Feeding this customized prompt to your model, youâ€™re enabling it to produce creative, unique business names by using the parser.

Itâ€™s possible to use output parsers inside of LCEL by using this syntax:
</code></pre><p>chain = prompt | model | output_parser</p>
<pre tabindex="0"><code>Letâ€™s add the output parser directly to the chain.

Input:
</code></pre><p>parser = PydanticOutputParser(pydantic_object=BusinessNames)
chain = chat_prompt | model | parser</p>
<p>result = chain.invoke(
{
&ldquo;principles&rdquo;: principles,
&ldquo;industry&rdquo;: &ldquo;Data Science&rdquo;,
&ldquo;format_instructions&rdquo;: parser.get_format_instructions(),
}
)
print(result)</p>
<pre tabindex="0"><code>Output:
</code></pre><p>names=[BusinessName(name=&lsquo;DataTech&rsquo;, rating_score=9.5),&hellip;]</p>
<p>The chain is now responsible for prompt formatting, LLM calling, and parsing the LLMâ€™s response into aÂ <code>Pydantic</code>Â object.</p>
<hr>
<h2 id="the-preceding-promptsuse-pydantic-models-and-output-parsers-allowing-you-explicitly-tell-an-llm-your-desired-response-format">SPECIFY FORMAT
The preceding promptsÂ use Pydantic models and output parsers, allowing you explicitly tell an LLM your desired response format.</h2>
<h2 id="its-worth-knowing-that-by-asking-an-llm-to-provide-structured-json-output-you-can-create-a-flexible-and-generalizable-api-from-the-llms-response-there-are-limitations-to-this-such-as-the-size-of-the-json-created-and-the-reliability-of-your-prompts-but-it-still-is-a-promising-area-for-llm-applications">Itâ€™s worth knowing that by asking an LLM to provide structured JSON output, you can create a flexible and generalizable API from the LLMâ€™s response. There are limitations to this, such as the size of the JSON created and the reliability of your prompts, but it still is a promising area for LLM applications.</h2>
<p>WARNING</p>
<h2 id="you-should-take-care-of-edge-cases-as-well-as-adding-error-handling-statements-since-llm-outputs-might-not-always-be-in-your-desired-format">You should take care of edge cases as well as adding error handling statements, since LLM outputs might not always be in your desired format.</h2>
<p>Output parsers save you from the complexity and intricacy of regular expressions, providing easy-to-use functionalities for a variety of use cases. Now that youâ€™ve seen them in action, you can utilize output parsers to effortlessly structure and retrieve the data you need from an LLMâ€™s output, harnessing the full potential of AI for your tasks.</p>
<p>Furthermore, using parsers to structure the data extracted from LLMs allows you to easily choose how to organize outputs forÂ more efficient use. This can be useful if youâ€™re dealing with extensive lists and need to sort them by certain criteria, like business names.</p>
<h1 id="langchain-evals">LangChain Evals</h1>
<p>As well as output parsers to check forÂ formatting errors, most AI systems also make use ofÂ <em>evals</em>, or evaluation metrics, to measure the performance of each prompt response. LangChain has a number of off-the-shelf evaluators, which can be directly be logged in theirÂ <a href="https://oreil.ly/0Fn94">LangSmith</a>Â platform for further debugging, monitoring, and testing.Â <a href="https://wandb.ai/site">Weights and Biases</a>Â is alternative machine learning platform that offers similar functionality and tracing capabilities for LLMs.</p>
<p>Evaluation metrics are useful forÂ more than just prompt testing, as they can be used to identify positive and negative examples for retrieval as well as to build datasets for fine-tuning custom models.</p>
<p>Most eval metrics rely on a set of test cases, which are input and output pairings where you know the correct answer. Often these reference answers are created or curated manually by a human, but itâ€™s also common practice to use a smarter model like GPT-4 to generate the ground truth answers, which has been done for the following example. Given a list of descriptions of financial transactions, we used GPT-4 to classify each transaction with aÂ <code>transaction_category</code>Â andÂ <code>transaction_type</code>. The process can be found in theÂ <code>langchain-evals.ipynb</code>Â Jupyter Notebook in theÂ <a href="https://oreil.ly/a4Hut">GitHub repository</a>Â for the book.</p>
<p>With the GPT-4 answer being taken as the correct answer, itâ€™s now possible to rate the accuracy of smaller models like GPT-3.5-turbo and Mixtral 8x7b (calledÂ <code>mistral-small</code>Â in the API). If you can achieve good enough accuracy with a smaller model, you can save money or decrease latency. In addition, if that model is available open source likeÂ <a href="https://oreil.ly/Ec578">Mistralâ€™s model</a>, you can migrate that task to run on your own servers, avoiding sending potentially sensitive data outside of your organization. We recommend testing with an external API first, before going to the trouble of self-hosting an OS model.</p>
<p><a href="https://mistral.ai/">Remember to sign up</a>Â and subscribe to obtain an API key; then expose that as an environment variable by typing in your terminal:</p>
<ul>
<li><code>**export MISTRAL_API_KEY=api-key**</code></li>
</ul>
<p>The following script is part of aÂ <a href="https://oreil.ly/DqDOf">notebook</a>Â that has previously defined a dataframeÂ <code>df</code>. ForÂ brevity letâ€™s investigate only the evaluation section of the script, assuming a dataframe is already defined.</p>
<p>Input:</p>
<p>import os
from langchain_mistralai.chat_models import ChatMistralAI
from langchain.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from pydantic.v1 import BaseModel
from typing import Literal, Union
from langchain_core.output_parsers import StrOutputParser</p>
<h1 id="1-define-the-model">1. Define the model:</h1>
<p>mistral_api_key = os.environ[&ldquo;MISTRAL_API_KEY&rdquo;]</p>
<p>model = ChatMistralAI(model=&ldquo;mistral-small&rdquo;, mistral_api_key=mistral_api_key)</p>
<h1 id="2-define-the-prompt">2. Define the prompt:</h1>
<p>system_prompt = &ldquo;&ldquo;&ldquo;You are are an expert at analyzing
bank transactions, you will be categorizing a single
transaction.
Always return a transaction type and category:
do not return None.
Format Instructions:
{format_instructions}&rdquo;&rdquo;&rdquo;</p>
<p>user_prompt = &ldquo;&ldquo;&ldquo;Transaction Text:
{transaction}&rdquo;&rdquo;&rdquo;</p>
<p>prompt = ChatPromptTemplate.from_messages(
[
(
&ldquo;system&rdquo;,
system_prompt,
),
(
&ldquo;user&rdquo;,
user_prompt,
),
]
)</p>
<h1 id="3-define-the-pydantic-model">3. Define the pydantic model:</h1>
<p>class EnrichedTransactionInformation(BaseModel):
transaction_type: Union[
Literal[&ldquo;Purchase&rdquo;, &ldquo;Withdrawal&rdquo;, &ldquo;Deposit&rdquo;,
&ldquo;Bill Payment&rdquo;, &ldquo;Refund&rdquo;], None
]
transaction_category: Union[
Literal[&ldquo;Food&rdquo;, &ldquo;Entertainment&rdquo;, &ldquo;Transport&rdquo;,
&ldquo;Utilities&rdquo;, &ldquo;Rent&rdquo;, &ldquo;Other&rdquo;],
None,
]</p>
<h1 id="4-define-the-output-parser">4. Define the output parser:</h1>
<p>output_parser = PydanticOutputParser(
pydantic_object=EnrichedTransactionInformation)</p>
<h1 id="5-define-a-function-to-try-to-fix-and-remove-the-backslashes">5. Define a function to try to fix and remove the backslashes:</h1>
<p>def remove_back_slashes(string):
# double slash to escape the slash
cleaned_string = string.replace(&rdquo;\&quot;, &ldquo;&rdquo;)
return cleaned_string</p>
<h1 id="6-create-an-lcel-chain-that-fixes-the-formatting">6. Create an LCEL chain that fixes the formatting:</h1>
<p>chain = prompt | model | StrOutputParser() <br>
| remove_back_slashes | output_parser</p>
<p>transaction = df.iloc[0][&ldquo;Transaction Description&rdquo;]
result = chain.invoke(
{
&ldquo;transaction&rdquo;: transaction,
&ldquo;format_instructions&rdquo;: <br>
output_parser.get_format_instructions(),
}
)</p>
<h1 id="7-invoke-the-chain-for-the-whole-dataset">7. Invoke the chain for the whole dataset:</h1>
<p>results = []</p>
<p>for i, row in tqdm(df.iterrows(), total=len(df)):
transaction = row[&ldquo;Transaction Description&rdquo;]
try:
result = chain.invoke(
{
&ldquo;transaction&rdquo;: transaction,
&ldquo;format_instructions&rdquo;: <br>
output_parser.get_format_instructions(),
}
)
except:
result = EnrichedTransactionInformation(
transaction_type=None,
transaction_category=None
)</p>
<pre><code>results.append(result)
</code></pre>
<h1 id="8-add-the-results-to-the-dataframe-as-columns-transaction-type-and">8. Add the results to the dataframe, as columns transaction type and</h1>
<h1 id="transaction-category">transaction category:</h1>
<p>transaction_types = []
transaction_categories = []</p>
<p>for result in results:
transaction_types.append(result.transaction_type)
transaction_categories.append(
result.transaction_category)</p>
<p>df[&ldquo;mistral_transaction_type&rdquo;] = transaction_types
df[&ldquo;mistral_transaction_category&rdquo;] = transaction_categories
df.head()</p>
<pre tabindex="0"><code>Output:
</code></pre><h1 id="5-vector-databases-with-faiss-and-pinecone"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch05.html">5. Vector Databases With FAISS And Pinecone</a></h1>
<h1 id="6-autonomous-agents-with-memory-and-tools"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch06.html">6. Autonomous Agents With Memory And Tools</a></h1>
<h1 id="7-introduction-to-diffusion-models-for-image-generation"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch07.html">7. Introduction To Diffusion Models For Image Generation</a></h1>
<h1 id="8-standard-practices-for-image-generation-with-midjourney"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch08.html">8. Standard Practices For Image Generation With Midjourney</a></h1>
<h1 id="9-advanced-techniques-for-image-generation-with-stable-diffusion"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch09.html">9. Advanced Techniques For Image Generation With Stable Diffusion</a></h1>
<h1 id="10-building-ai-powered-applications"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ch10.html">10. Building AI-Powered Applications</a></h1>
<h1 id="index"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/ix01.html">Index</a></h1>
<h1 id="about-the-authors"><a href="https://learning.oreilly.com/library/view/prompt-engineering-for/9781098153427/colophon01.html">About The Authors</a></h1>

    </div>
</article>

    </main>

    <footer class="u-container u-py-xl u-text-center">
    <div class="u-mb-md">
        <span class="u-text-2xl">ğŸŒ±</span>
    </div>
    <p class="u-text-light">
        Â© 2025 Mengxi&#39;s Blog. Cultivated with intention.
    </p>
</footer>
</body>

</html>