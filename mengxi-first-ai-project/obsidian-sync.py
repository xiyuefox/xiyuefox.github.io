#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Obsidianç¬”è®°è½¬æ—¶é—´è½´JSONåŒæ­¥è„šæœ¬
å°†Obsidian Vaultä¸­çš„.mdæ–‡ä»¶è½¬æ¢ä¸ºæ—¶é—´è½´æ‰€éœ€çš„JSONæ ¼å¼
æ”¯æŒYAML Frontmatterå’Œæ— Frontmatterä¸¤ç§ç¬”è®°æ ¼å¼
"""

import os
import re
import json
from pathlib import Path
from datetime import datetime

# Obsidian Vaultç›®å½•
VAULT_PATH = os.getenv("OBSIDIAN_VAULT_PATH", "/Users/hulimofaqiu/Documents/obisidianç¬”è®°æ–‡ä»¶/")
# è¾“å‡ºJSONè·¯å¾„
# Get absolute path to the project root
PROJECT_ROOT = Path(__file__).parent.absolute()
OUTPUT_JSON = os.getenv("TIMELINE_JSON_PATH", str(PROJECT_ROOT / "blog/static/timeline-data.json"))
# å¿½ç•¥çš„ç›®å½•å’Œæ–‡ä»¶
IGNORE_PATTERNS = [".git", ".obsidian", ".DS_Store", ".smart-connections", ".smart-env", ".smtcmp_json_db", ".smtcmp_vector_db.tar.gz", ".vscode"]

# é¢œè‰²æ˜ å°„ï¼ˆç”¨äºä¸åŒç±»å‹çš„ç¬”è®°ï¼‰
CATEGORY_COLORS = {
    "å­¦ä¹ ç¬”è®°": "#b4d4a0",
    "çµæ„Ÿ": "#b8a084",
    "æ•™ç¨‹": "#f0d9b5",
    "ç¼–ç¨‹": "#d6e0f5",
    "æ•°æ®ç§‘å­¦": "#ffd9b3",
    "é»˜è®¤": "#e4e6eb"
}

def read_md_file(file_path):
    """è¯»å–Markdownæ–‡ä»¶å¹¶è§£æYAML Frontmatter"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # è§£æYAML Frontmatter
    yaml_match = re.match(r'^---\n(.*?)\n---\n', content, re.DOTALL)
    frontmatter = {}
    body = content

    if yaml_match:
        yaml_content = yaml_match.group(1)
        body = content[yaml_match.end():]
        for line in yaml_content.split('\n'):
            line = line.strip()
            if line and ':' in line:
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()
                # å¤„ç†æ•°ç»„ç±»å‹ï¼ˆå¦‚tagsï¼‰
                if value.startswith('[') and value.endswith(']'):
                    value = [v.strip().strip("'\"") for v in value[1:-1].split(',')]
                # å¤„ç†é€—å·åˆ†éš”çš„tagsï¼ˆå¦‚ï¼štags: Python, ç¼–ç¨‹ï¼‰
                elif ',' in value and key in ['tags', 'categories']:
                    value = [v.strip() for v in value.split(',')]
                frontmatter[key.lower()] = value

    return frontmatter, body

def extract_note_info(file_path, frontmatter, body):
    """æå–ç¬”è®°çš„å…³é”®ä¿¡æ¯"""
    file_name = os.path.basename(file_path)

    # ä»Frontmatterè·å–ä¿¡æ¯
    title = frontmatter.get('title', os.path.splitext(file_name)[0])
    date = frontmatter.get('date')
    tags = frontmatter.get('tags', [])
    if isinstance(tags, str):
        tags = [tags]
    category = frontmatter.get('category', 'é»˜è®¤')
    description = frontmatter.get('description', '')

    # å¦‚æœFrontmatteræ²¡æœ‰æ—¥æœŸï¼Œä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´
    if not date:
        mtime = os.path.getmtime(file_path)
        date = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d')

    # ä»bodyæå–æ‘˜è¦ï¼ˆå‰100å­—ï¼‰
    if not description:
        # å»é™¤Markdownå’ŒHTMLæ ¼å¼
        plain_text = re.sub(r'!\[\[.*?\]\]', '', body) # ç§»é™¤Obsidianå›¾ç‰‡ ![[...]]
        plain_text = re.sub(r'!\[.*?\]\(.*?\)', '', plain_text)  # ç§»é™¤Markdownå›¾ç‰‡
        plain_text = re.sub(r'\!\[.*?\]\[.*?\]', '', plain_text)  # ç§»é™¤Markdownå›¾ç‰‡å¼•ç”¨
        plain_text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', plain_text)  # ç§»é™¤é“¾æ¥ï¼Œä¿ç•™æ–‡æœ¬
        plain_text = re.sub(r'#+\s', '', plain_text)  # ç§»é™¤æ ‡é¢˜ç¬¦å·
        plain_text = re.sub(r'```.*?```', '', plain_text, flags=re.DOTALL)  # ç§»é™¤ä»£ç å—
        plain_text = re.sub(r'\*+', '', plain_text)  # ç§»é™¤å¼ºè°ƒç¬¦å·
        plain_text = re.sub(r'<[^>]+>', '', plain_text) # ç§»é™¤HTMLæ ‡ç­¾
        plain_text = plain_text.strip()
        description = plain_text[:100] + '...' if len(plain_text) > 100 else plain_text

    # ç¡®å®šç¬”è®°ç±»å‹
    note_type = "å­¦ä¹ ç¬”è®°" if category in ["æ•™ç¨‹", "ç¼–ç¨‹", "æ•°æ®ç§‘å­¦", "æŠ€æœ¯"] else "çµæ„Ÿ"

    # åˆ†é…é¢œè‰²
    color = CATEGORY_COLORS.get(category, CATEGORY_COLORS["é»˜è®¤"])

    # Extract internal connections (Wikilinks)
    # Matches [[Target]] or [[Target|Alias]]
    wikilink_pattern = r'\[\[(.*?)(?:\|.*?)?\]\]'
    links = re.findall(wikilink_pattern, body)
    
    # Clean links (remove anchors #...)
    clean_links = []
    for link in links:
        link_target = link.split('#')[0].strip()
        if link_target:
            clean_links.append(link_target)

    return {
        "date": date,
        "time": "00:00",  # å¦‚æœæ²¡æœ‰æ—¶é—´ä¿¡æ¯ï¼Œé»˜è®¤00:00
        "type": note_type,
        "title": title,
        "content": description,
        "tags": tags,
        "color": color,
        "links": clean_links, # Connections for Knowledge Graph
        "file_path": str(file_path)  # ä¿ç•™åŸæ–‡ä»¶è·¯å¾„
    }

def main():
    """ä¸»å‡½æ•°ï¼šéå†ç›®å½•å¹¶ç”ŸæˆJSON"""
    timeline_data = []

    # éå†Obsidianç›®å½•
    for root, dirs, files in os.walk(VAULT_PATH):
        # å¿½ç•¥ç‰¹å®šç›®å½•
        dirs[:] = [d for d in dirs if d not in IGNORE_PATTERNS]

        for file in files:
            if file.endswith('.md'):
                # Blacklist filter (Quality Control)
                blacklist = ["å¤–é«˜æ¡¥", "ç§Ÿæˆ¿", "çœ‹æˆ¿"]
                if any(keyword in file for keyword in blacklist):
                    continue

                file_path = os.path.join(root, file)
                
                # --- Quality Control Filtering ---
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        raw_content = f.read()
                except Exception:
                    continue # Skip unreadable files

                # 1. Check Completeness (Empty files)
                if not raw_content or len(raw_content.strip()) == 0:
                    continue

                # 2. Check Word Count (Min 100 characters roughly)
                # Simple heuristic: remove whitespace and count
                if len(re.sub(r'\s+', '', raw_content)) < 100:
                    continue

                # 3. Check for specific placeholders or "Draft" markers
                if "Untitled" in file or "æœªå‘½å" in file:
                    continue
                
                # Check Blacklist (Existing logic)
                blacklist = ["å¤–é«˜æ¡¥", "ç§Ÿæˆ¿", "çœ‹æˆ¿"]
                if any(keyword in file for keyword in blacklist):
                    continue

                # Pass quality check -> Process
                frontmatter, body = read_md_file(file_path)
                
                # Double check body content after frontmatter extraction
                if not body or len(body.strip()) < 50:
                    continue
                
                # Check for Draft status in Frontmatter
                is_draft = str(frontmatter.get('draft', '')).lower() == 'true'
                if is_draft:
                    print(f"ğŸš« Draft excluded: {file}")
                    continue

                note_info = extract_note_info(file_path, frontmatter, body)
                timeline_data.append(note_info)

    # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨æœ€å‰ï¼‰
    timeline_data.sort(key=lambda x: x['date'], reverse=True)

    # å†™å…¥JSONæ–‡ä»¶
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(timeline_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… åŒæ­¥å®Œæˆï¼å…±å¤„ç† {len(timeline_data)} ç¯‡ç¬”è®°")
    print(f"ğŸ“ ç”Ÿæˆçš„JSONæ–‡ä»¶ï¼š{OUTPUT_JSON}")

if __name__ == "__main__":
    main()
